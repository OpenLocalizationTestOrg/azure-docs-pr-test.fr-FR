---
title: Noyaux pour bloc-notes Jupyter sur les clusters Spark dans Azure HDInsight | Documents Microsoft
description: "Découvrez les noyaux PySpark, PySpark3 et Spark pour bloc-notes Jupyter qui sont disponibles avec les clusters Spark sur Azure HDInsight."
keywords: bloc-notes jupyter sur spark,jupyter spark
services: hdinsight
documentationcenter: 
author: nitinme
manager: jhubbard
editor: cgronlun
tags: azure-portal
ms.assetid: 0719e503-ee6d-41ac-b37e-3d77db8b121b
ms.service: hdinsight
ms.custom: hdinsightactive,hdiseo17may2017
ms.workload: big-data
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 05/15/2017
ms.author: nitinme
ms.openlocfilehash: 6cfd1c1e7b22f5460b78687c815d149e6c6deac9
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 07/11/2017
---
# <a name="kernels-for-jupyter-notebook-on-spark-clusters-in-azure-hdinsight"></a><span data-ttu-id="e4d7e-104">Noyaux pour bloc-notes Jupyter sur les clusters Spark dans Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="e4d7e-104">Kernels for Jupyter notebook on Spark clusters in Azure HDInsight</span></span> 

<span data-ttu-id="e4d7e-105">Les clusters Spark HDInsight fournissent des noyaux que vous pouvez utiliser avec le bloc-notes Jupyter sur Spark pour tester vos applications.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-105">HDInsight Spark clusters provide kernels that you can use with the Jupyter notebook on Spark for testing your applications.</span></span> <span data-ttu-id="e4d7e-106">Un noyau est un programme qui exécute et interprète votre code.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-106">A kernel is a program that runs and interprets your code.</span></span> <span data-ttu-id="e4d7e-107">Les trois noyaux sont les suivants :</span><span class="sxs-lookup"><span data-stu-id="e4d7e-107">The three kernels are:</span></span>

- <span data-ttu-id="e4d7e-108">**PySpark** : pour les applications écrites en Python2</span><span class="sxs-lookup"><span data-stu-id="e4d7e-108">**PySpark** - for applications written in Python2</span></span>
- <span data-ttu-id="e4d7e-109">**PySpark3** : pour les applications écrites en Python3</span><span class="sxs-lookup"><span data-stu-id="e4d7e-109">**PySpark3** - for applications written in Python3</span></span>
- <span data-ttu-id="e4d7e-110">**Spark** : pour les applications écrites en Scala</span><span class="sxs-lookup"><span data-stu-id="e4d7e-110">**Spark** - for applications written in Scala</span></span>

<span data-ttu-id="e4d7e-111">Dans cet article, vous allez apprendre à utiliser ces noyaux et découvrir les avantages de leur utilisation.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-111">In this article, you learn how to use these kernels and the benefits of using them.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="e4d7e-112">Composants requis</span><span class="sxs-lookup"><span data-stu-id="e4d7e-112">Prerequisites</span></span>

* <span data-ttu-id="e4d7e-113">Un cluster Apache Spark dans HDInsight.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-113">An Apache Spark cluster in HDInsight.</span></span> <span data-ttu-id="e4d7e-114">Pour obtenir des instructions, consultez [Création de clusters Apache Spark dans Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span><span class="sxs-lookup"><span data-stu-id="e4d7e-114">For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span></span>

## <a name="create-a-jupyter-notebook-on-spark-hdinsight"></a><span data-ttu-id="e4d7e-115">Créer un bloc-notes Jupyter sur Spark HDInsight</span><span class="sxs-lookup"><span data-stu-id="e4d7e-115">Create a Jupyter notebook on Spark HDInsight</span></span>

1. <span data-ttu-id="e4d7e-116">À partir du [portail Azure](https://portal.azure.com/), ouvrez votre cluster.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-116">From the [Azure portal](https://portal.azure.com/), open your cluster.</span></span>  <span data-ttu-id="e4d7e-117">Pour obtenir des instructions, consultez la page [Énumération et affichage des clusters](hdinsight-administer-use-portal-linux.md#list-and-show-clusters).</span><span class="sxs-lookup"><span data-stu-id="e4d7e-117">See [List and show clusters](hdinsight-administer-use-portal-linux.md#list-and-show-clusters) for the instructions.</span></span> <span data-ttu-id="e4d7e-118">Le cluster est ouvert dans un nouveau panneau du portail.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-118">The cluster is opened in a new portal blade.</span></span>

2. <span data-ttu-id="e4d7e-119">À partir de la section **Liens rapides** , cliquez sur **Tableaux de bord des clusters** pour ouvrir le panneau **Tableaux de bord des clusters**.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-119">From the **Quick links** section, click **Cluster dashboards** to open the **Cluster dashboards** blade.</span></span>  <span data-ttu-id="e4d7e-120">Si vous ne voyez pas **Liens rapides**, cliquez sur **Vue d’ensemble** dans le menu gauche du panneau.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-120">If you don't see **Quick Links**, click **Overview** from the left menu on the blade.</span></span>

    <span data-ttu-id="e4d7e-121">![Bloc-notes Jupyter sur Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Bloc-notes Jupyter sur Spark")</span><span class="sxs-lookup"><span data-stu-id="e4d7e-121">![Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Jupyter notebook on Spark")</span></span> 

3. <span data-ttu-id="e4d7e-122">Cliquez sur **Bloc-notes Jupyter**.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-122">Click **Jupyter Notebook**.</span></span> <span data-ttu-id="e4d7e-123">Si vous y êtes invité, entrez les informations d’identification d’administrateur pour le cluster.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-123">If prompted, enter the admin credentials for the cluster.</span></span>
   
   > [!NOTE]
   > <span data-ttu-id="e4d7e-124">Vous pouvez également atteindre le bloc-notes Jupyter sur le cluster Spark en ouvrant l’URL suivante dans votre navigateur.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-124">You may also reach the Jupyter notebook on Spark cluster by opening the following URL in your browser.</span></span> <span data-ttu-id="e4d7e-125">Remplacez **CLUSTERNAME** par le nom de votre cluster.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-125">Replace **CLUSTERNAME** with the name of your cluster:</span></span>
   >
   > `https://CLUSTERNAME.azurehdinsight.net/jupyter`
   > 
   > 

3. <span data-ttu-id="e4d7e-126">Cliquez sur **Nouveau**, puis sur **Pyspark**, **PySpark3** ou **Spark** pour créer un bloc-notes.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-126">Click **New**, and then click either **Pyspark**, **PySpark3**, or **Spark** to create a notebook.</span></span> <span data-ttu-id="e4d7e-127">Utilisez le noyau Spark pour les applications Scala, le noyau PySpark pour les applications Python2 et le noyau PySpark3 pour les applications Python3.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-127">Use the Spark kernel for Scala applications, PySpark kernel for Python2 applications, and PySpark3 kernel for Python3 applications.</span></span>
   
    <span data-ttu-id="e4d7e-128">![Noyaux pour bloc-notes Jupyter sur Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Noyaux pour bloc-notes Jupyter sur Spark")</span><span class="sxs-lookup"><span data-stu-id="e4d7e-128">![Kernels for Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Kernels for Jupyter notebook on Spark")</span></span> 

4. <span data-ttu-id="e4d7e-129">Un bloc-notes s’ouvre avec le noyau que vous avez sélectionné.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-129">A notebook opens with the kernel you selected.</span></span>

## <a name="benefits-of-using-the-kernels"></a><span data-ttu-id="e4d7e-130">Avantages de l’utilisation des noyaux</span><span class="sxs-lookup"><span data-stu-id="e4d7e-130">Benefits of using the kernels</span></span>

<span data-ttu-id="e4d7e-131">Voici quelques avantages liés à l’utilisation des nouveaux noyaux avec bloc-notes Jupyter sur des clusters Spark HDInsight.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-131">Here are a few benefits of using the new kernels with Jupyter notebook on Spark HDInsight clusters.</span></span>

- <span data-ttu-id="e4d7e-132">**Contextes prédéfinis**.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-132">**Preset contexts**.</span></span> <span data-ttu-id="e4d7e-133">Avec les noyaux **PySpark**, **PySpark3** ou **Spark**, vous n’avez pas besoin de définir les contextes Spark ou Hive explicitement avant de commencer à utiliser vos applications.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-133">With  **PySpark**, **PySpark3**, or the **Spark** kernels, you do not need to set the Spark or Hive contexts explicitly before you start working with your applications.</span></span> <span data-ttu-id="e4d7e-134">Ils sont disponibles par défaut.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-134">These are available by default.</span></span> <span data-ttu-id="e4d7e-135">Ces contextes sont les suivants :</span><span class="sxs-lookup"><span data-stu-id="e4d7e-135">These contexts are:</span></span>
   
   * <span data-ttu-id="e4d7e-136">**sc** : pour le contexte Spark</span><span class="sxs-lookup"><span data-stu-id="e4d7e-136">**sc** - for Spark context</span></span>
   * <span data-ttu-id="e4d7e-137">**sqlContext** : pour le contexte Hive</span><span class="sxs-lookup"><span data-stu-id="e4d7e-137">**sqlContext** - for Hive context</span></span>

    <span data-ttu-id="e4d7e-138">Par conséquent, vous n’avez pas à exécuter d’instructions telles que les suivantes pour définir les contextes :</span><span class="sxs-lookup"><span data-stu-id="e4d7e-138">So, you don't have to run statements like the following to set the contexts:</span></span>

        <span data-ttu-id="e4d7e-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span><span class="sxs-lookup"><span data-stu-id="e4d7e-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span></span>

    <span data-ttu-id="e4d7e-140">En revanche, vous pouvez utiliser directement les contextes prédéfinis dans votre application.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-140">Instead, you can directly use the preset contexts in your application.</span></span>

- <span data-ttu-id="e4d7e-141">**Commandes magiques de cellule**.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-141">**Cell magics**.</span></span> <span data-ttu-id="e4d7e-142">Le noyau PySpark fournit certaines « commandes magiques » prédéfinies, qui sont des commandes spéciales que vous pouvez appeler avec `%%` (par exemple, `%%MAGIC` <args>).</span><span class="sxs-lookup"><span data-stu-id="e4d7e-142">The PySpark kernel provides some predefined “magics”, which are special commands that you can call with `%%` (for example, `%%MAGIC` <args>).</span></span> <span data-ttu-id="e4d7e-143">La commande magique doit se trouver au tout début d’une cellule de code et autoriser plusieurs lignes de contenu.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-143">The magic command must be the first word in a code cell and allow for multiple lines of content.</span></span> <span data-ttu-id="e4d7e-144">Le mot magic doit être le premier mot de la cellule.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-144">The magic word should be the first word in the cell.</span></span> <span data-ttu-id="e4d7e-145">Ajouter quoi que ce soit avant la commande magique, même des commentaires, provoque une erreur.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-145">Adding anything before the magic, even comments, causes an error.</span></span>     <span data-ttu-id="e4d7e-146">Pour plus d’informations sur les commandes magiques, cliquez [ici](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span><span class="sxs-lookup"><span data-stu-id="e4d7e-146">For more information on magics, see [here](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span></span>
   
    <span data-ttu-id="e4d7e-147">Le tableau suivant répertorie les différentes commandes magiques disponibles par le biais des noyaux.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-147">The following table lists the different magics available through the kernels.</span></span>

   | <span data-ttu-id="e4d7e-148">Commande magique</span><span class="sxs-lookup"><span data-stu-id="e4d7e-148">Magic</span></span> | <span data-ttu-id="e4d7e-149">Exemple</span><span class="sxs-lookup"><span data-stu-id="e4d7e-149">Example</span></span> | <span data-ttu-id="e4d7e-150">Description</span><span class="sxs-lookup"><span data-stu-id="e4d7e-150">Description</span></span> |
   | --- | --- | --- |
   | <span data-ttu-id="e4d7e-151">help</span><span class="sxs-lookup"><span data-stu-id="e4d7e-151">help</span></span> |`%%help` |<span data-ttu-id="e4d7e-152">Génère une table de toutes les commandes magiques disponibles, accompagnées d’un exemple et d’une description</span><span class="sxs-lookup"><span data-stu-id="e4d7e-152">Generates a table of all the available magics with example and description</span></span> |
   | <span data-ttu-id="e4d7e-153">info</span><span class="sxs-lookup"><span data-stu-id="e4d7e-153">info</span></span> |`%%info` |<span data-ttu-id="e4d7e-154">Génère des informations de session pour le point de terminaison Livy actuel</span><span class="sxs-lookup"><span data-stu-id="e4d7e-154">Outputs session information for the current Livy endpoint</span></span> |
   | <span data-ttu-id="e4d7e-155">CONFIGURER</span><span class="sxs-lookup"><span data-stu-id="e4d7e-155">configure</span></span> |`%%configure -f`<br><span data-ttu-id="e4d7e-156">`{"executorMemory": "1000M"`</span><span class="sxs-lookup"><span data-stu-id="e4d7e-156">`{"executorMemory": "1000M"`,</span></span><br><span data-ttu-id="e4d7e-157">`"executorCores": 4`}</span><span class="sxs-lookup"><span data-stu-id="e4d7e-157">`"executorCores": 4`}</span></span> |<span data-ttu-id="e4d7e-158">Configure les paramètres de création d’une session.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-158">Configures the parameters for creating a session.</span></span> <span data-ttu-id="e4d7e-159">L’indicateur de forçage (-f) est obligatoire si une session a déjà été créée, afin de garantir que la session est supprimée et recréée.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-159">The force flag (-f) is mandatory if a session has already been created, which ensures that the session is dropped and recreated.</span></span> <span data-ttu-id="e4d7e-160">Consultez la section [POST /sessions Request Body de Livy](https://github.com/cloudera/livy#request-body) pour obtenir la liste des paramètres valides.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-160">Look at [Livy's POST /sessions Request Body](https://github.com/cloudera/livy#request-body) for a list of valid parameters.</span></span> <span data-ttu-id="e4d7e-161">Les paramètres doivent être passés en tant que chaîne JSON et être spécifiés sur la ligne suivant la commande magique, comme indiqué dans l’exemple de colonne.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-161">Parameters must be passed in as a JSON string and must be on the next line after the magic, as shown in the example column.</span></span> |
   | <span data-ttu-id="e4d7e-162">sql</span><span class="sxs-lookup"><span data-stu-id="e4d7e-162">sql</span></span> |`%%sql -o <variable name>`<br> `SHOW TABLES` |<span data-ttu-id="e4d7e-163">Exécute une requête Hive sur sqlContext.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-163">Executes a Hive query against the sqlContext.</span></span> <span data-ttu-id="e4d7e-164">Si le paramètre `-o` est passé, le résultat de la requête est conservé dans le contexte Python %%local en tant que trame de données [Pandas](http://pandas.pydata.org/) .</span><span class="sxs-lookup"><span data-stu-id="e4d7e-164">If the `-o` parameter is passed, the result of the query is persisted in the %%local Python context as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> |
   | <span data-ttu-id="e4d7e-165">local</span><span class="sxs-lookup"><span data-stu-id="e4d7e-165">local</span></span> |`%%local`<br>`a=1` |<span data-ttu-id="e4d7e-166">Tout le code dans les lignes suivantes est exécuté localement.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-166">All the code in subsequent lines is executed locally.</span></span> <span data-ttu-id="e4d7e-167">Le code doit être un code Python2 valide, même s’il ne correspond pas au noyau que vous utilisez.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-167">Code must be valid Python2 code even irrespective of the kernel you are using.</span></span> <span data-ttu-id="e4d7e-168">Donc, même si vous avez sélectionné les noyaux **PySpark3** ou **Spark** en créant le bloc-notes, si vous utilisez la commande magique `%%local` dans une cellule, cette cellule doit uniquement comporter un code Python2 valide.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-168">So, even if you selected **PySpark3** or **Spark** kernels while creating the notebook, if you use the `%%local` magic in a cell, that cell must only have valid Python2 code..</span></span> |
   | <span data-ttu-id="e4d7e-169">journaux</span><span class="sxs-lookup"><span data-stu-id="e4d7e-169">logs</span></span> |`%%logs` |<span data-ttu-id="e4d7e-170">Génère les journaux de la session Livy en cours.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-170">Outputs the logs for the current Livy session.</span></span> |
   | <span data-ttu-id="e4d7e-171">delete</span><span class="sxs-lookup"><span data-stu-id="e4d7e-171">delete</span></span> |`%%delete -f -s <session number>` |<span data-ttu-id="e4d7e-172">Supprime une session spécifique du point de terminaison Livy actuel.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-172">Deletes a specific session of the current Livy endpoint.</span></span> <span data-ttu-id="e4d7e-173">Notez que vous ne pouvez pas supprimer la session qui est lancée pour le noyau lui-même.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-173">Note that you cannot delete the session that is initiated for the kernel itself.</span></span> |
   | <span data-ttu-id="e4d7e-174">cleanup</span><span class="sxs-lookup"><span data-stu-id="e4d7e-174">cleanup</span></span> |`%%cleanup -f` |<span data-ttu-id="e4d7e-175">Supprime toutes les sessions pour le point de terminaison Livy actuel, y compris la session de ce bloc-notes.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-175">Deletes all the sessions for the current Livy endpoint, including this notebook's session.</span></span> <span data-ttu-id="e4d7e-176">L’indicateur de forçage -f est obligatoire.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-176">The force flag -f is mandatory.</span></span> |

   > [!NOTE]
   > <span data-ttu-id="e4d7e-177">Outre les commandes magiques ajoutées par le noyau PySpark, vous pouvez également utiliser les [commandes magiques IPython intégrées](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), notamment `%%sh`.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-177">In addition to the magics added by the PySpark kernel, you can also use the [built-in IPython magics](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), including `%%sh`.</span></span> <span data-ttu-id="e4d7e-178">Vous pouvez utiliser la commande magique `%%sh` pour exécuter des scripts et des blocs de code sur le nœud principal du cluster.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-178">You can use the `%%sh` magic to run scripts and block of code on the cluster headnode.</span></span>
   >
   >
2. <span data-ttu-id="e4d7e-179">**Visualisation automatique**.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-179">**Auto visualization**.</span></span> <span data-ttu-id="e4d7e-180">Le noyau **Pyspark** visualise automatiquement la sortie des requêtes Hive et SQL.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-180">The **Pyspark** kernel automatically visualizes the output of Hive and SQL queries.</span></span> <span data-ttu-id="e4d7e-181">Vous pouvez choisir entre plusieurs types de visualisations, notamment tableau, secteurs, courbes, aires et barres.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-181">You can choose between several different types of visualizations including Table, Pie, Line, Area, Bar.</span></span>

## <a name="parameters-supported-with-the-sql-magic"></a><span data-ttu-id="e4d7e-182">Paramètres pris en charge avec la commande magique %%sql</span><span class="sxs-lookup"><span data-stu-id="e4d7e-182">Parameters supported with the %%sql magic</span></span>
<span data-ttu-id="e4d7e-183">La commande magique `%%sql` prend en charge différents paramètres qui vous permettent de contrôler le type de sortie que vous recevez quand vous exécutez des requêtes.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-183">The `%%sql` magic supports different parameters that you can use to control the kind of output that you receive when you run queries.</span></span> <span data-ttu-id="e4d7e-184">Le tableau suivant répertorie les paramètres de sortie.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-184">The following table lists the output.</span></span>

| <span data-ttu-id="e4d7e-185">Paramètre</span><span class="sxs-lookup"><span data-stu-id="e4d7e-185">Parameter</span></span> | <span data-ttu-id="e4d7e-186">Exemple</span><span class="sxs-lookup"><span data-stu-id="e4d7e-186">Example</span></span> | <span data-ttu-id="e4d7e-187">Description</span><span class="sxs-lookup"><span data-stu-id="e4d7e-187">Description</span></span> |
| --- | --- | --- |
| <span data-ttu-id="e4d7e-188">-o</span><span class="sxs-lookup"><span data-stu-id="e4d7e-188">-o</span></span> |`-o <VARIABLE NAME>` |<span data-ttu-id="e4d7e-189">Utilisez ce paramètre pour conserver le résultat de la requête dans le contexte Python %%local en tant que trame de données [Pandas](http://pandas.pydata.org/) .</span><span class="sxs-lookup"><span data-stu-id="e4d7e-189">Use this parameter to persist the result of the query, in the %%local Python context, as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> <span data-ttu-id="e4d7e-190">Le nom de la variable dataframe est le nom de variable que vous spécifiez.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-190">The name of the dataframe variable is the variable name you specify.</span></span> |
| <span data-ttu-id="e4d7e-191">-q</span><span class="sxs-lookup"><span data-stu-id="e4d7e-191">-q</span></span> |`-q` |<span data-ttu-id="e4d7e-192">Utilisez ce paramètre pour désactiver les visualisations pour la cellule.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-192">Use this to turn off visualizations for the cell.</span></span> <span data-ttu-id="e4d7e-193">Si vous ne voulez pas visualiser automatiquement le contenu d’une cellule et préférez simplement capturer le contenu comme une trame de données, utilisez `-q -o <VARIABLE>`.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-193">If you don't want to auto-visualize the content of a cell and just want to capture it as a dataframe, then use `-q -o <VARIABLE>`.</span></span> <span data-ttu-id="e4d7e-194">Si vous souhaitez désactiver les visualisations sans capturer les résultats (par exemple, pour exécuter une requête SQL, comme une instruction `CREATE TABLE`), utilisez `-q` sans spécifier d’argument `-o`.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-194">If you want to turn off visualizations without capturing the results (for example, for running a SQL query, like a `CREATE TABLE` statement), use `-q` without specifying a `-o` argument.</span></span> |
| <span data-ttu-id="e4d7e-195">-m</span><span class="sxs-lookup"><span data-stu-id="e4d7e-195">-m</span></span> |`-m <METHOD>` |<span data-ttu-id="e4d7e-196">**METHOD** prend la valeur **take** ou **sample** (**take** est la valeur par défaut).</span><span class="sxs-lookup"><span data-stu-id="e4d7e-196">Where **METHOD** is either **take** or **sample** (default is **take**).</span></span> <span data-ttu-id="e4d7e-197">Si la méthode est **take**, le noyau sélectionne des éléments à partir du haut du jeu de données de résultats spécifié par la valeur MAXROWS (décrite plus bas dans ce tableau).</span><span class="sxs-lookup"><span data-stu-id="e4d7e-197">If the method is **take**, the kernel picks elements from the top of the result data set specified by MAXROWS (described later in this table).</span></span> <span data-ttu-id="e4d7e-198">Si la méthode est **sample**, le noyau échantillonne de façon aléatoire les éléments du jeu de données en fonction du paramètre `-r` (décrit ci-après dans ce tableau).</span><span class="sxs-lookup"><span data-stu-id="e4d7e-198">If the method is **sample**, the kernel randomly samples elements of the data set according to `-r` parameter, described next in this table.</span></span> |
| <span data-ttu-id="e4d7e-199">-r</span><span class="sxs-lookup"><span data-stu-id="e4d7e-199">-r</span></span> |`-r <FRACTION>` |<span data-ttu-id="e4d7e-200">Ici, **FRACTION** est un nombre à virgule flottante compris entre 0,0 et 1,0.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-200">Here **FRACTION** is a floating-point number between 0.0 and 1.0.</span></span> <span data-ttu-id="e4d7e-201">Si l’exemple de méthode de la requête SQL est `sample`, le noyau échantillonne de façon aléatoire la fraction spécifiée des éléments du jeu de résultats.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-201">If the sample method for the SQL query is `sample`, then the kernel randomly samples the specified fraction of the elements of the result set for you.</span></span> <span data-ttu-id="e4d7e-202">Par exemple, si vous exécutez une requête SQL avec les arguments `-m sample -r 0.01`, 1 % des lignes de résultat sont échantillonnées de façon aléatoire.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-202">For example, if you run a SQL query with the arguments `-m sample -r 0.01`, then 1% of the result rows are randomly sampled.</span></span> |
| -n |`-n <MAXROWS>` |<span data-ttu-id="e4d7e-203">**MAXROWS** est une valeur entière.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-203">**MAXROWS** is an integer value.</span></span> <span data-ttu-id="e4d7e-204">Le noyau limite le nombre de lignes de la sortie au nombre défini par **MAXROWS**.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-204">The kernel limits the number of output rows to **MAXROWS**.</span></span> <span data-ttu-id="e4d7e-205">Si **MAXROWS** est un nombre négatif comme **-1**, le nombre de lignes dans le jeu de résultats n’est pas limité.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-205">If **MAXROWS** is a negative number such as **-1**, then the number of rows in the result set is not limited.</span></span> |

<span data-ttu-id="e4d7e-206">**Exemple :**</span><span class="sxs-lookup"><span data-stu-id="e4d7e-206">**Example:**</span></span>

    %%sql -q -m sample -r 0.1 -n 500 -o query2
    SELECT * FROM hivesampletable

<span data-ttu-id="e4d7e-207">L’instruction ci-dessus effectue les actions suivantes :</span><span class="sxs-lookup"><span data-stu-id="e4d7e-207">The statement above does the following:</span></span>

* <span data-ttu-id="e4d7e-208">Elle sélectionne tous les enregistrements présents dans **hivesampletable**.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-208">Selects all records from **hivesampletable**.</span></span>
* <span data-ttu-id="e4d7e-209">Comme nous utilisons le paramètre - q, elle désactive la visualisation automatique.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-209">Because we use -q, it turns off auto-visualization.</span></span>
* <span data-ttu-id="e4d7e-210">Comme nous utilisons `-m sample -r 0.1 -n 500` , elle échantillonne de façon aléatoire 10 % des lignes présentes dans hivesampletable et limite la taille du jeu de résultats à 500 lignes.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-210">Because we use `-m sample -r 0.1 -n 500` it randomly samples 10% of the rows in the hivesampletable and limits the size of the result set to 500 rows.</span></span>
* <span data-ttu-id="e4d7e-211">Enfin, comme nous avons utilisé `-o query2` , elle enregistre également la sortie dans une trame de données appelée **query2**.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-211">Finally, because we used `-o query2` it also saves the output into a dataframe called **query2**.</span></span>

## <a name="considerations-while-using-the-new-kernels"></a><span data-ttu-id="e4d7e-212">Points à prendre en compte lors de l'utilisation des nouveaux noyaux</span><span class="sxs-lookup"><span data-stu-id="e4d7e-212">Considerations while using the new kernels</span></span>

<span data-ttu-id="e4d7e-213">Quel que soit le noyau que vous utilisez, laisser les blocs-notes s’exécuter consomme vos ressources de cluster.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-213">Whichever kernel you use, leaving the notebooks running consumes the cluster resources.</span></span>  <span data-ttu-id="e4d7e-214">Avec ces noyaux, les contextes étant prédéfinis, le simple fait de quitter les blocs-notes n’arrête pas le contexte. Par conséquent, les ressources du cluster restent en cours d’utilisation.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-214">With these kernels, because the contexts are preset, simply exiting the notebooks does not kill the context and hence the cluster resources continue to be in use.</span></span> <span data-ttu-id="e4d7e-215">Une bonne pratique consiste à utiliser l’option **Fermer et arrêter** à partir du menu **Fichier** du bloc-notes menu lorsque vous avez fini de l’utiliser, ce qui supprime le contexte puis ferme le bloc-notes.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-215">A good practice is to use the **Close and Halt** option from the notebook's **File** menu when you are finished using the notebook, which kills the context and then exits the notebook.</span></span>     

## <a name="show-me-some-examples"></a><span data-ttu-id="e4d7e-216">Voici quelques exemples :</span><span class="sxs-lookup"><span data-stu-id="e4d7e-216">Show me some examples</span></span>

<span data-ttu-id="e4d7e-217">Lorsque vous ouvrez un bloc-notes Jupyter, deux dossiers sont disponibles au niveau racine.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-217">When you open a Jupyter notebook, you see two folders available at the root level.</span></span>

* <span data-ttu-id="e4d7e-218">Le dossier **PySpark** contient des exemples de Notebooks qui utilisent le nouveau noyau **Python**.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-218">The **PySpark** folder has sample notebooks that use the new **Python** kernel.</span></span>
* <span data-ttu-id="e4d7e-219">Le dossier **Scala** comprend des exemples de blocs-notes qui utilisent le nouveau noyau **Spark**.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-219">The **Scala** folder has sample notebooks that use the new **Spark** kernel.</span></span>

<span data-ttu-id="e4d7e-220">Vous pouvez ouvrir le Notebook **00 - [READ ME FIRST] Spark Magic Kernel Features** à partir du dossier **PySpark** ou **Spark** pour en savoir plus sur les différentes commandes magiques disponibles.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-220">You can open the **00 - [READ ME FIRST] Spark Magic Kernel Features** notebook from the **PySpark** or **Spark** folder to learn about the different magics available.</span></span> <span data-ttu-id="e4d7e-221">Vous pouvez également recourir aux autres exemples de blocs-notes disponibles sous les deux dossiers pour savoir comment utiliser concrètement des blocs-notes Jupyter avec des clusters HDInsight Spark.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-221">You can also use the other sample notebooks available under the two folders to learn how to achieve different scenarios using Jupyter notebooks with HDInsight Spark clusters.</span></span>

## <a name="where-are-the-notebooks-stored"></a><span data-ttu-id="e4d7e-222">Où sont stockés les blocs-notes ?</span><span class="sxs-lookup"><span data-stu-id="e4d7e-222">Where are the notebooks stored?</span></span>

<span data-ttu-id="e4d7e-223">Les Notebooks Jupyter sont enregistrés dans le compte de stockage associé au cluster, dans le dossier **/HdiNotebooks** .</span><span class="sxs-lookup"><span data-stu-id="e4d7e-223">Jupyter notebooks are saved to the storage account associated with the cluster under the **/HdiNotebooks** folder.</span></span>  <span data-ttu-id="e4d7e-224">Les blocs-notes, les fichiers texte et les dossiers que vous créez dans Jupyter sont accessibles à partir du compte de stockage.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-224">Notebooks, text files, and folders that you create from within Jupyter are accessible from the storage account.</span></span>  <span data-ttu-id="e4d7e-225">Par exemple, si vous utilisez Jupyter pour créer un dossier **myfolder** et un bloc-notes **myfolder/mynotebook.ipynb**, vous pouvez accéder à ce bloc-notes dans `/HdiNotebooks/myfolder/mynotebook.ipynb` au sein du compte de stockage.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-225">For example, if you use Jupyter to create a folder **myfolder** and a notebook **myfolder/mynotebook.ipynb**, you can access that notebook at `/HdiNotebooks/myfolder/mynotebook.ipynb` within the storage account.</span></span>  <span data-ttu-id="e4d7e-226">L’inverse est également vrai : si vous chargez un bloc-notes directement dans votre compte de stockage dans `/HdiNotebooks/mynotebook1.ipynb`, le bloc-notes est également accessible à partir de Jupyter.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-226">The reverse is also true, that is, if you upload a notebook directly to your storage account at `/HdiNotebooks/mynotebook1.ipynb`, the notebook is visible from Jupyter as well.</span></span>  <span data-ttu-id="e4d7e-227">Les blocs-notes sont conservés dans le compte de stockage même après la suppression du cluster.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-227">Notebooks remain in the storage account even after the cluster is deleted.</span></span>

<span data-ttu-id="e4d7e-228">Les blocs-notes sont enregistrés dans le compte de stockage dans un mode compatible avec HDFS.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-228">The way notebooks are saved to the storage account is compatible with HDFS.</span></span> <span data-ttu-id="e4d7e-229">Si vous utilisez SSH dans le cluster, vous pouvez donc exécuter des commandes de gestion des fichiers telles que celles de l’extrait de code suivant :</span><span class="sxs-lookup"><span data-stu-id="e4d7e-229">So, if you SSH into the cluster you can use file management commands as shown in the following snippet:</span></span>

    hdfs dfs -ls /HdiNotebooks                               # List everything at the root directory – everything in this directory is visible to Jupyter from the home page
    hdfs dfs –copyToLocal /HdiNotebooks                    # Download the contents of the HdiNotebooks folder
    hdfs dfs –copyFromLocal example.ipynb /HdiNotebooks   # Upload a notebook example.ipynb to the root folder so it’s visible from Jupyter


<span data-ttu-id="e4d7e-230">En cas de problèmes d’accès au compte de stockage pour le cluster, les Notebooks sont également enregistrés sur le nœud principal `/var/lib/jupyter`.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-230">In case there are issues accessing the storage account for the cluster, the notebooks are also saved on the headnode `/var/lib/jupyter`.</span></span>

## <a name="supported-browser"></a><span data-ttu-id="e4d7e-231">Navigateur pris en charge</span><span class="sxs-lookup"><span data-stu-id="e4d7e-231">Supported browser</span></span>

<span data-ttu-id="e4d7e-232">Les blocs-notes Jupyter sur clusters Spark HDInsight sont pris en charge uniquement sur Google Chrome.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-232">Jupyter notebooks on Spark HDInsight clusters are supported only on Google Chrome.</span></span>

## <a name="feedback"></a><span data-ttu-id="e4d7e-233">Commentaires</span><span class="sxs-lookup"><span data-stu-id="e4d7e-233">Feedback</span></span>
<span data-ttu-id="e4d7e-234">Les nouveaux noyaux sont en phase d’évolution et gagneront en maturité avec le temps.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-234">The new kernels are in evolving stage and will mature over time.</span></span> <span data-ttu-id="e4d7e-235">Les API pourront également être amenés à évoluer au fur et à mesure des évolutions des noyaux.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-235">This could also mean that APIs could change as these kernels mature.</span></span> <span data-ttu-id="e4d7e-236">Nous aimerions recevoir vos commentaires concernant l'utilisation de ces nouveaux noyaux.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-236">We would appreciate any feedback that you have while using these new kernels.</span></span> <span data-ttu-id="e4d7e-237">Cela nous est utile pour préparer la version finale de ces noyaux.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-237">This is useful in shaping the final release of these kernels.</span></span> <span data-ttu-id="e4d7e-238">Vous pouvez laisser vos commentaires/remarques sous la section **Commentaires** en dessous de cet article.</span><span class="sxs-lookup"><span data-stu-id="e4d7e-238">You can leave your comments/feedback under the **Comments** section at the bottom of this article.</span></span>

## <span data-ttu-id="e4d7e-239"><a name="seealso"></a>Voir aussi</span><span class="sxs-lookup"><span data-stu-id="e4d7e-239"><a name="seealso"></a>See also</span></span>
* [<span data-ttu-id="e4d7e-240">Vue d’ensemble : Apache Spark sur Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="e4d7e-240">Overview: Apache Spark on Azure HDInsight</span></span>](hdinsight-apache-spark-overview.md)

### <a name="scenarios"></a><span data-ttu-id="e4d7e-241">Scénarios</span><span class="sxs-lookup"><span data-stu-id="e4d7e-241">Scenarios</span></span>
* [<span data-ttu-id="e4d7e-242">Spark avec BI : effectuez une analyse interactive des données à l’aide de Spark dans HDInsight avec des outils BI</span><span class="sxs-lookup"><span data-stu-id="e4d7e-242">Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools</span></span>](hdinsight-apache-spark-use-bi-tools.md)
* [<span data-ttu-id="e4d7e-243">Spark avec Machine Learning : Utiliser Spark dans HDInsight pour l’analyse de la température de bâtiments à l’aide de données HVAC</span><span class="sxs-lookup"><span data-stu-id="e4d7e-243">Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data</span></span>](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [<span data-ttu-id="e4d7e-244">Spark avec Machine Learning : Utiliser Spark dans HDInsight pour prédire les résultats de l’inspection des aliments</span><span class="sxs-lookup"><span data-stu-id="e4d7e-244">Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results</span></span>](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [<span data-ttu-id="e4d7e-245">Streaming Spark : Utiliser Spark dans HDInsight pour créer des applications de diffusion en continu en temps réel</span><span class="sxs-lookup"><span data-stu-id="e4d7e-245">Spark Streaming: Use Spark in HDInsight for building real-time streaming applications</span></span>](hdinsight-apache-spark-eventhub-streaming.md)
* [<span data-ttu-id="e4d7e-246">Analyse des journaux de site web à l’aide de Spark dans HDInsight</span><span class="sxs-lookup"><span data-stu-id="e4d7e-246">Website log analysis using Spark in HDInsight</span></span>](hdinsight-apache-spark-custom-library-website-log-analysis.md)

### <a name="create-and-run-applications"></a><span data-ttu-id="e4d7e-247">Créer et exécuter des applications</span><span class="sxs-lookup"><span data-stu-id="e4d7e-247">Create and run applications</span></span>
* [<span data-ttu-id="e4d7e-248">Créer une application autonome avec Scala</span><span class="sxs-lookup"><span data-stu-id="e4d7e-248">Create a standalone application using Scala</span></span>](hdinsight-apache-spark-create-standalone-application.md)
* [<span data-ttu-id="e4d7e-249">Exécuter des tâches à distance avec Livy sur un cluster Spark</span><span class="sxs-lookup"><span data-stu-id="e4d7e-249">Run jobs remotely on a Spark cluster using Livy</span></span>](hdinsight-apache-spark-livy-rest-interface.md)

### <a name="tools-and-extensions"></a><span data-ttu-id="e4d7e-250">Outils et extensions</span><span class="sxs-lookup"><span data-stu-id="e4d7e-250">Tools and extensions</span></span>
* [<span data-ttu-id="e4d7e-251">Utilisation du plugin d’outils HDInsight pour IntelliJ IDEA pour créer et soumettre des applications Spark Scala</span><span class="sxs-lookup"><span data-stu-id="e4d7e-251">Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applications</span></span>](hdinsight-apache-spark-intellij-tool-plugin.md)
* [<span data-ttu-id="e4d7e-252">Use HDInsight Tools Plugin for IntelliJ IDEA to debug Spark applications remotely) (Utiliser le plug-in Outils HDInsight pour IntelliJ IDEA pour déboguer des applications Spark à distance)</span><span class="sxs-lookup"><span data-stu-id="e4d7e-252">Use HDInsight Tools Plugin for IntelliJ IDEA to debug Spark applications remotely</span></span>](hdinsight-apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)
* [<span data-ttu-id="e4d7e-253">Utiliser des bloc-notes Zeppelin avec un cluster Spark sur HDInsight</span><span class="sxs-lookup"><span data-stu-id="e4d7e-253">Use Zeppelin notebooks with a Spark cluster on HDInsight</span></span>](hdinsight-apache-spark-zeppelin-notebook.md)
* [<span data-ttu-id="e4d7e-254">Utiliser des packages externes avec les blocs-notes Jupyter</span><span class="sxs-lookup"><span data-stu-id="e4d7e-254">Use external packages with Jupyter notebooks</span></span>](hdinsight-apache-spark-jupyter-notebook-use-external-packages.md)
* [<span data-ttu-id="e4d7e-255">Install Jupyter on your computer and connect to an HDInsight Spark cluster (Installer Jupyter sur un ordinateur et se connecter au cluster Spark sur HDInsight)</span><span class="sxs-lookup"><span data-stu-id="e4d7e-255">Install Jupyter on your computer and connect to an HDInsight Spark cluster</span></span>](hdinsight-apache-spark-jupyter-notebook-install-locally.md)

### <a name="manage-resources"></a><span data-ttu-id="e4d7e-256">Gestion des ressources</span><span class="sxs-lookup"><span data-stu-id="e4d7e-256">Manage resources</span></span>
* [<span data-ttu-id="e4d7e-257">Gérer les ressources du cluster Apache Spark dans Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="e4d7e-257">Manage resources for the Apache Spark cluster in Azure HDInsight</span></span>](hdinsight-apache-spark-resource-manager.md)
* [<span data-ttu-id="e4d7e-258">Suivi et débogage des tâches en cours d’exécution sur un cluster Apache Spark dans HDInsight</span><span class="sxs-lookup"><span data-stu-id="e4d7e-258">Track and debug jobs running on an Apache Spark cluster in HDInsight</span></span>](hdinsight-apache-spark-job-debugging.md)
