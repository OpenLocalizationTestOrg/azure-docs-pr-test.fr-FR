---
title: les clusters aaaKernels pour bloc-notes jupyter sur Spark dans Azure HDInsight | Documents Microsoft
description: "Découvrez les noyaux hello PySpark, PySpark3 et Spark pour bloc-notes jupyter disponible avec les clusters de Spark sur Azure HDInsight."
keywords: bloc-notes jupyter sur spark,jupyter spark
services: hdinsight
documentationcenter: 
author: nitinme
manager: jhubbard
editor: cgronlun
tags: azure-portal
ms.assetid: 0719e503-ee6d-41ac-b37e-3d77db8b121b
ms.service: hdinsight
ms.custom: hdinsightactive,hdiseo17may2017
ms.workload: big-data
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 05/15/2017
ms.author: nitinme
ms.openlocfilehash: 560c944fe850c5753ac9fa90550b804f0c47d14c
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 10/06/2017
---
# <a name="kernels-for-jupyter-notebook-on-spark-clusters-in-azure-hdinsight"></a><span data-ttu-id="0f363-104">Noyaux pour bloc-notes Jupyter sur les clusters Spark dans Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="0f363-104">Kernels for Jupyter notebook on Spark clusters in Azure HDInsight</span></span> 

<span data-ttu-id="0f363-105">Les clusters HDInsight Spark fournissent des noyaux que vous pouvez utiliser avec hello bloc-notes jupyter sur Spark pour tester vos applications.</span><span class="sxs-lookup"><span data-stu-id="0f363-105">HDInsight Spark clusters provide kernels that you can use with hello Jupyter notebook on Spark for testing your applications.</span></span> <span data-ttu-id="0f363-106">Un noyau est un programme qui exécute et interprète votre code.</span><span class="sxs-lookup"><span data-stu-id="0f363-106">A kernel is a program that runs and interprets your code.</span></span> <span data-ttu-id="0f363-107">les trois noyaux Hello sont les suivantes :</span><span class="sxs-lookup"><span data-stu-id="0f363-107">hello three kernels are:</span></span>

- <span data-ttu-id="0f363-108">**PySpark** : pour les applications écrites en Python2</span><span class="sxs-lookup"><span data-stu-id="0f363-108">**PySpark** - for applications written in Python2</span></span>
- <span data-ttu-id="0f363-109">**PySpark3** : pour les applications écrites en Python3</span><span class="sxs-lookup"><span data-stu-id="0f363-109">**PySpark3** - for applications written in Python3</span></span>
- <span data-ttu-id="0f363-110">**Spark** : pour les applications écrites en Scala</span><span class="sxs-lookup"><span data-stu-id="0f363-110">**Spark** - for applications written in Scala</span></span>

<span data-ttu-id="0f363-111">Dans cet article, vous apprendrez comment toouse ces avantages hello leur utilisation et noyaux.</span><span class="sxs-lookup"><span data-stu-id="0f363-111">In this article, you learn how toouse these kernels and hello benefits of using them.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="0f363-112">Composants requis</span><span class="sxs-lookup"><span data-stu-id="0f363-112">Prerequisites</span></span>

* <span data-ttu-id="0f363-113">Un cluster Apache Spark dans HDInsight.</span><span class="sxs-lookup"><span data-stu-id="0f363-113">An Apache Spark cluster in HDInsight.</span></span> <span data-ttu-id="0f363-114">Pour obtenir des instructions, consultez [Création de clusters Apache Spark dans Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span><span class="sxs-lookup"><span data-stu-id="0f363-114">For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span></span>

## <a name="create-a-jupyter-notebook-on-spark-hdinsight"></a><span data-ttu-id="0f363-115">Créer un bloc-notes Jupyter sur Spark HDInsight</span><span class="sxs-lookup"><span data-stu-id="0f363-115">Create a Jupyter notebook on Spark HDInsight</span></span>

1. <span data-ttu-id="0f363-116">À partir de hello [portail Azure](https://portal.azure.com/), ouvrez votre cluster.</span><span class="sxs-lookup"><span data-stu-id="0f363-116">From hello [Azure portal](https://portal.azure.com/), open your cluster.</span></span>  <span data-ttu-id="0f363-117">Consultez [liste et afficher les clusters](hdinsight-administer-use-portal-linux.md#list-and-show-clusters) pour obtenir des instructions hello.</span><span class="sxs-lookup"><span data-stu-id="0f363-117">See [List and show clusters](hdinsight-administer-use-portal-linux.md#list-and-show-clusters) for hello instructions.</span></span> <span data-ttu-id="0f363-118">cluster de Hello est ouvert dans un nouveau panneau de portail.</span><span class="sxs-lookup"><span data-stu-id="0f363-118">hello cluster is opened in a new portal blade.</span></span>

2. <span data-ttu-id="0f363-119">À partir de hello **liens rapides** , cliquez sur **tableaux de bord du Cluster** tooopen hello **tableaux de bord du Cluster** panneau.</span><span class="sxs-lookup"><span data-stu-id="0f363-119">From hello **Quick links** section, click **Cluster dashboards** tooopen hello **Cluster dashboards** blade.</span></span>  <span data-ttu-id="0f363-120">Si vous ne voyez pas **liens rapides**, cliquez sur **vue d’ensemble** à partir du menu de gauche hello sur le panneau de hello.</span><span class="sxs-lookup"><span data-stu-id="0f363-120">If you don't see **Quick Links**, click **Overview** from hello left menu on hello blade.</span></span>

    <span data-ttu-id="0f363-121">![Bloc-notes Jupyter sur Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Bloc-notes Jupyter sur Spark")</span><span class="sxs-lookup"><span data-stu-id="0f363-121">![Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Jupyter notebook on Spark")</span></span> 

3. <span data-ttu-id="0f363-122">Cliquez sur **Bloc-notes Jupyter**.</span><span class="sxs-lookup"><span data-stu-id="0f363-122">Click **Jupyter Notebook**.</span></span> <span data-ttu-id="0f363-123">Si vous y êtes invité, entrez les informations d’identification du admin de hello pour le cluster de hello.</span><span class="sxs-lookup"><span data-stu-id="0f363-123">If prompted, enter hello admin credentials for hello cluster.</span></span>
   
   > [!NOTE]
   > <span data-ttu-id="0f363-124">Vous pouvez également atteindre hello bloc-notes jupyter sur cluster Spark par hello ouverture suivante d’URL dans votre navigateur.</span><span class="sxs-lookup"><span data-stu-id="0f363-124">You may also reach hello Jupyter notebook on Spark cluster by opening hello following URL in your browser.</span></span> <span data-ttu-id="0f363-125">Remplacez **CLUSTERNAME** avec nom hello de votre cluster :</span><span class="sxs-lookup"><span data-stu-id="0f363-125">Replace **CLUSTERNAME** with hello name of your cluster:</span></span>
   >
   > `https://CLUSTERNAME.azurehdinsight.net/jupyter`
   > 
   > 

3. <span data-ttu-id="0f363-126">Cliquez sur **nouveau**, puis cliquez sur **Pyspark**, **PySpark3**, ou **Spark** toocreate un ordinateur portable.</span><span class="sxs-lookup"><span data-stu-id="0f363-126">Click **New**, and then click either **Pyspark**, **PySpark3**, or **Spark** toocreate a notebook.</span></span> <span data-ttu-id="0f363-127">Utiliser le noyau de Spark hello pour les applications Scala, noyau PySpark pour les applications Python2 et PySpark3 noyau pour les applications Python3.</span><span class="sxs-lookup"><span data-stu-id="0f363-127">Use hello Spark kernel for Scala applications, PySpark kernel for Python2 applications, and PySpark3 kernel for Python3 applications.</span></span>
   
    <span data-ttu-id="0f363-128">![Noyaux pour bloc-notes Jupyter sur Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Noyaux pour bloc-notes Jupyter sur Spark")</span><span class="sxs-lookup"><span data-stu-id="0f363-128">![Kernels for Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Kernels for Jupyter notebook on Spark")</span></span> 

4. <span data-ttu-id="0f363-129">Un bloc-notes s’ouvre avec le noyau hello que vous avez sélectionné.</span><span class="sxs-lookup"><span data-stu-id="0f363-129">A notebook opens with hello kernel you selected.</span></span>

## <a name="benefits-of-using-hello-kernels"></a><span data-ttu-id="0f363-130">Avantages de l’utilisation de noyaux de hello</span><span class="sxs-lookup"><span data-stu-id="0f363-130">Benefits of using hello kernels</span></span>

<span data-ttu-id="0f363-131">Voici quelques avantages de l’utilisation de noyaux hello avec bloc-notes jupyter sur les clusters HDInsight de Spark.</span><span class="sxs-lookup"><span data-stu-id="0f363-131">Here are a few benefits of using hello new kernels with Jupyter notebook on Spark HDInsight clusters.</span></span>

- <span data-ttu-id="0f363-132">**Contextes prédéfinis**.</span><span class="sxs-lookup"><span data-stu-id="0f363-132">**Preset contexts**.</span></span> <span data-ttu-id="0f363-133">Avec **PySpark**, **PySpark3**, ou hello **Spark** noyaux, vous n’avez pas besoin tooset hello Spark ou ruche contextes explicitement avant de commencer à utiliser avec vos applications.</span><span class="sxs-lookup"><span data-stu-id="0f363-133">With  **PySpark**, **PySpark3**, or hello **Spark** kernels, you do not need tooset hello Spark or Hive contexts explicitly before you start working with your applications.</span></span> <span data-ttu-id="0f363-134">Ils sont disponibles par défaut.</span><span class="sxs-lookup"><span data-stu-id="0f363-134">These are available by default.</span></span> <span data-ttu-id="0f363-135">Ces contextes sont les suivants :</span><span class="sxs-lookup"><span data-stu-id="0f363-135">These contexts are:</span></span>
   
   * <span data-ttu-id="0f363-136">**sc** : pour le contexte Spark</span><span class="sxs-lookup"><span data-stu-id="0f363-136">**sc** - for Spark context</span></span>
   * <span data-ttu-id="0f363-137">**sqlContext** : pour le contexte Hive</span><span class="sxs-lookup"><span data-stu-id="0f363-137">**sqlContext** - for Hive context</span></span>

    <span data-ttu-id="0f363-138">Par conséquent, vous n’avez pas hello suivant des contextes de hello tooset toorun instructions :</span><span class="sxs-lookup"><span data-stu-id="0f363-138">So, you don't have toorun statements like hello following tooset hello contexts:</span></span>

        <span data-ttu-id="0f363-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span><span class="sxs-lookup"><span data-stu-id="0f363-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span></span>

    <span data-ttu-id="0f363-140">Au lieu de cela, vous pouvez utiliser directement hello présélection contextes dans votre application.</span><span class="sxs-lookup"><span data-stu-id="0f363-140">Instead, you can directly use hello preset contexts in your application.</span></span>

- <span data-ttu-id="0f363-141">**Commandes magiques de cellule**.</span><span class="sxs-lookup"><span data-stu-id="0f363-141">**Cell magics**.</span></span> <span data-ttu-id="0f363-142">Hello PySpark noyau fournit certaines prédéfinies « magics », qui sont des commandes spéciales que vous pouvez appeler avec `%%` (par exemple, `%%MAGIC` <args>).</span><span class="sxs-lookup"><span data-stu-id="0f363-142">hello PySpark kernel provides some predefined “magics”, which are special commands that you can call with `%%` (for example, `%%MAGIC` <args>).</span></span> <span data-ttu-id="0f363-143">commande magic Hello doit être hello premier mot d’une cellule de code et de plusieurs lignes de contenu.</span><span class="sxs-lookup"><span data-stu-id="0f363-143">hello magic command must be hello first word in a code cell and allow for multiple lines of content.</span></span> <span data-ttu-id="0f363-144">word magique de Hello doit être hello premier mot de cellule de hello.</span><span class="sxs-lookup"><span data-stu-id="0f363-144">hello magic word should be hello first word in hello cell.</span></span> <span data-ttu-id="0f363-145">Ajout de quoi que ce soit avant magic hello, même des commentaires, provoque une erreur.</span><span class="sxs-lookup"><span data-stu-id="0f363-145">Adding anything before hello magic, even comments, causes an error.</span></span>     <span data-ttu-id="0f363-146">Pour plus d’informations sur les commandes magiques, cliquez [ici](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span><span class="sxs-lookup"><span data-stu-id="0f363-146">For more information on magics, see [here](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span></span>
   
    <span data-ttu-id="0f363-147">Hello tableau suivant répertorie les magics différents de hello disponibles via les noyaux hello.</span><span class="sxs-lookup"><span data-stu-id="0f363-147">hello following table lists hello different magics available through hello kernels.</span></span>

   | <span data-ttu-id="0f363-148">Commande magique</span><span class="sxs-lookup"><span data-stu-id="0f363-148">Magic</span></span> | <span data-ttu-id="0f363-149">Exemple</span><span class="sxs-lookup"><span data-stu-id="0f363-149">Example</span></span> | <span data-ttu-id="0f363-150">Description</span><span class="sxs-lookup"><span data-stu-id="0f363-150">Description</span></span> |
   | --- | --- | --- |
   | <span data-ttu-id="0f363-151">help</span><span class="sxs-lookup"><span data-stu-id="0f363-151">help</span></span> |`%%help` |<span data-ttu-id="0f363-152">Génère une table de tous les magics disponibles hello avec exemple et la description</span><span class="sxs-lookup"><span data-stu-id="0f363-152">Generates a table of all hello available magics with example and description</span></span> |
   | <span data-ttu-id="0f363-153">info</span><span class="sxs-lookup"><span data-stu-id="0f363-153">info</span></span> |`%%info` |<span data-ttu-id="0f363-154">Génère des informations de session pour le point de terminaison Livy actuel hello</span><span class="sxs-lookup"><span data-stu-id="0f363-154">Outputs session information for hello current Livy endpoint</span></span> |
   | <span data-ttu-id="0f363-155">CONFIGURER</span><span class="sxs-lookup"><span data-stu-id="0f363-155">configure</span></span> |`%%configure -f`<br><span data-ttu-id="0f363-156">`{"executorMemory": "1000M"`</span><span class="sxs-lookup"><span data-stu-id="0f363-156">`{"executorMemory": "1000M"`,</span></span><br><span data-ttu-id="0f363-157">`"executorCores": 4`}</span><span class="sxs-lookup"><span data-stu-id="0f363-157">`"executorCores": 4`}</span></span> |<span data-ttu-id="0f363-158">Configure les paramètres hello pour la création d’une session.</span><span class="sxs-lookup"><span data-stu-id="0f363-158">Configures hello parameters for creating a session.</span></span> <span data-ttu-id="0f363-159">Hello indicateur force (-f) est obligatoire si une session a déjà été créée, ce qui garantit la session hello est supprimé et recréé.</span><span class="sxs-lookup"><span data-stu-id="0f363-159">hello force flag (-f) is mandatory if a session has already been created, which ensures that hello session is dropped and recreated.</span></span> <span data-ttu-id="0f363-160">Consultez la section [POST /sessions Request Body de Livy](https://github.com/cloudera/livy#request-body) pour obtenir la liste des paramètres valides.</span><span class="sxs-lookup"><span data-stu-id="0f363-160">Look at [Livy's POST /sessions Request Body](https://github.com/cloudera/livy#request-body) for a list of valid parameters.</span></span> <span data-ttu-id="0f363-161">Paramètres doivent être passées comme une chaîne JSON et doivent être sur la ligne suivante de hello après magique de hello, comme indiqué dans l’exemple, la colonne hello.</span><span class="sxs-lookup"><span data-stu-id="0f363-161">Parameters must be passed in as a JSON string and must be on hello next line after hello magic, as shown in hello example column.</span></span> |
   | <span data-ttu-id="0f363-162">sql</span><span class="sxs-lookup"><span data-stu-id="0f363-162">sql</span></span> |`%%sql -o <variable name>`<br> `SHOW TABLES` |<span data-ttu-id="0f363-163">Exécute une requête Hive sur hello sqlContext.</span><span class="sxs-lookup"><span data-stu-id="0f363-163">Executes a Hive query against hello sqlContext.</span></span> <span data-ttu-id="0f363-164">Si hello `-o` paramètre est passé, résultat hello de requête de hello est conservée dans hello %% contexte Python local en tant qu’un [Pandas](http://pandas.pydata.org/) trame de données.</span><span class="sxs-lookup"><span data-stu-id="0f363-164">If hello `-o` parameter is passed, hello result of hello query is persisted in hello %%local Python context as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> |
   | <span data-ttu-id="0f363-165">local</span><span class="sxs-lookup"><span data-stu-id="0f363-165">local</span></span> |`%%local`<br>`a=1` |<span data-ttu-id="0f363-166">Tout code hello dans les lignes suivantes est exécuté localement.</span><span class="sxs-lookup"><span data-stu-id="0f363-166">All hello code in subsequent lines is executed locally.</span></span> <span data-ttu-id="0f363-167">Code doit être un code Python2 valide même indépendamment de noyau hello que vous utilisez.</span><span class="sxs-lookup"><span data-stu-id="0f363-167">Code must be valid Python2 code even irrespective of hello kernel you are using.</span></span> <span data-ttu-id="0f363-168">C’est le cas, même si vous avez sélectionné **PySpark3** ou **Spark** noyaux lors de la création d’ordinateur portable hello, si vous utilisez hello `%%local` magique dans une cellule, cette cellule doit seulement avoir un code Python2 valide...</span><span class="sxs-lookup"><span data-stu-id="0f363-168">So, even if you selected **PySpark3** or **Spark** kernels while creating hello notebook, if you use hello `%%local` magic in a cell, that cell must only have valid Python2 code..</span></span> |
   | <span data-ttu-id="0f363-169">journaux</span><span class="sxs-lookup"><span data-stu-id="0f363-169">logs</span></span> |`%%logs` |<span data-ttu-id="0f363-170">Sorties hello des journaux de session de Livy hello en cours.</span><span class="sxs-lookup"><span data-stu-id="0f363-170">Outputs hello logs for hello current Livy session.</span></span> |
   | <span data-ttu-id="0f363-171">delete</span><span class="sxs-lookup"><span data-stu-id="0f363-171">delete</span></span> |`%%delete -f -s <session number>` |<span data-ttu-id="0f363-172">Supprime une session spécifique de point de terminaison Livy actuel hello.</span><span class="sxs-lookup"><span data-stu-id="0f363-172">Deletes a specific session of hello current Livy endpoint.</span></span> <span data-ttu-id="0f363-173">Notez que vous ne peut pas supprimer la session de hello qui est initiée pour noyau hello lui-même.</span><span class="sxs-lookup"><span data-stu-id="0f363-173">Note that you cannot delete hello session that is initiated for hello kernel itself.</span></span> |
   | <span data-ttu-id="0f363-174">cleanup</span><span class="sxs-lookup"><span data-stu-id="0f363-174">cleanup</span></span> |`%%cleanup -f` |<span data-ttu-id="0f363-175">Supprime toutes les sessions de hello pour hello Livy point de terminaison actuel, y compris la session de cet ordinateur portable.</span><span class="sxs-lookup"><span data-stu-id="0f363-175">Deletes all hello sessions for hello current Livy endpoint, including this notebook's session.</span></span> <span data-ttu-id="0f363-176">indicateur de force Hello -f est obligatoire.</span><span class="sxs-lookup"><span data-stu-id="0f363-176">hello force flag -f is mandatory.</span></span> |

   > [!NOTE]
   > <span data-ttu-id="0f363-177">En outre toohello magics ajouté par le noyau de PySpark hello, vous pouvez également utiliser hello [intégrés IPython magics](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), y compris `%%sh`.</span><span class="sxs-lookup"><span data-stu-id="0f363-177">In addition toohello magics added by hello PySpark kernel, you can also use hello [built-in IPython magics](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), including `%%sh`.</span></span> <span data-ttu-id="0f363-178">Vous pouvez utiliser hello `%%sh` magique toorun scripts et bloc de code sur le nœud principal du cluster hello.</span><span class="sxs-lookup"><span data-stu-id="0f363-178">You can use hello `%%sh` magic toorun scripts and block of code on hello cluster headnode.</span></span>
   >
   >
2. <span data-ttu-id="0f363-179">**Visualisation automatique**.</span><span class="sxs-lookup"><span data-stu-id="0f363-179">**Auto visualization**.</span></span> <span data-ttu-id="0f363-180">Hello **Pyspark** noyau visualise automatiquement sortie hello de ruche et les requêtes SQL.</span><span class="sxs-lookup"><span data-stu-id="0f363-180">hello **Pyspark** kernel automatically visualizes hello output of Hive and SQL queries.</span></span> <span data-ttu-id="0f363-181">Vous pouvez choisir entre plusieurs types de visualisations, notamment tableau, secteurs, courbes, aires et barres.</span><span class="sxs-lookup"><span data-stu-id="0f363-181">You can choose between several different types of visualizations including Table, Pie, Line, Area, Bar.</span></span>

## <a name="parameters-supported-with-hello-sql-magic"></a><span data-ttu-id="0f363-182">Paramètres pris en charge par hello %% magique de sql</span><span class="sxs-lookup"><span data-stu-id="0f363-182">Parameters supported with hello %%sql magic</span></span>
<span data-ttu-id="0f363-183">Hello `%%sql` magique prend en charge des paramètres différents que vous pouvez utiliser le type de hello toocontrol de sortie que vous recevez lorsque vous exécutez des requêtes.</span><span class="sxs-lookup"><span data-stu-id="0f363-183">hello `%%sql` magic supports different parameters that you can use toocontrol hello kind of output that you receive when you run queries.</span></span> <span data-ttu-id="0f363-184">Hello tableau suivant répertorie la sortie de hello.</span><span class="sxs-lookup"><span data-stu-id="0f363-184">hello following table lists hello output.</span></span>

| <span data-ttu-id="0f363-185">Paramètre</span><span class="sxs-lookup"><span data-stu-id="0f363-185">Parameter</span></span> | <span data-ttu-id="0f363-186">Exemple</span><span class="sxs-lookup"><span data-stu-id="0f363-186">Example</span></span> | <span data-ttu-id="0f363-187">Description</span><span class="sxs-lookup"><span data-stu-id="0f363-187">Description</span></span> |
| --- | --- | --- |
| <span data-ttu-id="0f363-188">-o</span><span class="sxs-lookup"><span data-stu-id="0f363-188">-o</span></span> |`-o <VARIABLE NAME>` |<span data-ttu-id="0f363-189">Utilisez le paramètre toopersist hello le résultat de requête de hello, Bonjour %% contexte local de Python, comme un [Pandas](http://pandas.pydata.org/) trame de données.</span><span class="sxs-lookup"><span data-stu-id="0f363-189">Use this parameter toopersist hello result of hello query, in hello %%local Python context, as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> <span data-ttu-id="0f363-190">nom de Hello de variable de trame de données hello est nom de variable hello que vous spécifiez.</span><span class="sxs-lookup"><span data-stu-id="0f363-190">hello name of hello dataframe variable is hello variable name you specify.</span></span> |
| <span data-ttu-id="0f363-191">-q</span><span class="sxs-lookup"><span data-stu-id="0f363-191">-q</span></span> |`-q` |<span data-ttu-id="0f363-192">Utilisez cette tooturn off visualisations pour la cellule de hello.</span><span class="sxs-lookup"><span data-stu-id="0f363-192">Use this tooturn off visualizations for hello cell.</span></span> <span data-ttu-id="0f363-193">Si vous ne souhaitez pas tooauto-visualiser le contenu d’une cellule hello et souhaitez toocapture sous la forme d’une trame de données, puis d’utiliser `-q -o <VARIABLE>`.</span><span class="sxs-lookup"><span data-stu-id="0f363-193">If you don't want tooauto-visualize hello content of a cell and just want toocapture it as a dataframe, then use `-q -o <VARIABLE>`.</span></span> <span data-ttu-id="0f363-194">Si vous souhaitez tooturn off visualisations sans capturer les résultats hello (par exemple, pour exécuter une requête SQL, comme un `CREATE TABLE` instruction), utilisez `-q` sans spécifier un `-o` argument.</span><span class="sxs-lookup"><span data-stu-id="0f363-194">If you want tooturn off visualizations without capturing hello results (for example, for running a SQL query, like a `CREATE TABLE` statement), use `-q` without specifying a `-o` argument.</span></span> |
| <span data-ttu-id="0f363-195">-m</span><span class="sxs-lookup"><span data-stu-id="0f363-195">-m</span></span> |`-m <METHOD>` |<span data-ttu-id="0f363-196">**METHOD** prend la valeur **take** ou **sample** (**take** est la valeur par défaut).</span><span class="sxs-lookup"><span data-stu-id="0f363-196">Where **METHOD** is either **take** or **sample** (default is **take**).</span></span> <span data-ttu-id="0f363-197">Si la méthode hello est **prendre**, noyau de hello récupère les éléments de haut hello hello résultats du jeu de données spécifié par le nombre maximal de lignes (décrite plus loin dans ce tableau).</span><span class="sxs-lookup"><span data-stu-id="0f363-197">If hello method is **take**, hello kernel picks elements from hello top of hello result data set specified by MAXROWS (described later in this table).</span></span> <span data-ttu-id="0f363-198">Si la méthode hello est **exemple**, éléments hello du jeu de données en fonction de trop échantillonne de noyau de hello`-r` paramètre décrit ci-après dans cette table.</span><span class="sxs-lookup"><span data-stu-id="0f363-198">If hello method is **sample**, hello kernel randomly samples elements of hello data set according too`-r` parameter, described next in this table.</span></span> |
| <span data-ttu-id="0f363-199">-r</span><span class="sxs-lookup"><span data-stu-id="0f363-199">-r</span></span> |`-r <FRACTION>` |<span data-ttu-id="0f363-200">Ici, **FRACTION** est un nombre à virgule flottante compris entre 0,0 et 1,0.</span><span class="sxs-lookup"><span data-stu-id="0f363-200">Here **FRACTION** is a floating-point number between 0.0 and 1.0.</span></span> <span data-ttu-id="0f363-201">Si la méthode d’échantillonnage hello pour la requête SQL hello est `sample`, puis le noyau de hello échantillonne fraction spécifiée de hello d’éléments hello hello du jeu de résultats pour vous.</span><span class="sxs-lookup"><span data-stu-id="0f363-201">If hello sample method for hello SQL query is `sample`, then hello kernel randomly samples hello specified fraction of hello elements of hello result set for you.</span></span> <span data-ttu-id="0f363-202">Par exemple, si vous exécutez une requête SQL avec des arguments de hello `-m sample -r 0.01`, 1 % des lignes de résultat hello sont échantillonnées aléatoirement.</span><span class="sxs-lookup"><span data-stu-id="0f363-202">For example, if you run a SQL query with hello arguments `-m sample -r 0.01`, then 1% of hello result rows are randomly sampled.</span></span> |
| -n |`-n <MAXROWS>` |<span data-ttu-id="0f363-203">**MAXROWS** est une valeur entière.</span><span class="sxs-lookup"><span data-stu-id="0f363-203">**MAXROWS** is an integer value.</span></span> <span data-ttu-id="0f363-204">noyau de Hello limite le nombre de hello de lignes de sortie trop**MAXROWS**.</span><span class="sxs-lookup"><span data-stu-id="0f363-204">hello kernel limits hello number of output rows too**MAXROWS**.</span></span> <span data-ttu-id="0f363-205">Si **MAXROWS** est un nombre négatif comme **-1**, nombre de hello de lignes dans le jeu de résultats hello n’est pas limité.</span><span class="sxs-lookup"><span data-stu-id="0f363-205">If **MAXROWS** is a negative number such as **-1**, then hello number of rows in hello result set is not limited.</span></span> |

<span data-ttu-id="0f363-206">**Exemple :**</span><span class="sxs-lookup"><span data-stu-id="0f363-206">**Example:**</span></span>

    %%sql -q -m sample -r 0.1 -n 500 -o query2
    SELECT * FROM hivesampletable

<span data-ttu-id="0f363-207">instruction Hello ci-dessus hello suivant :</span><span class="sxs-lookup"><span data-stu-id="0f363-207">hello statement above does hello following:</span></span>

* <span data-ttu-id="0f363-208">Elle sélectionne tous les enregistrements présents dans **hivesampletable**.</span><span class="sxs-lookup"><span data-stu-id="0f363-208">Selects all records from **hivesampletable**.</span></span>
* <span data-ttu-id="0f363-209">Comme nous utilisons le paramètre - q, elle désactive la visualisation automatique.</span><span class="sxs-lookup"><span data-stu-id="0f363-209">Because we use -q, it turns off auto-visualization.</span></span>
* <span data-ttu-id="0f363-210">Étant donné que nous utilisons `-m sample -r 0.1 -n 500` il échantillonne de 10 % des lignes hello dans hello hivesampletable et limites hello taille hello ensemble too500 de lignes de résultats.</span><span class="sxs-lookup"><span data-stu-id="0f363-210">Because we use `-m sample -r 0.1 -n 500` it randomly samples 10% of hello rows in hello hivesampletable and limits hello size of hello result set too500 rows.</span></span>
* <span data-ttu-id="0f363-211">Enfin, étant donné que nous avons utilisé `-o query2` il enregistre également une sortie de hello dans une trame de données appelée **requête2**.</span><span class="sxs-lookup"><span data-stu-id="0f363-211">Finally, because we used `-o query2` it also saves hello output into a dataframe called **query2**.</span></span>

## <a name="considerations-while-using-hello-new-kernels"></a><span data-ttu-id="0f363-212">Considérations lors de l’utilisation de noyaux hello</span><span class="sxs-lookup"><span data-stu-id="0f363-212">Considerations while using hello new kernels</span></span>

<span data-ttu-id="0f363-213">Quelle que soit la noyau que vous utilisez, en laissant les blocs-notes hello en cours d’exécution consomme des ressources de cluster hello.</span><span class="sxs-lookup"><span data-stu-id="0f363-213">Whichever kernel you use, leaving hello notebooks running consumes hello cluster resources.</span></span>  <span data-ttu-id="0f363-214">Ces noyaux, car les contextes de hello sont prédéfinis, simplement en cours de fermeture blocs-notes de hello ne pas tuer le contexte de hello et par conséquent, les ressources de cluster hello continueront toobe en cours d’utilisation.</span><span class="sxs-lookup"><span data-stu-id="0f363-214">With these kernels, because hello contexts are preset, simply exiting hello notebooks does not kill hello context and hence hello cluster resources continue toobe in use.</span></span> <span data-ttu-id="0f363-215">Il est conseillé de toouse hello **fermer et s’arrêter** option à partir de l’ordinateur portable hello **fichier** menu lorsque vous avez terminé d’utiliser le bloc-notes hello, qui supprime le contexte de hello et puis se ferme hello bloc-notes.</span><span class="sxs-lookup"><span data-stu-id="0f363-215">A good practice is toouse hello **Close and Halt** option from hello notebook's **File** menu when you are finished using hello notebook, which kills hello context and then exits hello notebook.</span></span>     

## <a name="show-me-some-examples"></a><span data-ttu-id="0f363-216">Voici quelques exemples :</span><span class="sxs-lookup"><span data-stu-id="0f363-216">Show me some examples</span></span>

<span data-ttu-id="0f363-217">Lorsque vous ouvrez un bloc-notes jupyter, vous voyez deux dossiers disponibles au niveau racine de hello.</span><span class="sxs-lookup"><span data-stu-id="0f363-217">When you open a Jupyter notebook, you see two folders available at hello root level.</span></span>

* <span data-ttu-id="0f363-218">Hello **PySpark** dossier a blocs-notes de l’exemple hello de cette utilisation new **Python** noyau.</span><span class="sxs-lookup"><span data-stu-id="0f363-218">hello **PySpark** folder has sample notebooks that use hello new **Python** kernel.</span></span>
* <span data-ttu-id="0f363-219">Hello **Scala** dossier a blocs-notes de l’exemple hello de cette utilisation new **Spark** noyau.</span><span class="sxs-lookup"><span data-stu-id="0f363-219">hello **Scala** folder has sample notebooks that use hello new **Spark** kernel.</span></span>

<span data-ttu-id="0f363-220">Vous pouvez ouvrir hello **00 - [lecture ME première] fonctionnalités de noyau Spark Magic** bloc-notes de hello **PySpark** ou **Spark** toolearn dossier sur magics différents de hello disponibles.</span><span class="sxs-lookup"><span data-stu-id="0f363-220">You can open hello **00 - [READ ME FIRST] Spark Magic Kernel Features** notebook from hello **PySpark** or **Spark** folder toolearn about hello different magics available.</span></span> <span data-ttu-id="0f363-221">Vous pouvez également utiliser hello autres blocs-notes exemple disponibles sous hello deux dossiers toolearn comment tooachieve différents scénarios à l’aide du bloc-notes portables avec les clusters HDInsight Spark.</span><span class="sxs-lookup"><span data-stu-id="0f363-221">You can also use hello other sample notebooks available under hello two folders toolearn how tooachieve different scenarios using Jupyter notebooks with HDInsight Spark clusters.</span></span>

## <a name="where-are-hello-notebooks-stored"></a><span data-ttu-id="0f363-222">Où sont stockées les blocs-notes de hello ?</span><span class="sxs-lookup"><span data-stu-id="0f363-222">Where are hello notebooks stored?</span></span>

<span data-ttu-id="0f363-223">Notebook portables sont enregistrés toohello compte de stockage associé au cluster hello sous hello **/HdiNotebooks** dossier.</span><span class="sxs-lookup"><span data-stu-id="0f363-223">Jupyter notebooks are saved toohello storage account associated with hello cluster under hello **/HdiNotebooks** folder.</span></span>  <span data-ttu-id="0f363-224">Ordinateurs portables, des fichiers texte et des dossiers que vous créez à partir de Notebook sont accessibles à partir du compte de stockage hello.</span><span class="sxs-lookup"><span data-stu-id="0f363-224">Notebooks, text files, and folders that you create from within Jupyter are accessible from hello storage account.</span></span>  <span data-ttu-id="0f363-225">Par exemple, si vous utilisez Notebook toocreate un dossier **MonDossier** et un ordinateur portable **myfolder/mynotebook.ipynb**, vous pouvez accéder à cet ordinateur portable à `/HdiNotebooks/myfolder/mynotebook.ipynb` dans le compte de stockage hello.</span><span class="sxs-lookup"><span data-stu-id="0f363-225">For example, if you use Jupyter toocreate a folder **myfolder** and a notebook **myfolder/mynotebook.ipynb**, you can access that notebook at `/HdiNotebooks/myfolder/mynotebook.ipynb` within hello storage account.</span></span>  <span data-ttu-id="0f363-226">Hello inverse est également vrai, autrement dit, si vous téléchargez un bloc-notes directement du compte de stockage de tooyour à `/HdiNotebooks/mynotebook1.ipynb`, portable de hello est également visible dans le bloc-notes.</span><span class="sxs-lookup"><span data-stu-id="0f363-226">hello reverse is also true, that is, if you upload a notebook directly tooyour storage account at `/HdiNotebooks/mynotebook1.ipynb`, hello notebook is visible from Jupyter as well.</span></span>  <span data-ttu-id="0f363-227">Ordinateurs portables restent dans le compte de stockage hello même après la suppression de cluster de hello.</span><span class="sxs-lookup"><span data-stu-id="0f363-227">Notebooks remain in hello storage account even after hello cluster is deleted.</span></span>

<span data-ttu-id="0f363-228">méthode Hello portables sont enregistrées de compte de stockage toohello est compatible avec HDFS.</span><span class="sxs-lookup"><span data-stu-id="0f363-228">hello way notebooks are saved toohello storage account is compatible with HDFS.</span></span> <span data-ttu-id="0f363-229">Par conséquent, si vous SSH en cluster hello que vous pouvez utiliser fichier de commandes de gestion comme indiqué dans hello suivant extrait de code :</span><span class="sxs-lookup"><span data-stu-id="0f363-229">So, if you SSH into hello cluster you can use file management commands as shown in hello following snippet:</span></span>

    hdfs dfs -ls /HdiNotebooks                               # List everything at hello root directory – everything in this directory is visible tooJupyter from hello home page
    hdfs dfs –copyToLocal /HdiNotebooks                    # Download hello contents of hello HdiNotebooks folder
    hdfs dfs –copyFromLocal example.ipynb /HdiNotebooks   # Upload a notebook example.ipynb toohello root folder so it’s visible from Jupyter


<span data-ttu-id="0f363-230">En cas de problèmes d’accès du compte de stockage hello pour le cluster de hello, ordinateurs portables de hello sont également enregistrées sur le nœud principal de hello `/var/lib/jupyter`.</span><span class="sxs-lookup"><span data-stu-id="0f363-230">In case there are issues accessing hello storage account for hello cluster, hello notebooks are also saved on hello headnode `/var/lib/jupyter`.</span></span>

## <a name="supported-browser"></a><span data-ttu-id="0f363-231">Navigateur pris en charge</span><span class="sxs-lookup"><span data-stu-id="0f363-231">Supported browser</span></span>

<span data-ttu-id="0f363-232">Les blocs-notes Jupyter sur clusters Spark HDInsight sont pris en charge uniquement sur Google Chrome.</span><span class="sxs-lookup"><span data-stu-id="0f363-232">Jupyter notebooks on Spark HDInsight clusters are supported only on Google Chrome.</span></span>

## <a name="feedback"></a><span data-ttu-id="0f363-233">Commentaires</span><span class="sxs-lookup"><span data-stu-id="0f363-233">Feedback</span></span>
<span data-ttu-id="0f363-234">noyaux Hello sont en pleine évolution étape et arrivent au fil du temps.</span><span class="sxs-lookup"><span data-stu-id="0f363-234">hello new kernels are in evolving stage and will mature over time.</span></span> <span data-ttu-id="0f363-235">Les API pourront également être amenés à évoluer au fur et à mesure des évolutions des noyaux.</span><span class="sxs-lookup"><span data-stu-id="0f363-235">This could also mean that APIs could change as these kernels mature.</span></span> <span data-ttu-id="0f363-236">Nous aimerions recevoir vos commentaires concernant l'utilisation de ces nouveaux noyaux.</span><span class="sxs-lookup"><span data-stu-id="0f363-236">We would appreciate any feedback that you have while using these new kernels.</span></span> <span data-ttu-id="0f363-237">Cela est utile dans la mise en forme de la version finale de hello de ces noyaux.</span><span class="sxs-lookup"><span data-stu-id="0f363-237">This is useful in shaping hello final release of these kernels.</span></span> <span data-ttu-id="0f363-238">Vous pouvez laisser vos commentaires/commentaires sous hello **commentaires** section bas hello de cet article.</span><span class="sxs-lookup"><span data-stu-id="0f363-238">You can leave your comments/feedback under hello **Comments** section at hello bottom of this article.</span></span>

## <span data-ttu-id="0f363-239"><a name="seealso"></a>Voir aussi</span><span class="sxs-lookup"><span data-stu-id="0f363-239"><a name="seealso"></a>See also</span></span>
* [<span data-ttu-id="0f363-240">Vue d’ensemble : Apache Spark sur Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="0f363-240">Overview: Apache Spark on Azure HDInsight</span></span>](hdinsight-apache-spark-overview.md)

### <a name="scenarios"></a><span data-ttu-id="0f363-241">Scénarios</span><span class="sxs-lookup"><span data-stu-id="0f363-241">Scenarios</span></span>
* [<span data-ttu-id="0f363-242">Spark avec BI : effectuez une analyse interactive des données à l’aide de Spark dans HDInsight avec des outils BI</span><span class="sxs-lookup"><span data-stu-id="0f363-242">Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools</span></span>](hdinsight-apache-spark-use-bi-tools.md)
* [<span data-ttu-id="0f363-243">Spark avec Machine Learning : utilisez Spark dans HDInsight pour l’analyse de la température des bâtiments à l’aide des données des systèmes HVAC</span><span class="sxs-lookup"><span data-stu-id="0f363-243">Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data</span></span>](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [<span data-ttu-id="0f363-244">Spark avec Machine Learning : Spark utilisation dans résultats de l’inspection alimentaires toopredict HDInsight</span><span class="sxs-lookup"><span data-stu-id="0f363-244">Spark with Machine Learning: Use Spark in HDInsight toopredict food inspection results</span></span>](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [<span data-ttu-id="0f363-245">Streaming Spark : Utiliser Spark dans HDInsight pour créer des applications de diffusion en continu en temps réel</span><span class="sxs-lookup"><span data-stu-id="0f363-245">Spark Streaming: Use Spark in HDInsight for building real-time streaming applications</span></span>](hdinsight-apache-spark-eventhub-streaming.md)
* [<span data-ttu-id="0f363-246">Analyse des journaux de site web à l’aide de Spark dans HDInsight</span><span class="sxs-lookup"><span data-stu-id="0f363-246">Website log analysis using Spark in HDInsight</span></span>](hdinsight-apache-spark-custom-library-website-log-analysis.md)

### <a name="create-and-run-applications"></a><span data-ttu-id="0f363-247">Créer et exécuter des applications</span><span class="sxs-lookup"><span data-stu-id="0f363-247">Create and run applications</span></span>
* [<span data-ttu-id="0f363-248">Créer une application autonome avec Scala</span><span class="sxs-lookup"><span data-stu-id="0f363-248">Create a standalone application using Scala</span></span>](hdinsight-apache-spark-create-standalone-application.md)
* [<span data-ttu-id="0f363-249">Exécuter des tâches à distance avec Livy sur un cluster Spark</span><span class="sxs-lookup"><span data-stu-id="0f363-249">Run jobs remotely on a Spark cluster using Livy</span></span>](hdinsight-apache-spark-livy-rest-interface.md)

### <a name="tools-and-extensions"></a><span data-ttu-id="0f363-250">Outils et extensions</span><span class="sxs-lookup"><span data-stu-id="0f363-250">Tools and extensions</span></span>
* [<span data-ttu-id="0f363-251">Utiliser le plug-in des outils HDInsight pour IntelliJ idée toocreate et soumettre des applications de Spark Scala</span><span class="sxs-lookup"><span data-stu-id="0f363-251">Use HDInsight Tools Plugin for IntelliJ IDEA toocreate and submit Spark Scala applications</span></span>](hdinsight-apache-spark-intellij-tool-plugin.md)
* [<span data-ttu-id="0f363-252">Utiliser des plug-in des outils HDInsight pour les applications de Spark toodebug IntelliJ idée à distance</span><span class="sxs-lookup"><span data-stu-id="0f363-252">Use HDInsight Tools Plugin for IntelliJ IDEA toodebug Spark applications remotely</span></span>](hdinsight-apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)
* [<span data-ttu-id="0f363-253">Utiliser des bloc-notes Zeppelin avec un cluster Spark sur HDInsight</span><span class="sxs-lookup"><span data-stu-id="0f363-253">Use Zeppelin notebooks with a Spark cluster on HDInsight</span></span>](hdinsight-apache-spark-zeppelin-notebook.md)
* [<span data-ttu-id="0f363-254">Utiliser des packages externes avec les blocs-notes Jupyter</span><span class="sxs-lookup"><span data-stu-id="0f363-254">Use external packages with Jupyter notebooks</span></span>](hdinsight-apache-spark-jupyter-notebook-use-external-packages.md)
* [<span data-ttu-id="0f363-255">Installer Notebook sur votre ordinateur et vous connecter tooan cluster HDInsight Spark</span><span class="sxs-lookup"><span data-stu-id="0f363-255">Install Jupyter on your computer and connect tooan HDInsight Spark cluster</span></span>](hdinsight-apache-spark-jupyter-notebook-install-locally.md)

### <a name="manage-resources"></a><span data-ttu-id="0f363-256">Gestion des ressources</span><span class="sxs-lookup"><span data-stu-id="0f363-256">Manage resources</span></span>
* [<span data-ttu-id="0f363-257">Gérer les ressources de cluster d’Apache Spark hello dans Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="0f363-257">Manage resources for hello Apache Spark cluster in Azure HDInsight</span></span>](hdinsight-apache-spark-resource-manager.md)
* [<span data-ttu-id="0f363-258">Track and debug jobs running on an Apache Spark cluster in HDInsight (Suivi et débogage des tâches en cours d’exécution sur un cluster Apache Spark dans HDInsight)</span><span class="sxs-lookup"><span data-stu-id="0f363-258">Track and debug jobs running on an Apache Spark cluster in HDInsight</span></span>](hdinsight-apache-spark-job-debugging.md)
