---
title: "Didacticiel Data Factory : premier pipeline de données | Microsoft Docs"
description: "Ce didacticiel Azure Data Factory vous montre comment créer et planifier une fabrique de données qui traite les données à l’aide du script Hive sur un cluster Hadoop."
services: data-factory
keywords: didacticiel azure data factory, cluster hadoop, hadoop hive
documentationcenter: 
author: spelluru
manager: jhubbard
editor: 
ms.assetid: 81f36c76-6e78-4d93-a3f2-0317b413f1d0
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 07/10/2017
ms.author: spelluru
ms.openlocfilehash: 08e2988d455cca21726162d9fb128e91fd51f463
ms.sourcegitcommit: 18ad9bc049589c8e44ed277f8f43dcaa483f3339
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 08/29/2017
---
# <a name="tutorial-build-your-first-pipeline-to-transform-data-using-hadoop-cluster"></a><span data-ttu-id="48936-104">Didacticiel : Générer votre premier pipeline pour transformer les données à l’aide du cluster Hadoop</span><span class="sxs-lookup"><span data-stu-id="48936-104">Tutorial: Build your first pipeline to transform data using Hadoop cluster</span></span>
> [!div class="op_single_selector"]
> * [<span data-ttu-id="48936-105">Vue d’ensemble et étapes préalables requises</span><span class="sxs-lookup"><span data-stu-id="48936-105">Overview and prerequisites</span></span>](data-factory-build-your-first-pipeline.md)
> * [<span data-ttu-id="48936-106">Portail Azure</span><span class="sxs-lookup"><span data-stu-id="48936-106">Azure portal</span></span>](data-factory-build-your-first-pipeline-using-editor.md)
> * [<span data-ttu-id="48936-107">Visual Studio</span><span class="sxs-lookup"><span data-stu-id="48936-107">Visual Studio</span></span>](data-factory-build-your-first-pipeline-using-vs.md)
> * [<span data-ttu-id="48936-108">PowerShell</span><span class="sxs-lookup"><span data-stu-id="48936-108">PowerShell</span></span>](data-factory-build-your-first-pipeline-using-powershell.md)
> * [<span data-ttu-id="48936-109">modèle Azure Resource Manager</span><span class="sxs-lookup"><span data-stu-id="48936-109">Resource Manager template</span></span>](data-factory-build-your-first-pipeline-using-arm.md)
> * [<span data-ttu-id="48936-110">API REST</span><span class="sxs-lookup"><span data-stu-id="48936-110">REST API</span></span>](data-factory-build-your-first-pipeline-using-rest-api.md)

<span data-ttu-id="48936-111">Dans ce didacticiel, vous allez générer votre première fabrique de données Azure avec un pipeline de données.</span><span class="sxs-lookup"><span data-stu-id="48936-111">In this tutorial, you build your first Azure data factory with a data pipeline.</span></span> <span data-ttu-id="48936-112">Le pipeline transforme les données d’entrée en exécutant un script Hive sur un cluster Azure HDInsight (Hadoop) pour produire des données de sortie.</span><span class="sxs-lookup"><span data-stu-id="48936-112">The pipeline transforms input data by running Hive script on an Azure HDInsight (Hadoop) cluster to produce output data.</span></span>  

<span data-ttu-id="48936-113">Cet article fournit une vue d’ensemble et la configuration requise pour le didacticiel.</span><span class="sxs-lookup"><span data-stu-id="48936-113">This article provides overview and prerequisites for the tutorial.</span></span> <span data-ttu-id="48936-114">Si vous disposez de tout ce qui est nécessaire, vous pouvez suivre le didacticiel en utilisant les outils/Kits de développement logiciel (SDK) ci-après : Portail Azure, Visual Studio, PowerShell, modèle Resource Manager, API REST.</span><span class="sxs-lookup"><span data-stu-id="48936-114">After you complete the prerequisites, you can do the tutorial using one of the following tools/SDKs: Azure portal, Visual Studio, PowerShell, Resource Manager template, REST API.</span></span> <span data-ttu-id="48936-115">Sélectionnez l’une des options de la liste déroulante au début ou les liens à la fin de cet article pour suivre ce didacticiel en utilisant l’une ou l’autre des possibilités.</span><span class="sxs-lookup"><span data-stu-id="48936-115">Select one of the options in the drop-down list at the beginning (or) links at the end of this article to do the tutorial using one of these options.</span></span>    

## <a name="tutorial-overview"></a><span data-ttu-id="48936-116">Vue d’ensemble du didacticiel</span><span class="sxs-lookup"><span data-stu-id="48936-116">Tutorial overview</span></span>
<span data-ttu-id="48936-117">Dans ce didacticiel, vous effectuerez les étapes suivantes :</span><span class="sxs-lookup"><span data-stu-id="48936-117">In this tutorial, you perform the following steps:</span></span>

1. <span data-ttu-id="48936-118">Création d'une **fabrique de données**.</span><span class="sxs-lookup"><span data-stu-id="48936-118">Create a **data factory**.</span></span> <span data-ttu-id="48936-119">Une fabrique de données peut contenir un ou plusieurs pipelines de données qui déplacent et transforment des données.</span><span class="sxs-lookup"><span data-stu-id="48936-119">A data factory can contain one or more data pipelines that move and transform data.</span></span> 

    <span data-ttu-id="48936-120">Dans ce didacticiel, vous créez un pipeline dans la fabrique de données.</span><span class="sxs-lookup"><span data-stu-id="48936-120">In this tutorial, you create one pipeline in the data factory.</span></span> 
2. <span data-ttu-id="48936-121">Création d'un **pipeline**.</span><span class="sxs-lookup"><span data-stu-id="48936-121">Create a **pipeline**.</span></span> <span data-ttu-id="48936-122">Un pipeline peut avoir une ou plusieurs activités (exemples : activité de copie, activité Hive HDInsight).</span><span class="sxs-lookup"><span data-stu-id="48936-122">A pipeline can have one or more activities (Examples: Copy Activity, HDInsight Hive Activity).</span></span> <span data-ttu-id="48936-123">Cet exemple utilise l’activité Hive d’HDInsight, qui exécute un script Hive sur un cluster HDInsight Hadoop.</span><span class="sxs-lookup"><span data-stu-id="48936-123">This sample uses the HDInsight Hive activity that runs a Hive script on a HDInsight Hadoop cluster.</span></span> <span data-ttu-id="48936-124">Le script crée d’abord une table qui fait référence aux données de journaux web bruts stockées dans le stockage d’objets blob Azure, puis partitionne les données brutes par année et par mois.</span><span class="sxs-lookup"><span data-stu-id="48936-124">The script first creates a table that references the raw web log data stored in Azure blob storage and then partitions the raw data by year and month.</span></span>

    <span data-ttu-id="48936-125">Dans ce didacticiel, le pipeline utilise l’activité Hive pour transformer les données en exécutant une requête Hive sur un cluster Azure HDInsight Hadoop.</span><span class="sxs-lookup"><span data-stu-id="48936-125">In this tutorial, the pipeline uses the Hive Activity to transform data by running a Hive query on an Azure HDInsight Hadoop cluster.</span></span> 
3. <span data-ttu-id="48936-126">Créer des **services liés**.</span><span class="sxs-lookup"><span data-stu-id="48936-126">Create **linked services**.</span></span> <span data-ttu-id="48936-127">Vous créez un service lié pour lier un magasin de données ou un service de calcul à la fabrique de données.</span><span class="sxs-lookup"><span data-stu-id="48936-127">You create a linked service to link a data store or a compute service to the data factory.</span></span> <span data-ttu-id="48936-128">Un magasin de données comme Azure Storage conserve les données d’entrée/de sortie d’activités dans le pipeline.</span><span class="sxs-lookup"><span data-stu-id="48936-128">A data store such as Azure Storage holds input/output data of activities in the pipeline.</span></span> <span data-ttu-id="48936-129">Un service de calcul comme un cluster HDInsight Hadoop traite/transforme des données.</span><span class="sxs-lookup"><span data-stu-id="48936-129">A compute service such as HDInsight Hadoop cluster processes/transforms data.</span></span>

    <span data-ttu-id="48936-130">Dans ce didacticiel, vous allez créer deux services liés : **Azure Storage** et **Azure HDInsight**.</span><span class="sxs-lookup"><span data-stu-id="48936-130">In this tutorial, you create two linked services: **Azure Storage** and **Azure HDInsight**.</span></span> <span data-ttu-id="48936-131">Le service lié du stockage Azure relie un compte de stockage Azure qui contient les données d’entrée/de sortie de la fabrique de données.</span><span class="sxs-lookup"><span data-stu-id="48936-131">The Azure Storage linked service links an Azure Storage Account that holds the input/output data to the data factory.</span></span> <span data-ttu-id="48936-132">Le service lié Azure HDInsight relie un cluster Azure HDInsight qui est utilisé pour transformer les données de la fabrique de données.</span><span class="sxs-lookup"><span data-stu-id="48936-132">Azure HDInsight linked service links an Azure HDInsight cluster that is used to transform data to the data factory.</span></span> 
3. <span data-ttu-id="48936-133">Créer des **jeux de données**d’entrée et de sortie.</span><span class="sxs-lookup"><span data-stu-id="48936-133">Create input and output **datasets**.</span></span> <span data-ttu-id="48936-134">Un jeu de données d’entrée représente l’entrée d’une activité dans le pipeline, tandis qu’un jeu de données de sortie représente la sortie de l’activité.</span><span class="sxs-lookup"><span data-stu-id="48936-134">An input dataset represents the input for an activity in the pipeline and an output dataset represents the output for the activity.</span></span>

    <span data-ttu-id="48936-135">Dans ce didacticiel, les jeux de données d’entrée et de sortie indiquent des emplacements de données d’entrée et de sortie dans le stockage Blob Azure.</span><span class="sxs-lookup"><span data-stu-id="48936-135">In this tutorial, the input and output datasets specify locations of input and output data in the Azure Blob Storage.</span></span> <span data-ttu-id="48936-136">Le service lié du stockage Azure indique quel compte de stockage Azure est utilisé.</span><span class="sxs-lookup"><span data-stu-id="48936-136">The Azure Storage linked service specifies what Azure Storage Account is used.</span></span> <span data-ttu-id="48936-137">Un jeu de données d’entrée spécifie l’emplacement des fichiers d’entrée, tandis qu’un jeu de données de sortie indique l’emplacement des fichiers de sortie.</span><span class="sxs-lookup"><span data-stu-id="48936-137">An input dataset specifies where the input files are located and an output dataset specifies where the output files are placed.</span></span> 


<span data-ttu-id="48936-138">Pour obtenir une présentation détaillée d’Azure Data Factory, consultez l’article [Présentation d’Azure Data Factory](data-factory-introduction.md).</span><span class="sxs-lookup"><span data-stu-id="48936-138">See [Introduction to Azure Data Factory](data-factory-introduction.md) article for a detailed overview of Azure Data Factory.</span></span>
  
<span data-ttu-id="48936-139">Voici la **vue schématique** de l’exemple de fabrique de données que vous créez dans ce didacticiel.</span><span class="sxs-lookup"><span data-stu-id="48936-139">Here is the **diagram view** of the sample data factory you build in this tutorial.</span></span> <span data-ttu-id="48936-140">**MyFirstPipeline** a une activité de type Hive qui utilise le jeu de données **AzureBlobInput** comme une entrée et génère le jeu de données **AzureBlobOutput** en tant que sortie.</span><span class="sxs-lookup"><span data-stu-id="48936-140">**MyFirstPipeline** has one activity of type Hive that consumes **AzureBlobInput** dataset as an input and produces **AzureBlobOutput** dataset as an output.</span></span> 

![Vue Diagramme dans le didacticiel Data Factory](media/data-factory-build-your-first-pipeline/data-factory-tutorial-diagram-view.png)


<span data-ttu-id="48936-142">Dans ce didacticiel, le dossier **inputdata** du conteneur d’objets blob Azure **adfgetstarted** contient un fichier nommé input.log.</span><span class="sxs-lookup"><span data-stu-id="48936-142">In this tutorial, **inputdata** folder of the **adfgetstarted** Azure blob container contains one file named input.log.</span></span> <span data-ttu-id="48936-143">Ce fichier journal contient les entrées de trois mois : janvier, février et mars 2016.</span><span class="sxs-lookup"><span data-stu-id="48936-143">This log file has entries from three months: January, February, and March of 2016.</span></span> <span data-ttu-id="48936-144">Voici les échantillons de lignes pour chaque mois du fichier d’entrée.</span><span class="sxs-lookup"><span data-stu-id="48936-144">Here are the sample rows for each month in the input file.</span></span> 

```
2016-01-01,02:01:09,SAMPLEWEBSITE,GET,/blogposts/mvc4/step2.png,X-ARR-LOG-ID=2ec4b8ad-3cf0-4442-93ab-837317ece6a1,80,-,1.54.23.196,Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36,-,http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx,\N,200,0,0,53175,871 
2016-02-01,02:01:10,SAMPLEWEBSITE,GET,/blogposts/mvc4/step7.png,X-ARR-LOG-ID=d7472a26-431a-4a4d-99eb-c7b4fda2cf4c,80,-,1.54.23.196,Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36,-,http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx,\N,200,0,0,30184,871
2016-03-01,02:01:10,SAMPLEWEBSITE,GET,/blogposts/mvc4/step7.png,X-ARR-LOG-ID=d7472a26-431a-4a4d-99eb-c7b4fda2cf4c,80,-,1.54.23.196,Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36,-,http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx,\N,200,0,0,30184,871
```

<span data-ttu-id="48936-145">Quand le fichier est traité par le pipeline avec l’activité Hive d’HDInsight, l’activité exécute un script Hive sur le cluster HDInsight qui partitionne les données d’entrée par année et par mois.</span><span class="sxs-lookup"><span data-stu-id="48936-145">When the file is processed by the pipeline with HDInsight Hive Activity, the activity runs a Hive script on the HDInsight cluster that partitions input data by year and month.</span></span> <span data-ttu-id="48936-146">Le script crée trois dossiers de sortie qui contiennent un fichier avec les entrées de chaque mois.</span><span class="sxs-lookup"><span data-stu-id="48936-146">The script creates three output folders that contain a file with entries from each month.</span></span>  

```
adfgetstarted/partitioneddata/year=2016/month=1/000000_0
adfgetstarted/partitioneddata/year=2016/month=2/000000_0
adfgetstarted/partitioneddata/year=2016/month=3/000000_0
```

<span data-ttu-id="48936-147">Dans les échantillons de lignes ci-dessus, la première (avec 2016-01-01) est écrite dans le fichier 000000_0 dans le dossier month=1.</span><span class="sxs-lookup"><span data-stu-id="48936-147">From the sample lines shown above, the first one (with 2016-01-01) is written to the 000000_0 file in the month=1 folder.</span></span> <span data-ttu-id="48936-148">De même, la deuxième est écrite dans le fichier du dossier month=2 et la troisième est écrite dans le fichier du dossier month=3.</span><span class="sxs-lookup"><span data-stu-id="48936-148">Similarly, the second one is written to the file in the month=2 folder and the third one is written to the file in the month=3 folder.</span></span>  

## <a name="prerequisites"></a><span data-ttu-id="48936-149">Composants requis</span><span class="sxs-lookup"><span data-stu-id="48936-149">Prerequisites</span></span>
<span data-ttu-id="48936-150">Avant de commencer ce didacticiel, vous devez disposer des éléments suivants :</span><span class="sxs-lookup"><span data-stu-id="48936-150">Before you begin this tutorial, you must have the following prerequisites:</span></span>

1. <span data-ttu-id="48936-151">**Un abonnement Azure** : si vous n’en avez pas, vous pouvez créer un compte en quelques minutes pour une évaluation gratuite.</span><span class="sxs-lookup"><span data-stu-id="48936-151">**Azure subscription** - If you don't have an Azure subscription, you can create a free trial account in just a couple of minutes.</span></span> <span data-ttu-id="48936-152">Consultez l’article [Évaluation gratuite](https://azure.microsoft.com/pricing/free-trial/) pour savoir comment obtenir un compte d’évaluation gratuite.</span><span class="sxs-lookup"><span data-stu-id="48936-152">See the [Free Trial](https://azure.microsoft.com/pricing/free-trial/) article on how you can obtain a free trial account.</span></span>
2. <span data-ttu-id="48936-153">**Stockage Azure** : dans ce didacticiel, vous utilisez un compte de stockage Azure standard général pour stocker vos données.</span><span class="sxs-lookup"><span data-stu-id="48936-153">**Azure Storage** – You use a general-purpose standard Azure storage account for storing the data in this tutorial.</span></span> <span data-ttu-id="48936-154">Si vous ne possédez pas de compte de stockage Azure standard général, consultez l’article [Créer un compte de stockage](../storage/common/storage-create-storage-account.md#create-a-storage-account).</span><span class="sxs-lookup"><span data-stu-id="48936-154">If you don't have a general-purpose standard Azure storage account, see the [Create a storage account](../storage/common/storage-create-storage-account.md#create-a-storage-account) article.</span></span> <span data-ttu-id="48936-155">Après avoir créé le compte de stockage, notez le **nom du compte** et la **clé d’accès**.</span><span class="sxs-lookup"><span data-stu-id="48936-155">After you have created the storage account, note down the **account name** and **access key**.</span></span> <span data-ttu-id="48936-156">Consultez [Affichage, copie et régénération de clés d’accès de stockage](../storage/common/storage-create-storage-account.md#view-and-copy-storage-access-keys).</span><span class="sxs-lookup"><span data-stu-id="48936-156">See [View, copy and regenerate storage access keys](../storage/common/storage-create-storage-account.md#view-and-copy-storage-access-keys).</span></span>
3. <span data-ttu-id="48936-157">Téléchargez et consultez le fichier de requête Hive (**HQL**) situé à l’adresse : [https://adftutorialfiles.blob.core.windows.net/hivetutorial/partitionweblogs.hql](https://adftutorialfiles.blob.core.windows.net/hivetutorial/partitionweblogs.hql).</span><span class="sxs-lookup"><span data-stu-id="48936-157">Download and review the Hive query file (**HQL**) located at: [https://adftutorialfiles.blob.core.windows.net/hivetutorial/partitionweblogs.hql](https://adftutorialfiles.blob.core.windows.net/hivetutorial/partitionweblogs.hql).</span></span> <span data-ttu-id="48936-158">Cette requête transforme les données d’entrée pour produire des données de sortie.</span><span class="sxs-lookup"><span data-stu-id="48936-158">This query transforms input data to produce output data.</span></span> 
4. <span data-ttu-id="48936-159">Téléchargez et consultez l’exemple de fichier d’entrée (**input.log**) situé à l’adresse : [https://adftutorialfiles.blob.core.windows.net/hivetutorial/input.log](https://adftutorialfiles.blob.core.windows.net/hivetutorial/input.log).</span><span class="sxs-lookup"><span data-stu-id="48936-159">Download and review the sample input file (**input.log**) located at: [https://adftutorialfiles.blob.core.windows.net/hivetutorial/input.log](https://adftutorialfiles.blob.core.windows.net/hivetutorial/input.log)</span></span>
5. <span data-ttu-id="48936-160">Créez un conteneur de blobs nommé **adfgetstarted** dans votre stockage Blob Azure.</span><span class="sxs-lookup"><span data-stu-id="48936-160">Create a blob container named **adfgetstarted** in your Azure Blob Storage.</span></span> 
6. <span data-ttu-id="48936-161">Chargez le fichier **partitionweblogs.hql** dans le dossier **script** du conteneur **adfgetstarted**.</span><span class="sxs-lookup"><span data-stu-id="48936-161">Upload **partitionweblogs.hql** file to the **script** folder in the **adfgetstarted** container.</span></span> <span data-ttu-id="48936-162">Utilisez des outils tels que [l’Explorateur de stockage Microsoft Azure](http://storageexplorer.com/).</span><span class="sxs-lookup"><span data-stu-id="48936-162">Use tools such as [Microsoft Azure Storage Explorer](http://storageexplorer.com/).</span></span> 
7. <span data-ttu-id="48936-163">Chargez le fichier **input.log** dans le dossier **inputdata** du conteneur **adfgetstarted**.</span><span class="sxs-lookup"><span data-stu-id="48936-163">Upload **input.log** file to the **inputdata** folder in the **adfgetstarted** container.</span></span> 

<span data-ttu-id="48936-164">Si vous disposez de tout ce qui est nécessaire, sélectionnez l’un des outils/kits de développement logiciel (SDK) ci-dessous pour suivre ce didacticiel :</span><span class="sxs-lookup"><span data-stu-id="48936-164">After you complete the prerequisites, select one of the following tools/SDKs to do the tutorial:</span></span> 

- [<span data-ttu-id="48936-165">portail Azure</span><span class="sxs-lookup"><span data-stu-id="48936-165">Azure portal</span></span>](data-factory-build-your-first-pipeline-using-editor.md)
- [<span data-ttu-id="48936-166">Visual Studio</span><span class="sxs-lookup"><span data-stu-id="48936-166">Visual Studio</span></span>](data-factory-build-your-first-pipeline-using-vs.md)
- [<span data-ttu-id="48936-167">PowerShell</span><span class="sxs-lookup"><span data-stu-id="48936-167">PowerShell</span></span>](data-factory-build-your-first-pipeline-using-powershell.md)
- [<span data-ttu-id="48936-168">modèle Azure Resource Manager</span><span class="sxs-lookup"><span data-stu-id="48936-168">Resource Manager template</span></span>](data-factory-build-your-first-pipeline-using-arm.md)
- [<span data-ttu-id="48936-169">API REST</span><span class="sxs-lookup"><span data-stu-id="48936-169">REST API</span></span>](data-factory-build-your-first-pipeline-using-rest-api.md)

<span data-ttu-id="48936-170">Le portail Azure et Visual Studio proposent une méthode utilisant l’interface utilisateur graphique pour créer vos fabriques de données.</span><span class="sxs-lookup"><span data-stu-id="48936-170">Azure portal and Visual Studio provide GUI way of building your data factories.</span></span> <span data-ttu-id="48936-171">Quant aux options fournies par l’API REST, le modèle Resource Manager et PowerShell, elles vous permettent de créer vos fabriques de données via des scripts et des programmes.</span><span class="sxs-lookup"><span data-stu-id="48936-171">Whereas, PowerShell, Resource Manager Template, and REST API options provides scripting/programming way of building your data factories.</span></span>

> [!NOTE]
> <span data-ttu-id="48936-172">Dans ce didacticiel, le pipeline de données transforme les données d’entrée pour produire des données de sortie.</span><span class="sxs-lookup"><span data-stu-id="48936-172">The data pipeline in this tutorial transforms input data to produce output data.</span></span> <span data-ttu-id="48936-173">Il ne copie pas les données d’un magasin de données source vers un magasin de données de destination.</span><span class="sxs-lookup"><span data-stu-id="48936-173">It does not copy data from a source data store to a destination data store.</span></span> <span data-ttu-id="48936-174">Pour un didacticiel sur la copie de données à l’aide d’Azure Data Factory, consultez [Copie de données Blob Storage vers une base de données SQL à l’aide de Data Factory](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).</span><span class="sxs-lookup"><span data-stu-id="48936-174">For a tutorial on how to copy data using Azure Data Factory, see [Tutorial: Copy data from Blob Storage to SQL Database](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).</span></span>
> 
> <span data-ttu-id="48936-175">Vous pouvez chaîner deux activités (une après l’autre) en configurant le jeu de données de sortie d’une activité en tant que jeu de données d’entrée de l’autre activité.</span><span class="sxs-lookup"><span data-stu-id="48936-175">You can chain two activities (run one activity after another) by setting the output dataset of one activity as the input dataset of the other activity.</span></span> <span data-ttu-id="48936-176">Pour des informations détaillées, consultez [Planification et exécution avec Data Factory](data-factory-scheduling-and-execution.md).</span><span class="sxs-lookup"><span data-stu-id="48936-176">See [Scheduling and execution in Data Factory](data-factory-scheduling-and-execution.md) for detailed information.</span></span> 





  
