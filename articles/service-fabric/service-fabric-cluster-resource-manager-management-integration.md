---
title: "Intégration de Service Fabric Cluster Resource Manager et de Service Fabric Management | Microsoft Docs"
description: "Vue d’ensemble des points d’intégration entre Cluster Resource Manager et Service Fabric Management."
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: 
ms.assetid: 956cd0b8-b6e3-4436-a224-8766320e8cd7
ms.service: Service-Fabric
ms.devlang: dotnet
ms.topic: article
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 08/18/2017
ms.author: masnider
ms.openlocfilehash: 9601e758e1033b4e2f86c2c230d4f49479fe6f45
ms.sourcegitcommit: 50e23e8d3b1148ae2d36dad3167936b4e52c8a23
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 08/18/2017
---
# <a name="cluster-resource-manager-integration-with-service-fabric-cluster-management"></a><span data-ttu-id="5a8a1-103">Intégration de Cluster Resource Manager à la gestion de cluster Service Fabric</span><span class="sxs-lookup"><span data-stu-id="5a8a1-103">Cluster resource manager integration with Service Fabric cluster management</span></span>
<span data-ttu-id="5a8a1-104">Service Fabric Cluster Resource Manager n’actionne pas les mises à niveau dans Service Fabric, mais il y participe.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-104">The Service Fabric Cluster Resource Manager doesn't drive upgrades in Service Fabric, but it is involved.</span></span> <span data-ttu-id="5a8a1-105">La première manière dont Cluster Resource Manager aide à la gestion consiste à suivre l’état souhaité du cluster et des services qu’il contient.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-105">The first way that the Cluster Resource Manager helps with management is by tracking the desired state of the cluster and the services inside it.</span></span> <span data-ttu-id="5a8a1-106">Cluster Resource Manager envoie des rapports d’intégrité lorsqu’il ne peut pas placer le cluster dans la configuration souhaitée.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-106">The Cluster Resource Manager sends out health reports when it cannot put the cluster into the desired configuration.</span></span> <span data-ttu-id="5a8a1-107">Par exemple, si la capacité est insuffisante, Cluster Resource Manager envoie des avertissements d’intégrité et des erreurs indiquant le problème.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-107">For example, if there is insufficient capacity the Cluster Resource Manager sends out health warnings and errors indicating the problem.</span></span> <span data-ttu-id="5a8a1-108">Le fonctionnement des mises à niveau est un autre composant de l’intégration.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-108">Another piece of integration has to do with how upgrades work.</span></span> <span data-ttu-id="5a8a1-109">Cluster Resource Manager modifie légèrement son comportement pendant les mises à niveau.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-109">The Cluster Resource Manager alters its behavior slightly during upgrades.</span></span>  

## <a name="health-integration"></a><span data-ttu-id="5a8a1-110">Intégration de l’intégrité</span><span class="sxs-lookup"><span data-stu-id="5a8a1-110">Health integration</span></span>
<span data-ttu-id="5a8a1-111">Cluster Resource Manager assure un suivi permanent des règles que vous avez définies pour le placement de vos services.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-111">The Cluster Resource Manager constantly tracks the rules you have defined for placing your services.</span></span> <span data-ttu-id="5a8a1-112">Il surveille aussi la capacité restante pour chaque métrique au niveau des nœuds et du cluster dans son ensemble.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-112">It also tracks the remaining capacity for each metric on the nodes and in the cluster and in the cluster as a whole.</span></span> <span data-ttu-id="5a8a1-113">S’il ne peut pas respecter ces règles ou si la capacité est insuffisante, des erreurs et des avertissements d’intégrité sont émis.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-113">If it can't satisfy those rules or if there is insufficient capacity, health warnings and errors are emitted.</span></span> <span data-ttu-id="5a8a1-114">Par exemple, si la capacité d’un nœud est dépassée, Cluster Resource Manager tente de résoudre la situation en déplaçant des services.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-114">For example, if a node is over capacity and the Cluster Resource Manager will try to fix the situation by moving services.</span></span> <span data-ttu-id="5a8a1-115">S’il ne peut pas résoudre le problème, il émet un avertissement d’intégrité indiquant pour quel nœud la capacité a été dépassée, ainsi que les mesures concernées.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-115">If it can't correct the situation it emits a health warning indicating which node is over capacity, and for which metrics.</span></span>

<span data-ttu-id="5a8a1-116">Les violations des contraintes de placement constituent un autre exemple des avertissements d’intégrité de Resource Manager.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-116">Another example of the Resource Manager's health warnings is violations of placement constraints.</span></span> <span data-ttu-id="5a8a1-117">Par exemple, si vous avez défini une contrainte de placement (comme `“NodeColor == Blue”`) et que Resource Manager détecte une violation de cette contrainte, il émet un avertissement d’intégrité.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-117">For example, if you have defined a placement constraint (such as `“NodeColor == Blue”`) and the Resource Manager detects a violation of that constraint, it emits a health warning.</span></span> <span data-ttu-id="5a8a1-118">Cela vaut pour les contraintes personnalisées et les contraintes par défaut (comme les contraintes de domaine d’erreur et de domaine de mise à niveau).</span><span class="sxs-lookup"><span data-stu-id="5a8a1-118">This is true for custom constraints and the default constraints (like the Fault Domain and Upgrade Domain constraints).</span></span>

<span data-ttu-id="5a8a1-119">Voici un exemple de rapport d’intégrité.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-119">Here’s an example of one such health report.</span></span> <span data-ttu-id="5a8a1-120">Dans ce cas, le rapport d’intégrité concerne l’une des partitions de service du système.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-120">In this case, the health report is for one of the system service’s partitions.</span></span> <span data-ttu-id="5a8a1-121">Le message d’intégrité indique que les réplicas de cette partition sont temporairement empaquetés dans des domaines de mise à niveau trop peu nombreux.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-121">The health message indicates the replicas of that partition are temporarily packed into too few Upgrade Domains.</span></span>

```posh
PS C:\Users\User > Get-WindowsFabricPartitionHealth -PartitionId '00000000-0000-0000-0000-000000000001'


PartitionId           : 00000000-0000-0000-0000-000000000001
AggregatedHealthState : Warning
UnhealthyEvaluations  :
                        Unhealthy event: SourceId='System.PLB', Property='ReplicaConstraintViolation_UpgradeDomain', HealthState='Warning', ConsiderWarningAsError=false.

ReplicaHealthStates   :
                        ReplicaId             : 130766528804733380
                        AggregatedHealthState : Ok

                        ReplicaId             : 130766528804577821
                        AggregatedHealthState : Ok

                        ReplicaId             : 130766528854889931
                        AggregatedHealthState : Ok

                        ReplicaId             : 130766528804577822
                        AggregatedHealthState : Ok

                        ReplicaId             : 130837073190680024
                        AggregatedHealthState : Ok

HealthEvents          :
                        SourceId              : System.PLB
                        Property              : ReplicaConstraintViolation_UpgradeDomain
                        HealthState           : Warning
                        SequenceNumber        : 130837100116930204
                        SentAt                : 8/10/2015 7:53:31 PM
                        ReceivedAt            : 8/10/2015 7:53:33 PM
                        TTL                   : 00:01:05
                        Description           : The Load Balancer has detected a Constraint Violation for this Replica: fabric:/System/FailoverManagerService Secondary Partition 00000000-0000-0000-0000-000000000001 is
                        violating the Constraint: UpgradeDomain Details: UpgradeDomain ID -- 4, Replica on NodeName -- Node.8 Currently Upgrading -- false Distribution Policy -- Packing
                        RemoveWhenExpired     : True
                        IsExpired             : False
                        Transitions           : Ok->Warning = 8/10/2015 7:13:02 PM, LastError = 1/1/0001 12:00:00 AM
```

<span data-ttu-id="5a8a1-122">Voici ce que nous apprend ce message d’intégrité :</span><span class="sxs-lookup"><span data-stu-id="5a8a1-122">Here's what this health message is telling us is:</span></span>

1. <span data-ttu-id="5a8a1-123">Tous les réplicas proprement dits sont intègres : chacun indique AggregatedHealthState : Ok</span><span class="sxs-lookup"><span data-stu-id="5a8a1-123">All the replicas themselves are healthy: Each has AggregatedHealthState : Ok</span></span>
2. <span data-ttu-id="5a8a1-124">La contrainte de distribution de domaine de mise à niveau n’est actuellement pas respectée.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-124">The Upgrade Domain distribution constraint is currently being violated.</span></span> <span data-ttu-id="5a8a1-125">Cela signifie qu’un domaine de mise à niveau particulier compte plus de réplicas que prévu pour cette partition.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-125">This means a particular Upgrade Domain has more replicas from this partition than it should.</span></span>
3. <span data-ttu-id="5a8a1-126">L’identité du nœud qui contient le réplica à l’origine de la violation.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-126">Which node contains the replica causing the violation.</span></span> <span data-ttu-id="5a8a1-127">Dans ce cas, il s’agit du nœud nommé « Node.8 ».</span><span class="sxs-lookup"><span data-stu-id="5a8a1-127">In this case it's the node with the name "Node.8"</span></span>
4. <span data-ttu-id="5a8a1-128">Si cette partition fait actuellement l’objet d’une mise à niveau (« Currently Upgrading -- false »).</span><span class="sxs-lookup"><span data-stu-id="5a8a1-128">Whether an upgrade is currently happening for this partition ("Currently Upgrading -- false")</span></span>
5. <span data-ttu-id="5a8a1-129">La stratégie de distribution pour ce service : « Distribution Policy -- Packing ».</span><span class="sxs-lookup"><span data-stu-id="5a8a1-129">The distribution policy for this service: "Distribution Policy -- Packing".</span></span> <span data-ttu-id="5a8a1-130">Ceci est régi par la `RequireDomainDistribution` [stratégie de positionnement](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md#requiring-replica-distribution-and-disallowing-packing).</span><span class="sxs-lookup"><span data-stu-id="5a8a1-130">This is governed by the `RequireDomainDistribution` [placement policy](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md#requiring-replica-distribution-and-disallowing-packing).</span></span> <span data-ttu-id="5a8a1-131">« Packing » indique que dans ce cas, la distribution de domaine n’était _pas_ obligatoire. Nous savons donc que la stratégie de positionnement n’a pas été spécifiée pour ce service.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-131">"Packing" indicates that in this case DomainDistribution was _not_ required, so we know that placement policy was not specified for this service.</span></span> 
6. <span data-ttu-id="5a8a1-132">La date et l’heure du rapport (10/08/2015 19:13:02)</span><span class="sxs-lookup"><span data-stu-id="5a8a1-132">When the report happened - 8/10/2015 7:13:02 PM</span></span>

<span data-ttu-id="5a8a1-133">Ces informations alimentent les alertes qui se déclenchent en production pour vous informer qu’un problème est survenu. Elles servent aussi à détecter et à mettre un terme aux mises à niveau incorrectes.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-133">Information like this powers alerts that fire in production to let you know something has gone wrong and is also used to detect and halt bad upgrades.</span></span> <span data-ttu-id="5a8a1-134">Dans ce cas, nous aimerions savoir pourquoi Resource Manager a décidé de placer les réplicas dans le domaine de mise à niveau.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-134">In this case, we’d want to see if we can figure out why the Resource Manager had to pack the replicas into the Upgrade Domain.</span></span> <span data-ttu-id="5a8a1-135">En règle générale, l’empaquetage est temporaire et peut résulter, par exemple, de l’arrêt des nœuds dans les autres domaines de mise à niveau.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-135">Usually packing is transient because the nodes in the other Upgrade Domains were down, for example.</span></span>

<span data-ttu-id="5a8a1-136">Supposons que Cluster Resource Manager tente de placer certains services, mais qu’aucune solution ne fonctionne.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-136">Let’s say the Cluster Resource Manager is trying to place some services, but there aren't any solutions that work.</span></span> <span data-ttu-id="5a8a1-137">Quand les services ne peuvent pas être placés, c’est généralement pour l’une des raisons suivantes :</span><span class="sxs-lookup"><span data-stu-id="5a8a1-137">When services can't be placed, it is usually for one of the following reasons:</span></span>

1. <span data-ttu-id="5a8a1-138">Une situation temporaire empêche de placer correctement l’instance ou le réplica de ce service.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-138">Some transient condition has made it impossible to place this service instance or replica correctly</span></span>
2. <span data-ttu-id="5a8a1-139">Les conditions de placement du service ne peuvent pas être satisfaites.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-139">The service’s placement requirements are unsatisfiable.</span></span>

<span data-ttu-id="5a8a1-140">En pareil cas, les rapports d’intégrité de Cluster Resource Manager vous aident à déterminer pourquoi le service ne peut pas être placé.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-140">In these cases, health reports from the Cluster Resource Manager help you determine why the service can’t be placed.</span></span> <span data-ttu-id="5a8a1-141">C’est ce que nous appelons une séquence d’élimination de contrainte.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-141">We call this process the constraint elimination sequence.</span></span> <span data-ttu-id="5a8a1-142">Au cours de ce processus, le système passe en revue les contraintes configurées affectant le service et les enregistrements qu’elles éliminent.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-142">During it, the system walks through the configured constraints affecting the service and records what they eliminate.</span></span> <span data-ttu-id="5a8a1-143">Lorsque les services ne peuvent pas être placés, vous pouvez ainsi voir quels nœuds ont été éliminés et pour quelle raison.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-143">This way when services aren’t able to be placed, you can see which nodes were eliminated and why.</span></span>

## <a name="constraint-types"></a><span data-ttu-id="5a8a1-144">Types de contrainte</span><span class="sxs-lookup"><span data-stu-id="5a8a1-144">Constraint types</span></span>
<span data-ttu-id="5a8a1-145">Examinons les différentes contraintes qui apparaissent dans ces rapports d’intégrité.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-145">Let’s talk about each of the different constraints in these health reports.</span></span> <span data-ttu-id="5a8a1-146">Les messages de contrôle d’intégrité liés à ces contraintes s’affichent quand les réplicas ne peuvent pas être placés.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-146">You will see health messages related to these constraints when replicas can't be placed.</span></span>

* <span data-ttu-id="5a8a1-147">**ReplicaExclusionStatic** et **ReplicaExclusionDynamic** : ces contraintes indiquent qu’une solution a été rejetée, car deux objets de service de la même partition auraient dû être placés sur le même nœud.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-147">**ReplicaExclusionStatic** and **ReplicaExclusionDynamic**: These constraints indicates that a solution was rejected because two service objects from the same partition would have to be placed on the same node.</span></span> <span data-ttu-id="5a8a1-148">Cette opération n’est pas autorisée, car une défaillance de ce nœud aurait un trop fort impact sur cette partition.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-148">This isn’t allowed because then failure of that node would overly impact that partition.</span></span> <span data-ttu-id="5a8a1-149">ReplicaExclusionStatic et ReplicaExclusionDynamic appliquent pratiquement la même règle et les différences importent peu.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-149">ReplicaExclusionStatic and ReplicaExclusionDynamic are almost the same rule and the differences don't really matter.</span></span> <span data-ttu-id="5a8a1-150">Si vous constatez qu’une séquence d’élimination de contrainte contient la contrainte ReplicaExclusionDynamic ou ReplicaExclusionStatic, Cluster Resource Manager considère qu’il n’y a pas suffisamment de nœuds.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-150">If you are seeing a constraint elimination sequence containing either the ReplicaExclusionStatic or ReplicaExclusionDynamic constraint, the Cluster Resource Manager thinks that there aren’t enough nodes.</span></span> <span data-ttu-id="5a8a1-151">Les solutions restantes doivent utiliser ces placements non valides qui ne sont pas autorisés.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-151">This requires remaining solutions to use these invalid placements which are disallowed.</span></span> <span data-ttu-id="5a8a1-152">Les autres contraintes de la séquence indiquent généralement pourquoi les nœuds sont éliminés en premier lieu.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-152">The other constraints in the sequence will usually tell us why nodes are being eliminated in the first place.</span></span>
* <span data-ttu-id="5a8a1-153">**PlacementConstraint** : si vous voyez ce message, cela signifie que certains nœuds ne correspondant pas aux contraintes de placement du service ont été éliminés.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-153">**PlacementConstraint**: If you see this message, it means that we eliminated some nodes because they didn’t match the service’s placement constraints.</span></span> <span data-ttu-id="5a8a1-154">Nous avons suivi les contraintes de placement actuellement configurées dans le cadre de ce message.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-154">We trace out the currently configured placement constraints as a part of this message.</span></span> <span data-ttu-id="5a8a1-155">Ceci est normal si une contrainte de placement est définie.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-155">This is normal if you have a placement constraint defined.</span></span> <span data-ttu-id="5a8a1-156">Cependant, si la contrainte de placement entraîne à tort l’élimination d’un trop grand nombre de nœuds, c’est de cette façon que vous le remarquerez.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-156">However, if placement constraint is incorrectly causing too many nodes to be eliminated this is how you would notice.</span></span>
* <span data-ttu-id="5a8a1-157">**NodeCapacity** : cette contrainte signifie que Cluster Resource Manager n’a pas pu placer les réplicas sur les nœuds indiqués, car cela les aurait mis en surcapacité.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-157">**NodeCapacity**: This constraint means that the Cluster Resource Manager couldn’t place the replicas on the indicated nodes because that would put them over capacity.</span></span>
* <span data-ttu-id="5a8a1-158">**Affinity** : cette contrainte indique que les réplicas n’ont pas été placés sur les nœuds affectés pour ne pas enfreindre la contrainte Affinity.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-158">**Affinity**: This constraint indicates that we couldn’t place the replica on the affected nodes since it would cause a violation of the affinity constraint.</span></span> <span data-ttu-id="5a8a1-159">Plus d’informations sur la contrainte Affinity, lisez [cet article](service-fabric-cluster-resource-manager-advanced-placement-rules-affinity.md)</span><span class="sxs-lookup"><span data-stu-id="5a8a1-159">More information on affinity is in [this article](service-fabric-cluster-resource-manager-advanced-placement-rules-affinity.md)</span></span>
* <span data-ttu-id="5a8a1-160">**FaultDomain** et **UpgradeDomain** : cette contrainte élimine des nœuds si le placement du réplica sur les nœuds indiqués se traduit par une compression dans un domaine de mise à niveau ou d’erreur spécifique.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-160">**FaultDomain** and **UpgradeDomain**: This constraint eliminates nodes if placing the replica on the indicated nodes would cause packing in a particular fault or upgrade domain.</span></span> <span data-ttu-id="5a8a1-161">Plusieurs exemples décrivant cette contrainte sont présentés dans la rubrique [Contraintes des domaines d’erreur et de mise à niveau, et comportement résultant](service-fabric-cluster-resource-manager-cluster-description.md)</span><span class="sxs-lookup"><span data-stu-id="5a8a1-161">Several examples discussing this constraint are presented in the topic on [fault and upgrade domain constraints and resulting behavior](service-fabric-cluster-resource-manager-cluster-description.md)</span></span>
* <span data-ttu-id="5a8a1-162">**PreferredLocation** : cette contrainte ne doit normalement pas retirer des nœuds de la solution, car elle s’exécute par défaut en tant qu’optimisation.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-162">**PreferredLocation**: You shouldn’t normally see this constraint removing nodes from the solution since it runs as an optimization by default.</span></span> <span data-ttu-id="5a8a1-163">La contrainte d’emplacement par défaut est aussi présente pendant les mises à niveau.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-163">The preferred location constraint is also present during upgrades.</span></span> <span data-ttu-id="5a8a1-164">À ces occasions, elle déplace les services à l’emplacement d’origine qui était le leur au moment où la mise à niveau a démarré.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-164">During upgrade it is used to move services back to where they were when the upgrade started.</span></span>

## <a name="blocklisting-nodes"></a><span data-ttu-id="5a8a1-165">Mise des nœuds en liste de blocage</span><span class="sxs-lookup"><span data-stu-id="5a8a1-165">Blocklisting Nodes</span></span>
<span data-ttu-id="5a8a1-166">Cluster Resource Manager peut aussi afficher un message de contrôle d’intégrité quand des nœuds sont mis en liste de blocage.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-166">Another health message the Cluster Resource Manager reports is when nodes are blocklisted.</span></span> <span data-ttu-id="5a8a1-167">La mise des nœuds en liste de blocage peut être considérée comme une contrainte provisoire qui s’applique automatiquement.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-167">You can think of blocklisting as a temporary constraint that is automatically applied for you.</span></span> <span data-ttu-id="5a8a1-168">Les nœuds sont mis en liste de blocage quand ils subissent des défaillances à répétition pendant le lacement d’instances de ce type de service.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-168">Nodes get blocklisted when they experience repeated failures when launching instances of that service type.</span></span> <span data-ttu-id="5a8a1-169">Les nœuds sont mis en liste de blocage par type de service,</span><span class="sxs-lookup"><span data-stu-id="5a8a1-169">Nodes are blocklisted on a per-service-type basis.</span></span> <span data-ttu-id="5a8a1-170">si bien qu’un nœud peut être placé dans une liste de blocage pour un type de service, mais pas pour un autre.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-170">A node may be blocklisted for one service type but not another.</span></span> 

<span data-ttu-id="5a8a1-171">Le placement dans une liste de blocage se manifeste souvent en phase de développement : certains bogues provoquent le blocage de votre hôte de service au démarrage.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-171">You'll see blocklisting kick in often during development: some bug causes your service host to crash on startup.</span></span> <span data-ttu-id="5a8a1-172">Service Fabric tente à plusieurs reprises de créer l’hôte de service, mais l’erreur se répète.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-172">Service Fabric tries to create the service host a few times, and the failure keeps occurring.</span></span> <span data-ttu-id="5a8a1-173">Après plusieurs tentatives, le nœud est mis en liste de blocage et Cluster Resource Manager essaie de créer le service à un autre emplacement.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-173">After a few attempts, the node gets blocklisted, and the Cluster Resource Manager will try to create the service elsewhere.</span></span> <span data-ttu-id="5a8a1-174">Si cette erreur se répète sur plusieurs nœuds, il est possible que tous les nœuds du cluster se retrouvent dans la liste de blocage.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-174">If that failure keeps happening on multiple nodes, it's possible that all of the valid nodes in the cluster end up blocked.</span></span> <span data-ttu-id="5a8a1-175">La mise en liste de blocage peut aussi concerner un trop grand nombre de nœuds, au point qu’il n’en reste pas suffisamment pour lancer le service à l’échelle souhaitée.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-175">Blocklisting cna also remove so many nodes that not enough can successfully launch the service to meet the desired scale.</span></span> <span data-ttu-id="5a8a1-176">Il est fort probable dans ce cas que Cluster Resource Manager affiche des erreurs ou des avertissements supplémentaires pour indiquer que le service présente moins de réplicas ou d’instances que prévu, ainsi que des messages de contrôle d’intégrité décrivant la défaillance à l’origine de la mise en liste de blocage.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-176">You'll typically see additional errors or warnings from the Cluster Resource Manager indicating that the service is below the desired replica or instance count, as well as health messages indicating what the failure is that's leading to the blocklisting in the first place.</span></span>

<span data-ttu-id="5a8a1-177">La mise en liste de blocage n’est pas une condition permanente.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-177">Blocklisting is not a permanent condition.</span></span> <span data-ttu-id="5a8a1-178">Après quelques minutes, le nœud est retiré de la liste de blocage et Service Fabric réactive les services sur ce nœud.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-178">After a few minutes, the node is removed from the blocklist and Service Fabric may activate the services on that node again.</span></span> <span data-ttu-id="5a8a1-179">En revanche, si le service continue d’échouer, le nœud est remis en liste de blocage pour ce type de service.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-179">If services continue to fail, the node is blocklisted for that service type again.</span></span> 

### <a name="constraint-priorities"></a><span data-ttu-id="5a8a1-180">Priorités de contrainte</span><span class="sxs-lookup"><span data-stu-id="5a8a1-180">Constraint priorities</span></span>

> [!WARNING]
> <span data-ttu-id="5a8a1-181">Il n’est pas recommandé de modifier les priorités des contraintes. De plus, cela peut avoir des répercutions défavorables sur votre cluster.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-181">Changing constraint priorities is not recommended and may have significant adverse effects on your cluster.</span></span> <span data-ttu-id="5a8a1-182">Vous trouverez ci-dessous des informations fournies à titre indicatif sur les priorités des contraintes par défaut et leur comportement.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-182">The below information is provided for reference of the default constraint priorities and their behavior.</span></span> 
>

<span data-ttu-id="5a8a1-183">Face à toutes ces contraintes, vous vous dites peut-être : « Ce qui compte avant tout dans mon système, ce sont les contraintes de domaine d’erreur.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-183">With all of these constraints, you may have been thinking “Hey – I think that fault domain constraints are the most important thing in my system.</span></span> <span data-ttu-id="5a8a1-184">Pour être certain que la contrainte de domaine d’erreur va bien être respectée, je vais enfreindre les autres contraintes ».</span><span class="sxs-lookup"><span data-stu-id="5a8a1-184">In order to ensure the fault domain constraint isn't violated, I’m willing to violate other constraints.”</span></span>

<span data-ttu-id="5a8a1-185">Les contraintes peuvent être configurées avec différents niveaux de priorité,</span><span class="sxs-lookup"><span data-stu-id="5a8a1-185">Constraints can be configured with different priority levels.</span></span> <span data-ttu-id="5a8a1-186">Ces composants sont les suivants :</span><span class="sxs-lookup"><span data-stu-id="5a8a1-186">These are:</span></span>

   - <span data-ttu-id="5a8a1-187">« stricte » (0)</span><span class="sxs-lookup"><span data-stu-id="5a8a1-187">“hard” (0)</span></span>
   - <span data-ttu-id="5a8a1-188">« souple » (1)</span><span class="sxs-lookup"><span data-stu-id="5a8a1-188">“soft” (1)</span></span>
   - <span data-ttu-id="5a8a1-189">« optimisation » (2)</span><span class="sxs-lookup"><span data-stu-id="5a8a1-189">“optimization” (2)</span></span>
   - <span data-ttu-id="5a8a1-190">« désactivé » (-1).</span><span class="sxs-lookup"><span data-stu-id="5a8a1-190">“off” (-1).</span></span> 
   
<span data-ttu-id="5a8a1-191">Par défaut, la plupart des contraintes sont configurées comme des contraintes strictes.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-191">Most of the constraints are configured as hard constraints by default.</span></span>

<span data-ttu-id="5a8a1-192">Il n’est pas courant de modifier la priorité des contraintes.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-192">Changing the priority of constraints is uncommon.</span></span> <span data-ttu-id="5a8a1-193">Dans certaines circonstances, la modification des priorités des contraintes était une nécessité, généralement pour contourner un bogue ou un comportement qui avait un impact sur l’environnement.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-193">There have been times where constraint priorities needed to change, usually to work around some other bug or behavior that was impacting the environment.</span></span> <span data-ttu-id="5a8a1-194">En règle générale, la flexibilité de l’infrastructure de priorité de contrainte a bien fonctionné, mais elle n’est pas souvent nécessaire.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-194">Generally the flexibility of the constraint priority infrastructure has worked very well, but it isn't needed often.</span></span> <span data-ttu-id="5a8a1-195">La plupart du temps, les priorités par défaut sont conservées.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-195">Most of the time everything sits at their default priorities.</span></span> 

<span data-ttu-id="5a8a1-196">Les niveaux de priorité ne signifient pas qu’une contrainte donnée _sera_ enfreinte, ni qu’elle sera toujours respectée.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-196">The priority levels don't mean that a given constraint _will_ be violated, nor that it will always be met.</span></span> <span data-ttu-id="5a8a1-197">Les priorités des contraintes définissent l’ordre dans lequel les contraintes sont appliquées.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-197">Constraint priorities define an order in which constraints are enforced.</span></span> <span data-ttu-id="5a8a1-198">Les priorités permettent de définir des compromis quand il est impossible de satisfaire toutes les contraintes.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-198">Priorities define the tradeoffs when it is impossible to satisfy all constraints.</span></span> <span data-ttu-id="5a8a1-199">En règle générale, si rien de particulier ne se passe dans l’environnement, les contraintes peuvent toutes être satisfaites.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-199">Usually all the constraints can be satisfied unless there's something else going on in the environment.</span></span> <span data-ttu-id="5a8a1-200">Les conflits de contraintes ou la présence d’un grand nombre de défaillances simultanées sont des exemples de scénarios qui peuvent aboutir à des violations de contraintes.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-200">Some examples of scenarios that will lead to constraint violations are conflicting constraints, or large numbers of concurrent failures.</span></span>

<span data-ttu-id="5a8a1-201">Dans des situations avancées, vous pouvez modifier les priorités des contraintes.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-201">In advanced situations, you can change the constraint priorities.</span></span> <span data-ttu-id="5a8a1-202">Supposons, par exemple, que vous vouliez faire en sorte que la contrainte Affinity soit enfreinte pour résoudre des problèmes de capacité de nœuds.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-202">For example, say you wanted to ensure that affinity would always be violated when necessary to solve node capacity issues.</span></span> <span data-ttu-id="5a8a1-203">Pour ce faire, vous pouvez définir la priorité de la contrainte Affinity sur « souple » (1) et laisser la contrainte Capacity sur « stricte » (0).</span><span class="sxs-lookup"><span data-stu-id="5a8a1-203">To achieve this, you could set the priority of the affinity constraint to “soft” (1) and leave the capacity constraint set to “hard” (0).</span></span>

<span data-ttu-id="5a8a1-204">Les valeurs de priorité par défaut associées aux différentes contraintes sont indiquées dans le fichier config suivant :</span><span class="sxs-lookup"><span data-stu-id="5a8a1-204">The default priority values for the different constraints are specified in the following config:</span></span>

<span data-ttu-id="5a8a1-205">ClusterManifest.xml</span><span class="sxs-lookup"><span data-stu-id="5a8a1-205">ClusterManifest.xml</span></span>

```xml
        <Section Name="PlacementAndLoadBalancing">
            <Parameter Name="PlacementConstraintPriority" Value="0" />
            <Parameter Name="CapacityConstraintPriority" Value="0" />
            <Parameter Name="AffinityConstraintPriority" Value="0" />
            <Parameter Name="FaultDomainConstraintPriority" Value="0" />
            <Parameter Name="UpgradeDomainConstraintPriority" Value="1" />
            <Parameter Name="PreferredLocationConstraintPriority" Value="2" />
        </Section>
```

<span data-ttu-id="5a8a1-206">via ClusterConfig.json pour les déploiements autonomes ou Template.json pour les clusters hébergés sur Azure :</span><span class="sxs-lookup"><span data-stu-id="5a8a1-206">via ClusterConfig.json for Standalone deployments or Template.json for Azure hosted clusters:</span></span>

```json
"fabricSettings": [
  {
    "name": "PlacementAndLoadBalancing",
    "parameters": [
      {
          "name": "PlacementConstraintPriority",
          "value": "0"
      },
      {
          "name": "CapacityConstraintPriority",
          "value": "0"
      },
      {
          "name": "AffinityConstraintPriority",
          "value": "0"
      },
      {
          "name": "FaultDomainConstraintPriority",
          "value": "0"
      },
      {
          "name": "UpgradeDomainConstraintPriority",
          "value": "1"
      },
      {
          "name": "PreferredLocationConstraintPriority",
          "value": "2"
      }
    ]
  }
]
```

## <a name="fault-domain-and-upgrade-domain-constraints"></a><span data-ttu-id="5a8a1-207">Contraintes de domaine d’erreur et de domaine de mise à niveau</span><span class="sxs-lookup"><span data-stu-id="5a8a1-207">Fault domain and upgrade domain constraints</span></span>
<span data-ttu-id="5a8a1-208">Cluster Resource Manager vise à maintenir une répartition harmonieuse des services entre les domaines d’erreur et de mise à niveau.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-208">The Cluster Resource Manager wants to keep services spread out among fault and upgrade domains.</span></span> <span data-ttu-id="5a8a1-209">Pour cela, une contrainte est intégrée dans le moteur de Cluster Resource Manager.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-209">It models this as a constraint inside the Cluster Resource Manager’s engine.</span></span> <span data-ttu-id="5a8a1-210">Pour plus d’informations sur leur utilisation et leur comportement spécifique, consultez l’article relatif à la [configuration de cluster](service-fabric-cluster-resource-manager-cluster-description.md#fault-and-upgrade-domain-constraints-and-resulting-behavior).</span><span class="sxs-lookup"><span data-stu-id="5a8a1-210">For more information on how they are used and their specific behavior, check out the article on [cluster configuration](service-fabric-cluster-resource-manager-cluster-description.md#fault-and-upgrade-domain-constraints-and-resulting-behavior).</span></span>

<span data-ttu-id="5a8a1-211">Cluster Resource Manager peut être amené à empaqueter deux réplicas dans un domaine de mise à niveau pour gérer les mises à niveau, les défaillances ou d’autres violations de contraintes.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-211">The Cluster Resource Manager may need to pack a couple replicas into an upgrade domain in order to deal with upgrades, failures, or other constraint violations.</span></span> <span data-ttu-id="5a8a1-212">Normalement, l’empaquetage dans des domaines d’erreur ou de mise à niveau intervient uniquement quand plusieurs défaillances ou évolutions se produisent dans le système et empêchent un positionnement correct.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-212">Packing into fault or upgrade domains normally happens only when there are several failures or other churn in the system preventing correct placement.</span></span> <span data-ttu-id="5a8a1-213">Si vous voulez empêcher l’empaquetage même dans ces situations, vous pouvez utiliser la `RequireDomainDistribution` [stratégie de positionnement](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md#requiring-replica-distribution-and-disallowing-packing).</span><span class="sxs-lookup"><span data-stu-id="5a8a1-213">If you wish to prevent packing even during these situations, you can utilize the `RequireDomainDistribution` [placement policy](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md#requiring-replica-distribution-and-disallowing-packing).</span></span> <span data-ttu-id="5a8a1-214">Notez que cela peut avoir des effets secondaires et nuire à la disponibilité et à la fiabilité du service. Pensez-y avant toute décision dans ce sens.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-214">Note that this may affect service availability and reliability as a side effect, so consider it carefully.</span></span>

<span data-ttu-id="5a8a1-215">Si l’environnement est correctement configuré, toutes les contraintes sont entièrement respectées, même pendant les mises à niveau.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-215">If the environment is configured correctly, all constraints are fully respected, even during upgrades.</span></span> <span data-ttu-id="5a8a1-216">L’essentiel est que Cluster Resource Manager surveille vos contraintes.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-216">The key thing is that the Cluster Resource Manager is watching out for your constraints.</span></span> <span data-ttu-id="5a8a1-217">Quand il détecte une violation, il le signale immédiatement et essaie de corriger le problème.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-217">When it detects a violation it immediately reports it and tries to correct the issue.</span></span>

## <a name="the-preferred-location-constraint"></a><span data-ttu-id="5a8a1-218">La contrainte d’emplacement par défaut</span><span class="sxs-lookup"><span data-stu-id="5a8a1-218">The preferred location constraint</span></span>
<span data-ttu-id="5a8a1-219">La contrainte PreferredLocation est un peu différente, car elle remplit deux fonctions différentes.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-219">The PreferredLocation constraint is a little different, as it has two different uses.</span></span> <span data-ttu-id="5a8a1-220">D’une part, cette contrainte est utilisée pendant les mises à niveau d’applications.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-220">One use of this constraint is during application upgrades.</span></span> <span data-ttu-id="5a8a1-221">Cluster Resource Manager gère automatiquement cette contrainte pendant les mises à niveau.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-221">The Cluster Resource Manager automatically manages this constraint during upgrades.</span></span> <span data-ttu-id="5a8a1-222">Elle permet de vérifier que les réplicas retrouvent leur emplacement initial à l’issue des mises à niveau.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-222">It is used to ensure that when upgrades are complete that replicas return to their initial locations.</span></span> <span data-ttu-id="5a8a1-223">D’autre part, la contrainte PreferredLocation est utilisée avec la [`PreferredPrimaryDomain` stratégie de positionnement](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md).</span><span class="sxs-lookup"><span data-stu-id="5a8a1-223">The other use of the PreferredLocation constraint is for the [`PreferredPrimaryDomain` placement policy](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md).</span></span> <span data-ttu-id="5a8a1-224">Il s’agit dans les deux cas d’optimisations. De ce fait, la contrainte PreferredLocation est la seule contrainte à être définie par défaut sur « optimisation ».</span><span class="sxs-lookup"><span data-stu-id="5a8a1-224">Both of these are optimizations, and hence the PreferredLocation constraint is the only constraint set to "Optimization" by default.</span></span>

## <a name="upgrades"></a><span data-ttu-id="5a8a1-225">Mises à niveau</span><span class="sxs-lookup"><span data-stu-id="5a8a1-225">Upgrades</span></span>
<span data-ttu-id="5a8a1-226">Cluster Resource Manager est également utile durant les mises à niveau d’applications et de cluster, où il exécute deux tâches :</span><span class="sxs-lookup"><span data-stu-id="5a8a1-226">The Cluster Resource Manager also helps during application and cluster upgrades, during which it has two jobs:</span></span>

* <span data-ttu-id="5a8a1-227">vérifier que les règles du cluster ne sont pas compromises ;</span><span class="sxs-lookup"><span data-stu-id="5a8a1-227">ensure that the rules of the cluster are not compromised</span></span>
* <span data-ttu-id="5a8a1-228">essayer de faire en sorte que la mise à niveau se déroule sans problème.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-228">try to help the upgrade go smoothly</span></span>

### <a name="keep-enforcing-the-rules"></a><span data-ttu-id="5a8a1-229">Application continue des règles</span><span class="sxs-lookup"><span data-stu-id="5a8a1-229">Keep enforcing the rules</span></span>
<span data-ttu-id="5a8a1-230">L’élément principal à retenir est que les règles (les contraintes strictes, comme les contraintes de placement et les capacités) sont toujours appliquées pendant les mises à niveau.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-230">The main thing to be aware of is that the rules – the strict constraints like placement constraints and capacities - are still enforced during upgrades.</span></span> <span data-ttu-id="5a8a1-231">Les contraintes de placement s’assurent que vos charges de travail s’exécutent uniquement où elles y sont autorisées, notamment pendant les mises à niveau.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-231">Placement constraints ensure that your workloads only run where they are allowed to, even during upgrades.</span></span> <span data-ttu-id="5a8a1-232">Quand les services sont très ralentis, les mises à niveau peuvent prendre plus de temps.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-232">When services are highly constrained, upgrades can take longer.</span></span> <span data-ttu-id="5a8a1-233">Quand le service ou le nœud en question est mis hors service pour une mise à jour, plusieurs destinations possibles peuvent être proposées.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-233">When the service or the node it is running on is brought down for an update there may be few options for where it can go.</span></span>

### <a name="smart-replacements"></a><span data-ttu-id="5a8a1-234">Remplacements actifs</span><span class="sxs-lookup"><span data-stu-id="5a8a1-234">Smart replacements</span></span>
<span data-ttu-id="5a8a1-235">Lorsqu’une mise à niveau démarre, Resource Manager prend une capture instantanée de l’organisation actuelle du cluster.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-235">When an upgrade starts, the Resource Manager takes a snapshot of the current arrangement of the cluster.</span></span> <span data-ttu-id="5a8a1-236">À mesure que chaque domaine de mise à niveau aboutit, il tente de rétablir l’organisation initiale des services qui figuraient dans ce domaine de mise à niveau.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-236">As each Upgrade Domain completes, it attempts to return the services that were in that Upgrade Domain to their original arrangement.</span></span> <span data-ttu-id="5a8a1-237">De cette manière, il existe tout au plus deux transitions possibles pour un service pendant la mise à niveau.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-237">This way there are at most two transitions for a service during the upgrade.</span></span> <span data-ttu-id="5a8a1-238">Il y a une sortie du nœud affecté et un retour dans celui-ci.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-238">There is one move out of the affected node and one move back in.</span></span> <span data-ttu-id="5a8a1-239">Le fait de rétablir le cluster ou le service tel qu’il était avant la mise à niveau garantit également que la mise à niveau n’affecte pas la disposition du cluster.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-239">Returning the cluster or service to how it was before the upgrade also ensures the upgrade doesn’t impact the layout of the cluster.</span></span> 

### <a name="reduced-churn"></a><span data-ttu-id="5a8a1-240">Évolution limitée</span><span class="sxs-lookup"><span data-stu-id="5a8a1-240">Reduced churn</span></span>
<span data-ttu-id="5a8a1-241">Pendant les mises à niveau, Cluster Resource Manager désactive aussi l’équilibrage.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-241">Another thing that happens during upgrades is that the Cluster Resource Manager turns off balancing.</span></span> <span data-ttu-id="5a8a1-242">Le fait de suspendre l’équilibrage empêche les réactions inutiles envers la mise à niveau, comme le déplacement de services vers des nœuds qui ont été vidés pour la mise à niveau.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-242">Preventing balancing prevents unnecessary reactions to the upgrade itself, like moving services into nodes that were emptied for the upgrade.</span></span> <span data-ttu-id="5a8a1-243">Si la mise à niveau en question est une mise à niveau du cluster, alors l’ensemble du cluster n’est pas équilibré au cours de la mise à niveau.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-243">If the upgrade in question is a Cluster upgrade, then the entire cluster is not balanced during the upgrade.</span></span> <span data-ttu-id="5a8a1-244">La vérification des contraintes reste active, seul le mouvement basé sur l’équilibrage proactif des métriques est désactivé.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-244">Constraint checks stay active, only movement based on the proactive balancing of metrics is disabled.</span></span>

### <a name="buffered-capacity--upgrade"></a><span data-ttu-id="5a8a1-245">Capacité mise en mémoire tampon et mise à niveau</span><span class="sxs-lookup"><span data-stu-id="5a8a1-245">Buffered Capacity & Upgrade</span></span>
<span data-ttu-id="5a8a1-246">En règle générale, vous voulez que la mise à niveau arrive à son terme même si le cluster est ralenti ou proche de la saturation.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-246">Generally you want the upgrade to complete even if the cluster is constrained or close to full.</span></span> <span data-ttu-id="5a8a1-247">La gestion de la capacité du cluster est encore plus importante pendant les mises à niveau qu’en temps normal.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-247">Managing the capacity of the cluster is even more important during upgrades than usual.</span></span> <span data-ttu-id="5a8a1-248">Selon le nombre de domaines de mise à niveau, entre 5 et 20 pour cent de la capacité doit être migrée pendant le déploiement de la mise à niveau sur le cluster.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-248">Depending on the number of upgrade domains, between 5 and 20 percent of capacity must be migrated as the upgrade rolls through the cluster.</span></span> <span data-ttu-id="5a8a1-249">Ce travail doit aller quelque part.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-249">That work has to go somewhere.</span></span> <span data-ttu-id="5a8a1-250">C’est dans ce contexte que la notion de [capacité mise en mémoire tampon](service-fabric-cluster-resource-manager-cluster-description.md#buffered-capacity) prend tout son sens.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-250">This is where the notion of [buffered capacities](service-fabric-cluster-resource-manager-cluster-description.md#buffered-capacity) is useful.</span></span> <span data-ttu-id="5a8a1-251">La capacité mise en mémoire tampon est respectée en phase de fonctionnement normal.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-251">Buffered capacity is respected during normal operation.</span></span> <span data-ttu-id="5a8a1-252">Cluster Resource Manager peut remplir les nœuds jusqu’à leur capacité totale (en consommant la mémoire tampon) au cours des mises à niveau, si nécessaire.</span><span class="sxs-lookup"><span data-stu-id="5a8a1-252">The Cluster Resource Manager may fill nodes up to their total capacity (consuming the buffer) during upgrades if necessary.</span></span>

## <a name="next-steps"></a><span data-ttu-id="5a8a1-253">Étapes suivantes</span><span class="sxs-lookup"><span data-stu-id="5a8a1-253">Next steps</span></span>
* <span data-ttu-id="5a8a1-254">Commencez au début pour [obtenir une présentation de Service Fabric Cluster Resource Manager](service-fabric-cluster-resource-manager-introduction.md)</span><span class="sxs-lookup"><span data-stu-id="5a8a1-254">Start from the beginning and [get an Introduction to the Service Fabric Cluster Resource Manager](service-fabric-cluster-resource-manager-introduction.md)</span></span>
