---
title: "Guide du langage de spécification des réseaux neuronaux Net# | Microsoft Docs"
description: "Syntaxe pour le neuronaux Net # réseaux langage de spécification, ainsi que des exemples montrant comment créer un modèle de réseau neuronal personnalisé dans Microsoft Azure ML à l’aide de Net #"
services: machine-learning
documentationcenter: 
author: jeannt
manager: jhubbard
editor: cgronlun
ms.assetid: cfd1454b-47df-4745-b064-ce5f9b3be303
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 03/31/2017
ms.author: jeannt
ms.openlocfilehash: 965c60ffde55041cc3864d06d81f5590c7ea1c11
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 07/11/2017
---
# <a name="guide-to-net-neural-network-specification-language-for-azure-machine-learning"></a><span data-ttu-id="84cf4-103">Guide du langage de spécification des réseaux neuronaux Net# pour Azure Machine Learning</span><span class="sxs-lookup"><span data-stu-id="84cf4-103">Guide to Net# neural network specification language for Azure Machine Learning</span></span>
## <a name="overview"></a><span data-ttu-id="84cf4-104">Vue d'ensemble</span><span class="sxs-lookup"><span data-stu-id="84cf4-104">Overview</span></span>
<span data-ttu-id="84cf4-105">Net# est un langage développé par Microsoft qui définit les architectures de réseaux neuronaux.</span><span class="sxs-lookup"><span data-stu-id="84cf4-105">Net# is a language developed by Microsoft that is used to define neural network architectures.</span></span> <span data-ttu-id="84cf4-106">Vous pouvez utiliser Net# dans des modules de réseau neuronal dans Microsoft Azure Machine Learning.</span><span class="sxs-lookup"><span data-stu-id="84cf4-106">You can use Net# in neural network modules in Microsoft Azure Machine Learning.</span></span>

<!-- This function doesn't currentlyappear in the MicrosoftML documentation. If it is added in a future update, we can uncomment this text.

, or in the `rxNeuralNetwork()` function in [MicrosoftML](https://msdn.microsoft.com/microsoft-r/microsoftml/microsoftml). 

-->

<span data-ttu-id="84cf4-107">Dans cet article, vous découvrirez les concepts de base nécessaires au développement d’un réseau neuronal personnalisé :</span><span class="sxs-lookup"><span data-stu-id="84cf4-107">In this article, you will learn basic concepts needed to develop a custom neural network:</span></span> 

* <span data-ttu-id="84cf4-108">Conditions des réseaux neuronaux et définition des principaux composants</span><span class="sxs-lookup"><span data-stu-id="84cf4-108">Neural network requirements and how to define the primary components</span></span>
* <span data-ttu-id="84cf4-109">Syntaxe et mots clés du langage de spécification Net#</span><span class="sxs-lookup"><span data-stu-id="84cf4-109">The syntax and keywords of the Net# specification language</span></span>
* <span data-ttu-id="84cf4-110">Exemples de réseau neuronal personnalisé créé à l’aide de Net #</span><span class="sxs-lookup"><span data-stu-id="84cf4-110">Examples of custom neural networks created using Net#</span></span> 

[!INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]

## <a name="neural-network-basics"></a><span data-ttu-id="84cf4-111">Principes fondamentaux des réseaux neuronaux</span><span class="sxs-lookup"><span data-stu-id="84cf4-111">Neural network basics</span></span>
<span data-ttu-id="84cf4-112">Un réseau neuronal se compose de ***nœuds***, organisés en ***couches***, et de ***connexions*** pondérées (ou ***bords***) reliant les nœuds.</span><span class="sxs-lookup"><span data-stu-id="84cf4-112">A neural network structure consists of ***nodes*** that are organized in ***layers***, and weighted ***connections*** (or ***edges***) between the nodes.</span></span> <span data-ttu-id="84cf4-113">Les connexions sont directionnelles ; chaque connexion présente un nœud ***source*** et un nœud de ***destination***.</span><span class="sxs-lookup"><span data-stu-id="84cf4-113">The connections are directional, and each connection has a ***source*** node and a ***destination*** node.</span></span>  

<span data-ttu-id="84cf4-114">Chaque ***couche apte à l’apprentissage*** (couche masquée ou de sortie) présente un ou plusieurs ***faisceaux de connexions***.</span><span class="sxs-lookup"><span data-stu-id="84cf4-114">Each ***trainable layer*** (a hidden or an output layer) has one or more ***connection bundles***.</span></span> <span data-ttu-id="84cf4-115">Un faisceau de connexions se compose d'une couche source et d'une définition des connexions de cette couche.</span><span class="sxs-lookup"><span data-stu-id="84cf4-115">A connection bundle consists of a source layer and a specification of the connections from that source layer.</span></span> <span data-ttu-id="84cf4-116">Toutes les connexions d’un même faisceau partagent la même ***couche source*** et la même ***couche de destination***.</span><span class="sxs-lookup"><span data-stu-id="84cf4-116">All the connections in a given bundle share the same ***source layer*** and the same ***destination layer***.</span></span> <span data-ttu-id="84cf4-117">En Net#, on considère qu'un faisceau de connexions appartient à sa couche de destination.</span><span class="sxs-lookup"><span data-stu-id="84cf4-117">In Net#, a connection bundle is considered as belonging to the bundle's destination layer.</span></span>  

<span data-ttu-id="84cf4-118">Net# prend en charge différents types de faisceau de connexions, ce qui vous permet de personnaliser la façon dont les entrées sont mappées aux couches masquées et aux sorties.</span><span class="sxs-lookup"><span data-stu-id="84cf4-118">Net# supports various kinds of connection bundles, which lets you customize the way inputs are mapped to hidden layers and mapped to the outputs.</span></span>   

<span data-ttu-id="84cf4-119">Le faisceau par défaut ou standard est un **faisceau complet**, dans lequel chaque nœud de la couche source est connecté à tous les nœuds de la couche de destination.</span><span class="sxs-lookup"><span data-stu-id="84cf4-119">The default or standard bundle is a **full bundle**, in which each node in the source layer is connected to every node in the destination layer.</span></span>  

<span data-ttu-id="84cf4-120">En outre, Net# prend en charge les quatre types de faisceaux de connexions avancés suivants :</span><span class="sxs-lookup"><span data-stu-id="84cf4-120">Additionally, Net# supports the following four kinds of advanced connection bundles:</span></span>  

* <span data-ttu-id="84cf4-121">**Faisceaux filtrés**.</span><span class="sxs-lookup"><span data-stu-id="84cf4-121">**Filtered bundles**.</span></span> <span data-ttu-id="84cf4-122">L’utilisateur peut définir un prédicat à l’aide de l’emplacement du nœud de la couche source et de celui de la couche de destination.</span><span class="sxs-lookup"><span data-stu-id="84cf4-122">The user can define a predicate by using the locations of the source layer node and the destination layer node.</span></span> <span data-ttu-id="84cf4-123">Les nœuds sont connectés chaque fois que le prédicat a la valeur True.</span><span class="sxs-lookup"><span data-stu-id="84cf4-123">Nodes are connected whenever the predicate is True.</span></span>
* <span data-ttu-id="84cf4-124">**Faisceaux convolutionnels**.</span><span class="sxs-lookup"><span data-stu-id="84cf4-124">**Convolutional bundles**.</span></span> <span data-ttu-id="84cf4-125">L'utilisateur peut définir de petits voisinages de nœuds dans la couche source.</span><span class="sxs-lookup"><span data-stu-id="84cf4-125">The user can define small neighborhoods of nodes in the source layer.</span></span> <span data-ttu-id="84cf4-126">Chaque nœud de la couche de destination est connecté à un voisinage de nœuds de la couche source.</span><span class="sxs-lookup"><span data-stu-id="84cf4-126">Each node in the destination layer is connected to one neighborhood of nodes in the source layer.</span></span>
* <span data-ttu-id="84cf4-127">**Faisceaux de regroupement** et **faisceaux de normalisation de réponse**.</span><span class="sxs-lookup"><span data-stu-id="84cf4-127">**Pooling bundles** and **Response normalization bundles**.</span></span> <span data-ttu-id="84cf4-128">Ces faisceaux sont similaires aux faisceaux convolutionnels dans le sens où l'utilisateur définit de petits voisinages de nœuds dans la couche source.</span><span class="sxs-lookup"><span data-stu-id="84cf4-128">These are similar to convolutional bundles in that the user defines small neighborhoods of nodes in the source layer.</span></span> <span data-ttu-id="84cf4-129">Il existe une différence : les poids des bords dans ces faisceaux ne sont pas aptes à l’apprentissage.</span><span class="sxs-lookup"><span data-stu-id="84cf4-129">The difference is that the weights of the edges in these bundles are not trainable.</span></span> <span data-ttu-id="84cf4-130">Au lieu de cela, une fonction prédéfinie est appliquée aux valeurs de nœud source, afin de déterminer la valeur du nœud de destination.</span><span class="sxs-lookup"><span data-stu-id="84cf4-130">Instead, a predefined function is applied to the source node values to determine the destination node value.</span></span>  

<span data-ttu-id="84cf4-131">L’utilisation de Net# pour définir la structure d’un réseau neuronal permet de spécifier des structures complexes, comme des réseaux neuronaux profonds ou des convolutions de dimensions arbitraires, connues pour améliorer l’apprentissage à partir de données telles que des images, des sons ou des vidéos.</span><span class="sxs-lookup"><span data-stu-id="84cf4-131">Using Net# to define the structure of a neural network makes it possible to define complex structures such as deep neural networks or convolutions of arbitrary dimensions, which are known to improve learning on data such as image, audio, or video.</span></span>  

## <a name="supported-customizations"></a><span data-ttu-id="84cf4-132">Personnalisations prises en charge</span><span class="sxs-lookup"><span data-stu-id="84cf4-132">Supported customizations</span></span>
<span data-ttu-id="84cf4-133">L’architecture des modèles de réseau neuronal que vous créez dans Microsoft Azure Machine Learning peut bénéficier d’une personnalisation avancée grâce à Net#.</span><span class="sxs-lookup"><span data-stu-id="84cf4-133">The architecture of neural network models that you create in Azure Machine Learning can be extensively customized by using Net#.</span></span> <span data-ttu-id="84cf4-134">Vous pouvez :</span><span class="sxs-lookup"><span data-stu-id="84cf4-134">You can:</span></span>  

* <span data-ttu-id="84cf4-135">créer des couches masquées et contrôler le nombre de nœuds dans chaque couche ;</span><span class="sxs-lookup"><span data-stu-id="84cf4-135">Create hidden layers and control the number of nodes in each layer.</span></span>
* <span data-ttu-id="84cf4-136">spécifier la façon dont les couches doivent être connectées les unes aux autres ;</span><span class="sxs-lookup"><span data-stu-id="84cf4-136">Specify how layers are to be connected to each other.</span></span>
* <span data-ttu-id="84cf4-137">définir des structures de connectivité spéciales, telles que des convolutions et des faisceaux à poids partagé ;</span><span class="sxs-lookup"><span data-stu-id="84cf4-137">Define special connectivity structures, such as convolutions and weight sharing bundles.</span></span>
* <span data-ttu-id="84cf4-138">spécifier différentes fonctions d'activation.</span><span class="sxs-lookup"><span data-stu-id="84cf4-138">Specify different activation functions.</span></span>  

<span data-ttu-id="84cf4-139">Pour plus d’informations sur la syntaxe du langage de spécification, consultez la section consacrée à la [définition de la structure](#Structure-specifications).</span><span class="sxs-lookup"><span data-stu-id="84cf4-139">For details of the specification language syntax, see [Structure Specification](#Structure-specifications).</span></span>  

<span data-ttu-id="84cf4-140">Pour des exemples de définition de réseaux neuronaux pour certaines tâches standard d’apprentissage automatique, simples comme complexes, consultez la section [Exemples](#Examples-of-Net#-usage).</span><span class="sxs-lookup"><span data-stu-id="84cf4-140">For examples of defining neural networks for some common machine learning tasks, from simplex to complex, see [Examples](#Examples-of-Net#-usage).</span></span>  

## <a name="general-requirements"></a><span data-ttu-id="84cf4-141">Conditions générales</span><span class="sxs-lookup"><span data-stu-id="84cf4-141">General requirements</span></span>
* <span data-ttu-id="84cf4-142">Il doit y avoir exactement une couche de sortie, au moins une couche d’entrée et aucune ou plusieurs couches masquées.</span><span class="sxs-lookup"><span data-stu-id="84cf4-142">There must be exactly one output layer, at least one input layer, and zero or more hidden layers.</span></span> 
* <span data-ttu-id="84cf4-143">Chaque couche a un nombre fixe de nœuds, organisés de façon conceptuelle dans un tableau rectangulaire aux dimensions arbitraires.</span><span class="sxs-lookup"><span data-stu-id="84cf4-143">Each layer has a fixed number of nodes, conceptually arranged in a rectangular array of arbitrary dimensions.</span></span> 
* <span data-ttu-id="84cf4-144">Les couches d'entrée n'ont pas de paramètre formé associé et représentent le point où les données d'instance entrent dans le réseau.</span><span class="sxs-lookup"><span data-stu-id="84cf4-144">Input layers have no associated trained parameters and represent the point where instance data enters the network.</span></span> 
* <span data-ttu-id="84cf4-145">Les couches aptes à l’apprentissage (couches masquées et de sortie) présentent des paramètres formés associés, appelés poids et biais.</span><span class="sxs-lookup"><span data-stu-id="84cf4-145">Trainable layers (the hidden and output layers) have associated trained parameters, known as weights and biases.</span></span> 
* <span data-ttu-id="84cf4-146">Les nœuds source et de destination doivent se trouver sur des couches indépendantes.</span><span class="sxs-lookup"><span data-stu-id="84cf4-146">The source and destination nodes must be in separate layers.</span></span> 
* <span data-ttu-id="84cf4-147">Les connexions doivent être non cycliques. En d'autres termes, il ne peut pas y avoir une chaîne de connexions ramenant au nœud source initial.</span><span class="sxs-lookup"><span data-stu-id="84cf4-147">Connections must be acyclic; in other words, there cannot be a chain of connections leading back to the initial source node.</span></span>
* <span data-ttu-id="84cf4-148">La couche de sortie ne peut pas être la couche source d'un faisceau de connexions.</span><span class="sxs-lookup"><span data-stu-id="84cf4-148">The output layer cannot be a source layer of a connection bundle.</span></span>  

## <a name="structure-specifications"></a><span data-ttu-id="84cf4-149">Spécifications de structures</span><span class="sxs-lookup"><span data-stu-id="84cf4-149">Structure specifications</span></span>
<span data-ttu-id="84cf4-150">Une spécification de structure de réseau neuronal se compose de trois sections : la **déclaration de constante**, la **déclaration de couche** et la **déclaration de connexion**.</span><span class="sxs-lookup"><span data-stu-id="84cf4-150">A neural network structure specification is composed of three sections: the **constant declaration**, the **layer declaration**, the **connection declaration**.</span></span> <span data-ttu-id="84cf4-151">Il existe également une section de **déclaration de partage**, qui est facultative.</span><span class="sxs-lookup"><span data-stu-id="84cf4-151">There is also an optional **share declaration** section.</span></span> <span data-ttu-id="84cf4-152">Ces sections peuvent être définies dans n'importe quel ordre.</span><span class="sxs-lookup"><span data-stu-id="84cf4-152">The sections can be specified in any order.</span></span>  

## <a name="constant-declaration"></a><span data-ttu-id="84cf4-153">Déclaration de constante</span><span class="sxs-lookup"><span data-stu-id="84cf4-153">Constant declaration</span></span>
<span data-ttu-id="84cf4-154">Ce type de déclaration est facultatif.</span><span class="sxs-lookup"><span data-stu-id="84cf4-154">A constant declaration is optional.</span></span> <span data-ttu-id="84cf4-155">Elle offre une méthode de définition des valeurs utilisées à d’autres emplacements de la définition du réseau neuronal.</span><span class="sxs-lookup"><span data-stu-id="84cf4-155">It provides a means to define values used elsewhere in the neural network definition.</span></span> <span data-ttu-id="84cf4-156">L'instruction de déclaration se compose d'un identifiant suivi du signe égal et d'une expression de valeur.</span><span class="sxs-lookup"><span data-stu-id="84cf4-156">The declaration statement consists of an identifier followed by an equal sign and a value expression.</span></span>   

<span data-ttu-id="84cf4-157">Par exemple, l’instruction suivante définit une constante **x** :</span><span class="sxs-lookup"><span data-stu-id="84cf4-157">For example, the following statement defines a constant **x**:</span></span>  

    Const X = 28;  

<span data-ttu-id="84cf4-158">Pour définir simultanément deux constantes ou plus, mettez les noms d’identificateur et les valeurs associées entre accolades, en les séparant par des points-virgules.</span><span class="sxs-lookup"><span data-stu-id="84cf4-158">To define two or more constants simultaneously, enclose the identifier names and values in braces, and separate them by using semicolons.</span></span> <span data-ttu-id="84cf4-159">Par exemple :</span><span class="sxs-lookup"><span data-stu-id="84cf4-159">For example:</span></span>  

    Const { X = 28; Y = 4; }  

<span data-ttu-id="84cf4-160">Le côté droit de chaque expression d’affectation peut être un entier, un nombre réel, une valeur booléenne (vrai/faux) ou une expression mathématique.</span><span class="sxs-lookup"><span data-stu-id="84cf4-160">The right-hand side of each assignment expression can be an integer, a real number, a Boolean value (True or False), or a mathematical expression.</span></span> <span data-ttu-id="84cf4-161">Par exemple :</span><span class="sxs-lookup"><span data-stu-id="84cf4-161">For example:</span></span>  

    Const { X = 17 * 2; Y = true; }  

## <a name="layer-declaration"></a><span data-ttu-id="84cf4-162">Déclaration de couche</span><span class="sxs-lookup"><span data-stu-id="84cf4-162">Layer declaration</span></span>
<span data-ttu-id="84cf4-163">La déclaration de couche est requise.</span><span class="sxs-lookup"><span data-stu-id="84cf4-163">The layer declaration is required.</span></span> <span data-ttu-id="84cf4-164">Elle définit la taille et la source de la couche, y compris ses attributs et faisceaux de connexions.</span><span class="sxs-lookup"><span data-stu-id="84cf4-164">It defines the size and source of the layer, including its connection bundles and attributes.</span></span> <span data-ttu-id="84cf4-165">L’instruction de déclaration commence par le nom de la couche (d’entrée, masquée ou de sortie), suivi de ses dimensions (un tuple d’entiers positifs).</span><span class="sxs-lookup"><span data-stu-id="84cf4-165">The declaration statement starts with the name of the layer (input, hidden, or output), followed by the dimensions of the layer (a tuple of positive integers).</span></span> <span data-ttu-id="84cf4-166">Par exemple :</span><span class="sxs-lookup"><span data-stu-id="84cf4-166">For example:</span></span>  

    input Data auto;
    hidden Hidden[5,20] from Data all;
    output Result[2] from Hidden all;  

* <span data-ttu-id="84cf4-167">Le produit des dimensions est le nombre de nœuds de la couche.</span><span class="sxs-lookup"><span data-stu-id="84cf4-167">The product of the dimensions is the number of nodes in the layer.</span></span> <span data-ttu-id="84cf4-168">Dans cet exemple, il y a deux dimensions [5,20], ce qui signifie qu’il y a 100 nœuds dans la couche.</span><span class="sxs-lookup"><span data-stu-id="84cf4-168">In this example, there are two dimensions [5,20], which means there are  100 nodes in the layer.</span></span>
* <span data-ttu-id="84cf4-169">Les couches peuvent être déclarés dans n'importe quel ordre, à une exception près : si plusieurs couches d'entrée sont définies, l'ordre dans lequel elles sont déclarées doit être le même que celui des fonctions dans les données d'entrée.</span><span class="sxs-lookup"><span data-stu-id="84cf4-169">The layers can be declared in any order, with one exception: If more than one input layer is defined, the order in which they are declared must match the order of features in the input data.</span></span>  

<span data-ttu-id="84cf4-170">Pour spécifier que le nombre de nœuds d’une couche doit être déterminé automatiquement, utilisez le mot clé **auto**.</span><span class="sxs-lookup"><span data-stu-id="84cf4-170">To specify that the number of nodes in a layer be determined automatically, use the **auto** keyword.</span></span> <span data-ttu-id="84cf4-171">Le mot clé **auto** peut avoir différents effets selon le type de couche :</span><span class="sxs-lookup"><span data-stu-id="84cf4-171">The **auto** keyword has different effects, depending on the layer:</span></span>  

* <span data-ttu-id="84cf4-172">Dans une déclaration de couche d'entrée, le nombre de nœuds est le nombre de caractéristiques dans les données d'entrée.</span><span class="sxs-lookup"><span data-stu-id="84cf4-172">In an input layer declaration, the number of nodes is the number of features in the input data.</span></span>
* <span data-ttu-id="84cf4-173">Dans une déclaration de couche masquée, le nombre de nœuds correspond au nombre spécifié par la valeur du paramètre définissant le **Nombre de nœuds masqués**.</span><span class="sxs-lookup"><span data-stu-id="84cf4-173">In a hidden layer declaration, the number of nodes is the number that is specified by the parameter value for **Number of hidden nodes**.</span></span> 
* <span data-ttu-id="84cf4-174">Dans une déclaration de couche de sortie, le nombre de nœuds est de 2 pour les classifications à deux classes, de 1 pour la régression et égal au nombre de nœuds de sortie pour une classification multiclasse.</span><span class="sxs-lookup"><span data-stu-id="84cf4-174">In an output layer declaration, the number of nodes is 2 for two-class classification, 1 for regression, and equal to the number of output nodes for multiclass classification.</span></span>   

<span data-ttu-id="84cf4-175">Par exemple, la définition de réseau ci-après permet de déterminer automatiquement la taille de toutes les couches :</span><span class="sxs-lookup"><span data-stu-id="84cf4-175">For example, the following network definition allows the size of all layers to be automatically determined:</span></span>  

    input Data auto;
    hidden Hidden auto from Data all;
    output Result auto from Hidden all;  


<span data-ttu-id="84cf4-176">Une déclaration d’une couche apte à l’apprentissage (couche masquée ou de sortie) peut éventuellement inclure la fonction de sortie (également appelée fonction d’activation), configurée par défaut sur **sigmoid** pour des modèles de classification, et **linéaire** pour des modèles de régression.</span><span class="sxs-lookup"><span data-stu-id="84cf4-176">A layer declaration for a trainable layer (the hidden or output layers) can optionally include the output function (also called an activation function), which defaults to **sigmoid** for classification models, and **linear** for regression models.</span></span> <span data-ttu-id="84cf4-177">(Même si vous utilisez la valeur par défaut, vous pouvez déclarer explicitement la fonction d'activation, si vous le souhaitez pour plus de clarté.)</span><span class="sxs-lookup"><span data-stu-id="84cf4-177">(Even if you use the default, you can explicitly state the activation function, if desired for clarity.)</span></span>

<span data-ttu-id="84cf4-178">Les fonctions de sortie suivantes sont prises en charge :</span><span class="sxs-lookup"><span data-stu-id="84cf4-178">The following output functions are supported:</span></span>  

* <span data-ttu-id="84cf4-179">sigmoid</span><span class="sxs-lookup"><span data-stu-id="84cf4-179">sigmoid</span></span>
* <span data-ttu-id="84cf4-180">linear</span><span class="sxs-lookup"><span data-stu-id="84cf4-180">linear</span></span>
* <span data-ttu-id="84cf4-181">softmax</span><span class="sxs-lookup"><span data-stu-id="84cf4-181">softmax</span></span>
* <span data-ttu-id="84cf4-182">rlinear</span><span class="sxs-lookup"><span data-stu-id="84cf4-182">rlinear</span></span>
* <span data-ttu-id="84cf4-183">square</span><span class="sxs-lookup"><span data-stu-id="84cf4-183">square</span></span>
* <span data-ttu-id="84cf4-184">sqrt</span><span class="sxs-lookup"><span data-stu-id="84cf4-184">sqrt</span></span>
* <span data-ttu-id="84cf4-185">srlinear</span><span class="sxs-lookup"><span data-stu-id="84cf4-185">srlinear</span></span>
* <span data-ttu-id="84cf4-186">abs</span><span class="sxs-lookup"><span data-stu-id="84cf4-186">abs</span></span>
* <span data-ttu-id="84cf4-187">tanh</span><span class="sxs-lookup"><span data-stu-id="84cf4-187">tanh</span></span> 
* <span data-ttu-id="84cf4-188">brlinear</span><span class="sxs-lookup"><span data-stu-id="84cf4-188">brlinear</span></span>  

<span data-ttu-id="84cf4-189">Par exemple, la déclaration suivante utilise la fonction **softmax** :</span><span class="sxs-lookup"><span data-stu-id="84cf4-189">For example, the following declaration uses the **softmax** function:</span></span>  

    output Result [100] softmax from Hidden all;  

## <a name="connection-declaration"></a><span data-ttu-id="84cf4-190">Déclaration de connexion</span><span class="sxs-lookup"><span data-stu-id="84cf4-190">Connection declaration</span></span>
<span data-ttu-id="84cf4-191">Immédiatement après avoir défini la couche entraînable, vous devez déclarer les connexions entre les couches définies.</span><span class="sxs-lookup"><span data-stu-id="84cf4-191">Immediately after defining the trainable layer, you must declare connections among the layers you have defined.</span></span> <span data-ttu-id="84cf4-192">Une déclaration de faisceau de connexions commence par le mot clé **from**, suivi du nom de la couche source du faisceau et du genre de faisceau à créer.</span><span class="sxs-lookup"><span data-stu-id="84cf4-192">The connection bundle declaration starts with the keyword **from**, followed by the name of the bundle's source layer and the kind of connection bundle to create.</span></span>   

<span data-ttu-id="84cf4-193">À ce jour, cinq types de faisceau de connexions sont pris en charge :</span><span class="sxs-lookup"><span data-stu-id="84cf4-193">Currently, five kinds of connection bundles are supported:</span></span>  

* <span data-ttu-id="84cf4-194">Faisceau **complet**, signalé par le mot clé **all**</span><span class="sxs-lookup"><span data-stu-id="84cf4-194">**Full** bundles, indicated by the keyword **all**</span></span>
* <span data-ttu-id="84cf4-195">Faisceau **filtré**, signalé par le mot clé **where**, suivi par une expression de prédicat</span><span class="sxs-lookup"><span data-stu-id="84cf4-195">**Filtered** bundles, indicated by the keyword **where**, followed by a predicate expression</span></span>
* <span data-ttu-id="84cf4-196">Faisceau **convolutionnel**, signalé par le mot clé **convolve**, suivi des attributs de convolution</span><span class="sxs-lookup"><span data-stu-id="84cf4-196">**Convolutional** bundles, indicated by the keyword **convolve**, followed by the convolution attributes</span></span>
* <span data-ttu-id="84cf4-197">Faisceau de **regroupement**, signalé par les mots clés **max pool** ou **mean pool**</span><span class="sxs-lookup"><span data-stu-id="84cf4-197">**Pooling** bundles, indicated by the keywords **max pool** or **mean pool**</span></span>
* <span data-ttu-id="84cf4-198">Faisceau de **normalisation de réponse**, signalé par le mot clé **response norm**</span><span class="sxs-lookup"><span data-stu-id="84cf4-198">**Response normalization** bundles, indicated by the keyword **response norm**</span></span>      

## <a name="full-bundles"></a><span data-ttu-id="84cf4-199">Faisceaux complets</span><span class="sxs-lookup"><span data-stu-id="84cf4-199">Full bundles</span></span>
<span data-ttu-id="84cf4-200">Les faisceaux de connexions complets incluent une connexion entre chaque nœud de la couche source et chaque nœud de la couche de destination.</span><span class="sxs-lookup"><span data-stu-id="84cf4-200">A full connection bundle includes a connection from each node in the source layer to each node in the destination layer.</span></span> <span data-ttu-id="84cf4-201">Il s'agit du type de connexion réseau par défaut.</span><span class="sxs-lookup"><span data-stu-id="84cf4-201">This is the default network connection type.</span></span>  

## <a name="filtered-bundles"></a><span data-ttu-id="84cf4-202">Faisceaux filtrés</span><span class="sxs-lookup"><span data-stu-id="84cf4-202">Filtered bundles</span></span>
<span data-ttu-id="84cf4-203">Une spécification de faisceau de connexion filtré inclut un prédicat, dont la syntaxe est assez similaire à celle d'une expression lambda C#.</span><span class="sxs-lookup"><span data-stu-id="84cf4-203">A filtered connection bundle specification includes a predicate, expressed syntactically, much like a C# lambda expression.</span></span> <span data-ttu-id="84cf4-204">L'exemple suivant est une définition de deux faisceaux filtrés :</span><span class="sxs-lookup"><span data-stu-id="84cf4-204">The following example defines two filtered bundles:</span></span>  

    input Pixels [10, 20];
    hidden ByRow[10, 12] from Pixels where (s,d) => s[0] == d[0];
    hidden ByCol[5, 20] from Pixels where (s,d) => abs(s[1] - d[1]) <= 1;  

* <span data-ttu-id="84cf4-205">Dans le prédicat de *ByRow*, **s** est un paramètre représentant un index dans le tableau rectangulaire de nœuds de la couche d’entrée, *Pixels*, tandis que **d** est un paramètre représentant un index dans le tableau de nœuds de la couche masquée, *ByRow*.</span><span class="sxs-lookup"><span data-stu-id="84cf4-205">In the predicate for *ByRow*, **s** is a parameter representing an index into the rectangular array of nodes of the input layer, *Pixels*, and **d** is a parameter representing an index into the array of nodes of the hidden layer, *ByRow*.</span></span> <span data-ttu-id="84cf4-206">Le type de **s** et de **d** est un tuple d’entiers de longueur 2.</span><span class="sxs-lookup"><span data-stu-id="84cf4-206">The type of both **s** and **d** is a tuple of integers of length two.</span></span> <span data-ttu-id="84cf4-207">D’un point de vue conceptuel, **s** couvre toutes les paires d’entiers avec *0 <= s[0] < 10* et *0 <= s[1] < 20*, tandis que **d** couvre toutes les paires d’entiers avec *0 <= d[0] < 10* et *0 <= d[1] < 12*.</span><span class="sxs-lookup"><span data-stu-id="84cf4-207">Conceptually, **s** ranges over all pairs of integers with *0 <= s[0] < 10* and *0 <= s[1] < 20*, and **d** ranges over all pairs of integers, with *0 <= d[0] < 10* and *0 <= d[1] < 12*.</span></span> 
* <span data-ttu-id="84cf4-208">Sur la droite de l'expression de prédicat se trouve une condition.</span><span class="sxs-lookup"><span data-stu-id="84cf4-208">On the right-hand side of the predicate expression, there is a condition.</span></span> <span data-ttu-id="84cf4-209">Dans cet exemple, pour chaque valeur de **s** et **d** permettant à la condition d’avoir la valeur True, il existe un bord à partir du nœud de couche source vers le nœud de couche de destination.</span><span class="sxs-lookup"><span data-stu-id="84cf4-209">In this example, for every value of **s** and **d** such that the condition is True, there is an edge from the source layer node to the destination layer node.</span></span> <span data-ttu-id="84cf4-210">Ainsi, cette expression de filtre indique que le faisceau inclut une connexion du nœud défini par **s** au nœud défini par **d**, dans tous les cas où s[0] est égal à d[0].</span><span class="sxs-lookup"><span data-stu-id="84cf4-210">Thus, this filter expression indicates that the bundle includes a connection from the node defined by **s** to the node defined by **d** in all cases where s[0] is equal to d[0].</span></span>  

<span data-ttu-id="84cf4-211">Vous pouvez également spécifier un ensemble de poids pour un faisceau filtré.</span><span class="sxs-lookup"><span data-stu-id="84cf4-211">Optionally, you can specify a set of weights for a filtered bundle.</span></span> <span data-ttu-id="84cf4-212">La valeur de l’attribut **Weights** doit être un tuple de valeurs à virgule flottante dont la longueur est égale au nombre de connexions défini par le faisceau.</span><span class="sxs-lookup"><span data-stu-id="84cf4-212">The value for the **Weights** attribute must be a tuple of floating point values with a length that matches the number of connections defined by the bundle.</span></span> <span data-ttu-id="84cf4-213">Par défaut, les poids sont générés de façon aléatoire.</span><span class="sxs-lookup"><span data-stu-id="84cf4-213">By default, weights are randomly generated.</span></span>  

<span data-ttu-id="84cf4-214">Les valeurs de poids sont regroupées par l'index de nœuds de destination.</span><span class="sxs-lookup"><span data-stu-id="84cf4-214">Weight values are grouped by the destination node index.</span></span> <span data-ttu-id="84cf4-215">Ainsi, si le premier nœud de destination est connecté à K nœuds sources, les *K* premiers éléments du tuple **Weights** correspondent aux poids du premier nœud de destination, selon l’ordre de l’index source.</span><span class="sxs-lookup"><span data-stu-id="84cf4-215">That is, if the first destination node is connected to K source nodes, the first *K* elements of the **Weights** tuple are the weights for the first destination node, in source index order.</span></span> <span data-ttu-id="84cf4-216">Il en va de même pour les nœuds de destination restants.</span><span class="sxs-lookup"><span data-stu-id="84cf4-216">The same applies for the remaining destination nodes.</span></span>  

<span data-ttu-id="84cf4-217">Il est possible de spécifier les poids directement comme des valeurs constantes.</span><span class="sxs-lookup"><span data-stu-id="84cf4-217">It's possible to specify weights directly as constant values.</span></span> <span data-ttu-id="84cf4-218">Par exemple, si vous connaissez déjà les poids, vous pouvez les spécifier en tant que constantes à l'aide de cette syntaxe :</span><span class="sxs-lookup"><span data-stu-id="84cf4-218">For example, if you learned the weights previously, you can specify them as constants using this syntax:</span></span>

    const Weights_1 = [0.0188045055, 0.130500451, ...]


## <a name="convolutional-bundles"></a><span data-ttu-id="84cf4-219">Faisceaux convolutionnels</span><span class="sxs-lookup"><span data-stu-id="84cf4-219">Convolutional bundles</span></span>
<span data-ttu-id="84cf4-220">Lorsque les données d’apprentissage présentent une structure homogène, des connexions convolutionnelles sont généralement utilisées pour l’apprentissage des caractéristiques de haut niveau des données.</span><span class="sxs-lookup"><span data-stu-id="84cf4-220">When the training data has a homogeneous structure, convolutional connections are commonly used to learn high-level features of the data.</span></span> <span data-ttu-id="84cf4-221">Par exemple, pour les données images, audio ou vidéo, la dimensionnalité spatiale ou temporelle peut être assez uniforme.</span><span class="sxs-lookup"><span data-stu-id="84cf4-221">For example, in image, audio, or video data, spatial or temporal dimensionality can be fairly uniform.</span></span>  

<span data-ttu-id="84cf4-222">Les faisceaux convolutionnels emploient des **noyaux** rectangulaires, glissés à travers les dimensions.</span><span class="sxs-lookup"><span data-stu-id="84cf4-222">Convolutional bundles employ rectangular **kernels** that are slid through the dimensions.</span></span> <span data-ttu-id="84cf4-223">En gros, chaque noyau définit un ensemble de poids appliqués dans les voisinages locaux, que l’on appelle **applications de noyau**.</span><span class="sxs-lookup"><span data-stu-id="84cf4-223">Essentially, each kernel defines a set of weights applied in local neighborhoods, referred to as **kernel applications**.</span></span> <span data-ttu-id="84cf4-224">Chaque application de noyau correspond à un nœud de la couche source appelé **nœud central**.</span><span class="sxs-lookup"><span data-stu-id="84cf4-224">Each kernel application corresponds to a node in the source layer, which is referred to as the **central node**.</span></span> <span data-ttu-id="84cf4-225">Les poids d'un noyau sont partagés entre de nombreuses connexions.</span><span class="sxs-lookup"><span data-stu-id="84cf4-225">The weights of a kernel are shared among many connections.</span></span> <span data-ttu-id="84cf4-226">Dans un faisceau convolutionnel, chaque noyau est rectangulaire et toutes les applications de noyau sont de la même taille.</span><span class="sxs-lookup"><span data-stu-id="84cf4-226">In a convolutional bundle, each kernel is rectangular and all kernel applications are the same size.</span></span>  

<span data-ttu-id="84cf4-227">Les faisceaux convolutionnels prennent en charge les attributs suivants :</span><span class="sxs-lookup"><span data-stu-id="84cf4-227">Convolutional bundles support the following attributes:</span></span>

<span data-ttu-id="84cf4-228">**InputShape** définit la dimensionnalité de la couche source pour ce faisceau convolutionnel.</span><span class="sxs-lookup"><span data-stu-id="84cf4-228">**InputShape** defines the dimensionality of the source layer for the purposes of this convolutional bundle.</span></span> <span data-ttu-id="84cf4-229">Cette valeur doit être un tuple d’entiers positifs.</span><span class="sxs-lookup"><span data-stu-id="84cf4-229">The value must be a tuple of positive integers.</span></span> <span data-ttu-id="84cf4-230">Le produit de ces entiers doit être égal au nombre de nœuds de la couche source, mais ne doit pas forcément correspondre à la dimensionnalité déclarée pour celle-ci.</span><span class="sxs-lookup"><span data-stu-id="84cf4-230">The product of the integers must equal the number of nodes in the source layer, but otherwise, it does not need to match the dimensionality declared for the source layer.</span></span> <span data-ttu-id="84cf4-231">La longueur de ce tuple devient la valeur **arity** (arité) du faisceau convolutionnel.</span><span class="sxs-lookup"><span data-stu-id="84cf4-231">The length of this tuple becomes the **arity** value for the convolutional bundle.</span></span> <span data-ttu-id="84cf4-232">En général, l’arité fait référence au nombre d’arguments ou d’opérandes qu’une fonction peut accepter.</span><span class="sxs-lookup"><span data-stu-id="84cf4-232">(Typically arity refers to the number of arguments or operands that a function can take.)</span></span>  

<span data-ttu-id="84cf4-233">Pour définir la forme et les emplacements des noyaux, utilisez les attributs **KernelShape**, **Stride**, **Padding**, **LowerPad** et **UpperPad** :</span><span class="sxs-lookup"><span data-stu-id="84cf4-233">To define the shape and locations of the kernels, use the attributes **KernelShape**, **Stride**, **Padding**, **LowerPad**, and **UpperPad**:</span></span>   

* <span data-ttu-id="84cf4-234">**KernelShape** : (obligatoire) définit la dimensionnalité de chaque noyau du faisceau convolutionnel.</span><span class="sxs-lookup"><span data-stu-id="84cf4-234">**KernelShape**: (required) Defines the dimensionality of each kernel for the convolutional bundle.</span></span> <span data-ttu-id="84cf4-235">Cette valeur doit être un tuple d’entiers positifs, dont la longueur est égale à l’arité du faisceau.</span><span class="sxs-lookup"><span data-stu-id="84cf4-235">The value must be a tuple of positive integers with a length that equals the arity of the bundle.</span></span> <span data-ttu-id="84cf4-236">Chaque composant de ce tuple doit avoir une valeur inférieure ou égale au composant correspondant de l’élément **InputShape**.</span><span class="sxs-lookup"><span data-stu-id="84cf4-236">Each component of this tuple must be no greater than the corresponding component of **InputShape**.</span></span> 
* <span data-ttu-id="84cf4-237">**Stride** : (facultatif) définit les tailles d’incrément ajustables de la convolution (une par dimension), soit la distance entre les nœuds centraux.</span><span class="sxs-lookup"><span data-stu-id="84cf4-237">**Stride**: (optional) Defines the sliding step sizes of the convolution (one step size for each dimension), that is the distance between the central nodes.</span></span> <span data-ttu-id="84cf4-238">Cette valeur doit être un tuple d'entiers positifs, dont la longueur correspond à l'arité du faisceau.</span><span class="sxs-lookup"><span data-stu-id="84cf4-238">The value must be a tuple of positive integers with a length that is the arity of the bundle.</span></span> <span data-ttu-id="84cf4-239">Chaque composant de ce tuple doit avoir une valeur inférieure ou égale au composant correspondant de l’élément **KernelShape**.</span><span class="sxs-lookup"><span data-stu-id="84cf4-239">Each component of this tuple must be no greater than the corresponding component of **KernelShape**.</span></span> <span data-ttu-id="84cf4-240">La valeur par défaut est un tuple dont tous les éléments sont égaux à un.</span><span class="sxs-lookup"><span data-stu-id="84cf4-240">The default value is a tuple with all components equal to one.</span></span> 
* <span data-ttu-id="84cf4-241">**Sharing** : (facultatif) définit le partage des poids pour chaque dimension de la convolution.</span><span class="sxs-lookup"><span data-stu-id="84cf4-241">**Sharing**: (optional) Defines the weight sharing for each dimension of the convolution.</span></span> <span data-ttu-id="84cf4-242">La valeur peut être une valeur booléenne unique ou un tuple de valeurs booléennes, dont la longueur est égale à l'arité du faisceau.</span><span class="sxs-lookup"><span data-stu-id="84cf4-242">The value can be a single Boolean value or a tuple of Boolean values with a length that is the arity of the bundle.</span></span> <span data-ttu-id="84cf4-243">Une valeur booléenne unique est étendue de façon à devenir un tuple de la bonne longueur dont tous les éléments sont égaux à la valeur spécifiée.</span><span class="sxs-lookup"><span data-stu-id="84cf4-243">A single Boolean value is extended to be a tuple of the correct length with all components equal to the specified value.</span></span> <span data-ttu-id="84cf4-244">La valeur par défaut est un tuple composé uniquement de valeurs True.</span><span class="sxs-lookup"><span data-stu-id="84cf4-244">The default value is a tuple that consists of all True values.</span></span> 
* <span data-ttu-id="84cf4-245">**MapCount** : (facultatif) définit le nombre de signatures pour le faisceau convolutionnel.</span><span class="sxs-lookup"><span data-stu-id="84cf4-245">**MapCount**: (optional) Defines the number of feature maps for the convolutional bundle.</span></span> <span data-ttu-id="84cf4-246">Cette valeur peut être un entier positif unique ou un tuple d’entiers positifs dont la longueur est égale à l’arité du faisceau.</span><span class="sxs-lookup"><span data-stu-id="84cf4-246">The value can be a single positive integer or a tuple of positive integers with a length that is the arity of the bundle.</span></span> <span data-ttu-id="84cf4-247">Une valeur d'entier unique est étendue de façon à devenir un tuple de la bonne longueur dont les premiers éléments sont égaux à la valeur spécifiée et dont tous les éléments restants sont égaux à un.</span><span class="sxs-lookup"><span data-stu-id="84cf4-247">A single integer value is extended to be a tuple of the correct length with the first components equal to the specified value and all the remaining components equal to one.</span></span> <span data-ttu-id="84cf4-248">La valeur par défaut est 1.</span><span class="sxs-lookup"><span data-stu-id="84cf4-248">The default value is one.</span></span> <span data-ttu-id="84cf4-249">Le nombre total de signatures est le produit des éléments du tuple.</span><span class="sxs-lookup"><span data-stu-id="84cf4-249">The total number of feature maps is the product of the components of the tuple.</span></span> <span data-ttu-id="84cf4-250">La factorisation de ce nombre total sur les éléments détermine la façon dont les valeurs de signature sont regroupées dans les nœuds de destination.</span><span class="sxs-lookup"><span data-stu-id="84cf4-250">The factoring of this total number across the components determines how the feature map values are grouped in the destination nodes.</span></span> 
* <span data-ttu-id="84cf4-251">**Weights** : (facultatif) définit les poids initiaux du faisceau.</span><span class="sxs-lookup"><span data-stu-id="84cf4-251">**Weights**: (optional) Defines the initial weights for the bundle.</span></span> <span data-ttu-id="84cf4-252">La valeur doit être un tuple de valeurs à virgule flottante dont la longueur correspond au nombre de noyaux, multiplié par le nombre de poids par noyau, tel que défini plus loin dans cet article.</span><span class="sxs-lookup"><span data-stu-id="84cf4-252">The value must be a tuple of floating point values with a length that is the number of kernels times the number of weights per kernel, as defined later in this article.</span></span> <span data-ttu-id="84cf4-253">Les poids par défaut sont générés de façon aléatoire.</span><span class="sxs-lookup"><span data-stu-id="84cf4-253">The default weights are randomly generated.</span></span>  

<span data-ttu-id="84cf4-254">Il existe deux ensembles de propriétés contrôlant le remplissage, qui s'excluent mutuellement :</span><span class="sxs-lookup"><span data-stu-id="84cf4-254">There are two sets of properties that control padding, the properties being mutually exclusive:</span></span>

* <span data-ttu-id="84cf4-255">**Padding** : (facultatif) détermine si l’entrée doit être remplie selon un **schéma de remplissage par défaut**.</span><span class="sxs-lookup"><span data-stu-id="84cf4-255">**Padding**: (optional) Determines whether the input should be padded by using a **default padding scheme**.</span></span> <span data-ttu-id="84cf4-256">La valeur peut être une valeur booléenne unique ou un tuple de valeurs booléennes, dont la longueur est égale à l’arité du faisceau.</span><span class="sxs-lookup"><span data-stu-id="84cf4-256">The value can be a single Boolean value, or it can be a tuple of Boolean values with a length that is the arity of the bundle.</span></span> <span data-ttu-id="84cf4-257">Une valeur booléenne unique est étendue de façon à devenir un tuple de la bonne longueur dont tous les éléments sont égaux à la valeur spécifiée.</span><span class="sxs-lookup"><span data-stu-id="84cf4-257">A single Boolean value is extended to be a tuple of the correct length with all components equal to the specified value.</span></span> <span data-ttu-id="84cf4-258">Si la valeur d'une dimension correspond à True, la source est remplie de façon logique dans cette dimension par des cellules de valeur zéro afin de prendre en charge d'autres applications de noyau, de façon que les nœuds centraux des premier et dernier noyaux de cette dimension soient les premier et dernier nœuds de cette dimension dans la couche source.</span><span class="sxs-lookup"><span data-stu-id="84cf4-258">If the value for a dimension is True, the source is logically padded in that dimension with zero-valued cells to support additional kernel applications, such that the central nodes of the first and last kernels in that dimension are the first and last nodes in that dimension in the source layer.</span></span> <span data-ttu-id="84cf4-259">Ainsi, le nombre de nœuds « factices » de chaque dimension est déterminé automatiquement, afin de correspondre exactement à *(InputShape[d] - 1) / Stride[d] + 1* noyaux dans la couche source remplie.</span><span class="sxs-lookup"><span data-stu-id="84cf4-259">Thus, the number of "dummy" nodes in each dimension is determined automatically, to fit exactly *(InputShape[d] - 1) / Stride[d] + 1* kernels into the padded source layer.</span></span> <span data-ttu-id="84cf4-260">Si la valeur d’une dimension correspond à False, les noyaux sont définis de façon que le nombre de nœuds omis soit le même de chaque côté (une différence de 1 au maximum est tolérée).</span><span class="sxs-lookup"><span data-stu-id="84cf4-260">If the value for a dimension is False, the kernels are defined so that the number of nodes on each side that are left out is the same (up to a difference of 1).</span></span> <span data-ttu-id="84cf4-261">La valeur par défaut de cet attribut est un tuple dont tous les éléments ont la valeur False.</span><span class="sxs-lookup"><span data-stu-id="84cf4-261">The default value of this attribute is a tuple with all components equal to False.</span></span>
* <span data-ttu-id="84cf4-262">**UpperPad** et **LowerPad** : (facultatifs) permettent de contrôler la quantité de remplissage à utiliser.</span><span class="sxs-lookup"><span data-stu-id="84cf4-262">**UpperPad** and **LowerPad**: (optional) Provide greater control over the amount of padding to use.</span></span> <span data-ttu-id="84cf4-263">**Important :** Ces attributs peuvent être définis si et seulement si la propriété **Padding** ci-dessus n’est ***pas*** définie.</span><span class="sxs-lookup"><span data-stu-id="84cf4-263">**Important:** These attributes can be defined if and only if the **Padding** property above is ***not*** defined.</span></span> <span data-ttu-id="84cf4-264">Les valeurs doivent être des tuples d’entiers dont la longueur est égale à l’arité du faisceau.</span><span class="sxs-lookup"><span data-stu-id="84cf4-264">The values should be integer-valued tuples with lengths that are the arity of the bundle.</span></span> <span data-ttu-id="84cf4-265">Lorsque ces attributs sont spécifiés, des nœuds « factices » sont ajoutés aux extrémités inférieure et supérieure de chaque dimension de la couche d’entrée.</span><span class="sxs-lookup"><span data-stu-id="84cf4-265">When these attributes are specified, "dummy" nodes are added to the lower and upper ends of each dimension of the input layer.</span></span> <span data-ttu-id="84cf4-266">Le nombre de nœuds ajoutés aux extrémités inférieure et supérieure de la dimension est déterminé respectivement par **LowerPad**[i] et **UpperPad**[i].</span><span class="sxs-lookup"><span data-stu-id="84cf4-266">The number of nodes added to the lower and upper ends in each dimension is determined by **LowerPad**[i] and **UpperPad**[i] respectively.</span></span> <span data-ttu-id="84cf4-267">Afin de veiller à ce que les noyaux correspondent à des nœuds « réels » et non « factices », les conditions suivantes doivent être respectées :</span><span class="sxs-lookup"><span data-stu-id="84cf4-267">To ensure that kernels correspond only to "real" nodes and not to "dummy" nodes, the following conditions must be met:</span></span>
  * <span data-ttu-id="84cf4-268">Chaque élément de **LowerPad** doit être strictement inférieur à KernelShape[d]/2.</span><span class="sxs-lookup"><span data-stu-id="84cf4-268">Each component of **LowerPad** must be strictly less than KernelShape[d]/2.</span></span> 
  * <span data-ttu-id="84cf4-269">Chaque élément de **UpperPad** ne doit pas être supérieur à KernelShape[d]/2.</span><span class="sxs-lookup"><span data-stu-id="84cf4-269">Each component of **UpperPad** must be no greater than KernelShape[d]/2.</span></span> 
  * <span data-ttu-id="84cf4-270">La valeur par défaut de ces attributs est un tuple dont tous les éléments sont égaux à 0.</span><span class="sxs-lookup"><span data-stu-id="84cf4-270">The default value of these attributes is a tuple with all components equal to 0.</span></span> 

<span data-ttu-id="84cf4-271">Le paramètre **Padding** = true permet d’obtenir le remplissage requis pour maintenir le « centre » du noyau à l’intérieur de l’entrée « réelle ».</span><span class="sxs-lookup"><span data-stu-id="84cf4-271">The setting **Padding** = true allows as much padding as is needed to keep the "center" of the kernel inside the "real" input.</span></span> <span data-ttu-id="84cf4-272">Cela modifie un peu le calcul de la taille de sortie.</span><span class="sxs-lookup"><span data-stu-id="84cf4-272">This changes the math a bit for computing the output size.</span></span> <span data-ttu-id="84cf4-273">En règle générale, la taille de sortie *D* est calculée comme suit : *D = (I - K) / S + 1*, où *I* est la taille d’entrée, *K* est la taille du noyau, *S* est le stride, et */* est la division d’entier (arrondi vers zéro).</span><span class="sxs-lookup"><span data-stu-id="84cf4-273">Generally, the output size *D* is computed as *D = (I - K) / S + 1*, where *I* is the input size, *K* is the kernel size, *S* is the stride, and */* is integer division (round toward zero).</span></span> <span data-ttu-id="84cf4-274">Si vous définissez UpperPad = [1, 1], la taille d’entrée *I* est effectivement 29 et, ainsi, *D = (29 - 5) / 2 + 1 = 13*.</span><span class="sxs-lookup"><span data-stu-id="84cf4-274">If you set UpperPad = [1, 1], the input size *I* is effectively 29, and thus *D = (29 - 5) / 2 + 1 = 13*.</span></span> <span data-ttu-id="84cf4-275">Toutefois, quand **Padding** = true, *I* est essentiellement augmenté de *K - 1* ; donc *D = ((28 + 4) - 5) / 2 + 1 = 27 / 2 + 1 = 13 + 1 = 14*.</span><span class="sxs-lookup"><span data-stu-id="84cf4-275">However, when **Padding** = true, essentially *I* gets bumped up by *K - 1*; hence *D = ((28 + 4) - 5) / 2 + 1 = 27 / 2 + 1 = 13 + 1 = 14*.</span></span> <span data-ttu-id="84cf4-276">En spécifiant des valeurs pour **UpperPad** et **LowerPad**, vous obtenez un meilleur contrôle sur le remplissage que si vous définissez simplement **Padding** = true.</span><span class="sxs-lookup"><span data-stu-id="84cf4-276">By specifying values for **UpperPad** and **LowerPad** you get much more control over the padding than if you just set **Padding** = true.</span></span>

<span data-ttu-id="84cf4-277">Pour plus d’informations sur les réseaux convolutionnels et leurs applications, consultez les articles suivants :</span><span class="sxs-lookup"><span data-stu-id="84cf4-277">For more information about convolutional networks and their applications, see these articles:</span></span>  

* [<span data-ttu-id="84cf4-278">http://deeplearning.net/tutorial/lenet.html </span><span class="sxs-lookup"><span data-stu-id="84cf4-278">http://deeplearning.net/tutorial/lenet.html </span></span>](http://deeplearning.net/tutorial/lenet.html)
* [<span data-ttu-id="84cf4-279">http://research.microsoft.com/pubs/68920/icdar03.pdf</span><span class="sxs-lookup"><span data-stu-id="84cf4-279">http://research.microsoft.com/pubs/68920/icdar03.pdf</span></span>](http://research.microsoft.com/pubs/68920/icdar03.pdf) 
* [<span data-ttu-id="84cf4-280">http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf</span><span class="sxs-lookup"><span data-stu-id="84cf4-280">http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf</span></span>](http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf)  

## <a name="pooling-bundles"></a><span data-ttu-id="84cf4-281">Faisceaux de regroupement</span><span class="sxs-lookup"><span data-stu-id="84cf4-281">Pooling bundles</span></span>
<span data-ttu-id="84cf4-282">Un **faisceau de regroupement** applique une géométrie similaire à la connectivité convolutionnelle, mais utilise des fonctions prédéfinies pour dériver les valeurs du nœud source vers la valeur du nœud de destination.</span><span class="sxs-lookup"><span data-stu-id="84cf4-282">A **pooling bundle** applies geometry similar to convolutional connectivity, but it uses predefined functions to source node values to derive the destination node value.</span></span> <span data-ttu-id="84cf4-283">De ce fait, les faisceaux de regroupement n’ont pas d’état entraînable (poids ou biais).</span><span class="sxs-lookup"><span data-stu-id="84cf4-283">Hence, pooling bundles have no trainable state (weights or biases).</span></span> <span data-ttu-id="84cf4-284">Ils prennent en charge tous les attributs convolutionnels, hormis **Sharing**, **MapCount** et **Weights**.</span><span class="sxs-lookup"><span data-stu-id="84cf4-284">Pooling bundles support all the convolutional attributes except **Sharing**, **MapCount**, and **Weights**.</span></span>  

<span data-ttu-id="84cf4-285">En général, les noyaux résumés par des unités de regroupement adjacentes ne se chevauchent pas.</span><span class="sxs-lookup"><span data-stu-id="84cf4-285">Typically, the kernels summarized by adjacent pooling units do not overlap.</span></span> <span data-ttu-id="84cf4-286">Si Stride[d] est égal à KernelShape[d] dans chaque dimension, la couche obtenue est la couche de regroupement locale traditionnelle, qui est généralement employée dans les réseaux neuronaux convolutionnels.</span><span class="sxs-lookup"><span data-stu-id="84cf4-286">If Stride[d] is equal to KernelShape[d] in each dimension, the layer obtained is the traditional local pooling layer, which is commonly employed in convolutional neural networks.</span></span> <span data-ttu-id="84cf4-287">Chaque nœud de destination calcule le maximum ou la moyenne des activités de son noyau dans la couche source.</span><span class="sxs-lookup"><span data-stu-id="84cf4-287">Each destination node computes the maximum or the mean of the activities of its kernel in the source layer.</span></span>  

<span data-ttu-id="84cf4-288">L'exemple suivant illustre un faisceau de regroupement :</span><span class="sxs-lookup"><span data-stu-id="84cf4-288">The following example illustrates a pooling bundle:</span></span> 

    hidden P1 [5, 12, 12]
      from C1 max pool {
        InputShape  = [ 5, 24, 24];
        KernelShape = [ 1,  2,  2];
        Stride      = [ 1,  2,  2];
      }  

* <span data-ttu-id="84cf4-289">L’arité du faisceau est de 3 (longueur des tuples **InputShape**, **KernelShape** et **Stride**).</span><span class="sxs-lookup"><span data-stu-id="84cf4-289">The arity of the bundle is 3 (the length of the tuples **InputShape**, **KernelShape**, and **Stride**).</span></span> 
* <span data-ttu-id="84cf4-290">Le nombre de nœuds de la couche source est égal à *5 * 24 * 24 = 2 880*.</span><span class="sxs-lookup"><span data-stu-id="84cf4-290">The number of nodes in the source layer is *5 * 24 * 24 = 2880*.</span></span> 
* <span data-ttu-id="84cf4-291">Il s’agit d’une couche de regroupement locale traditionnelle, car les valeurs **KernelShape** et **Stride** sont égales.</span><span class="sxs-lookup"><span data-stu-id="84cf4-291">This is a traditional local pooling layer because **KernelShape** and **Stride** are equal.</span></span> 
* <span data-ttu-id="84cf4-292">Le nombre de nœuds de la couche de destination est égal à *5 * 12 * 12 = 1 440*.</span><span class="sxs-lookup"><span data-stu-id="84cf4-292">The number of nodes in the destination layer is *5 * 12 * 12 = 1440*.</span></span>  

<span data-ttu-id="84cf4-293">Pour plus d’informations sur les couches de regroupement, consultez les articles suivants :</span><span class="sxs-lookup"><span data-stu-id="84cf4-293">For more information about pooling layers, see these articles:</span></span>  

* <span data-ttu-id="84cf4-294">[http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf) (Section 3.4)</span><span class="sxs-lookup"><span data-stu-id="84cf4-294">[http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf) (Section 3.4)</span></span>
* [<span data-ttu-id="84cf4-295">http://cs.nyu.edu/~koray/publis/lecun-iscas-10.pdf</span><span class="sxs-lookup"><span data-stu-id="84cf4-295">http://cs.nyu.edu/~koray/publis/lecun-iscas-10.pdf</span></span>](http://cs.nyu.edu/~koray/publis/lecun-iscas-10.pdf) 
* [<span data-ttu-id="84cf4-296">http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf</span><span class="sxs-lookup"><span data-stu-id="84cf4-296">http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf</span></span>](http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf)

## <a name="response-normalization-bundles"></a><span data-ttu-id="84cf4-297">Faisceaux de normalisation de réponse</span><span class="sxs-lookup"><span data-stu-id="84cf4-297">Response normalization bundles</span></span>
<span data-ttu-id="84cf4-298">La **normalisation de réponse** est un schéma de normalisation local initialement proposé par Geoffrey Hinton et al. dans le document [ImageNet Classification with Deep Convolutional Neural Networks](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf) (Classification ImageNet avec les réseaux neuronaux convolutionnels profonds).</span><span class="sxs-lookup"><span data-stu-id="84cf4-298">**Response normalization** is a local normalization scheme that was first introduced by Geoffrey Hinton, et al, in the paper [ImageNet Classiﬁcation with Deep Convolutional Neural Networks](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf).</span></span> <span data-ttu-id="84cf4-299">La normalisation de réponse permet de contribuer à la généralisation dans les réseaux neuronaux.</span><span class="sxs-lookup"><span data-stu-id="84cf4-299">Response normalization is used to aid generalization in neural nets.</span></span> <span data-ttu-id="84cf4-300">Lorsqu’un neurone s’active à un niveau très élevé, la couche de normalisation de réponse locale supprime le niveau d’activation des neurones alentour.</span><span class="sxs-lookup"><span data-stu-id="84cf4-300">When one neuron is firing at a very high activation level, a local response normalization layer suppresses the activation level of the surrounding neurons.</span></span> <span data-ttu-id="84cf4-301">Cette opération s’effectue à l’aide de trois paramètres (***α***, ***β*** et ***k***) et d’une structure convolutionnelle (ou forme de voisinage).</span><span class="sxs-lookup"><span data-stu-id="84cf4-301">This is done by using three parameters (***α***, ***β***, and ***k***) and a convolutional structure (or neighborhood shape).</span></span> <span data-ttu-id="84cf4-302">Chaque neurone ***y*** de la couche de destination correspond à un neurone ***x*** de la couche source.</span><span class="sxs-lookup"><span data-stu-id="84cf4-302">Every neuron in the destination layer ***y*** corresponds to a neuron ***x*** in the source layer.</span></span> <span data-ttu-id="84cf4-303">Le niveau de l’activation de l’élément ***y*** est calculé via la formule suivante, où ***f*** correspond au niveau de l’activation d’un neurone de la valeur ***Nx***, au noyau (ou à l’ensemble qui contient les neurones dans le voisinage de l’élément ***x***), comme défini par la structure convolutionnelle suivante :</span><span class="sxs-lookup"><span data-stu-id="84cf4-303">The activation level of ***y*** is given by the following formula, where ***f*** is the activation level of a neuron, and ***Nx*** is the kernel (or the set that contains the neurons in the neighborhood of ***x***), as defined by the following convolutional structure:</span></span>  

![][1]  

<span data-ttu-id="84cf4-304">Les faisceaux de normalisation de réponse prennent en charge tous les attributs convolutionnels, hormis **Sharing**, **MapCount** et **Weights**.</span><span class="sxs-lookup"><span data-stu-id="84cf4-304">Response normalization bundles support all the convolutional attributes except **Sharing**, **MapCount**, and **Weights**.</span></span>  

* <span data-ttu-id="84cf4-305">Si le noyau contient des neurones de la même signature que ***x***, le schéma de normalisation est appelé **normalisation de même signature**.</span><span class="sxs-lookup"><span data-stu-id="84cf4-305">If the kernel contains neurons in the same map as ***x***, the normalization scheme is referred to as **same map normalization**.</span></span> <span data-ttu-id="84cf4-306">Pour définir une normalisation de ce type, la première coordonnée de **InputShape** doit avoir la valeur 1.</span><span class="sxs-lookup"><span data-stu-id="84cf4-306">To define same map normalization, the first coordinate in **InputShape** must have the value 1.</span></span>
* <span data-ttu-id="84cf4-307">Si le noyau contient des neurones dans la même position spatiale que ***x***, mais situés dans d’autres signatures, le schéma de normalisation est appelé **normalisation intersignature**.</span><span class="sxs-lookup"><span data-stu-id="84cf4-307">If the kernel contains neurons in the same spatial position as ***x***, but the neurons are in other maps, the normalization scheme is called **across maps normalization**.</span></span> <span data-ttu-id="84cf4-308">Ce type de normalisation de réponse implémente une forme d'inhibition latérale inspirée par le type trouvé dans les vrais neurones et qui crée une compétition pour des niveaux d'activation élevés entre les sorties neuronales calculées sur différentes signatures.</span><span class="sxs-lookup"><span data-stu-id="84cf4-308">This type of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activation levels amongst neuron outputs computed on different maps.</span></span> <span data-ttu-id="84cf4-309">Pour définir une normalisation intersignature, la première coordonnée doit être un entier supérieur à 1 et inférieur ou égal au nombre de signatures. Les coordonnées restantes doivent avoir la valeur 1.</span><span class="sxs-lookup"><span data-stu-id="84cf4-309">To define across maps normalization, the first coordinate must be an integer greater than one and no greater than the number of maps, and the rest of the coordinates must have the value 1.</span></span>  

<span data-ttu-id="84cf4-310">Les faisceaux de normalisation de réponse appliquant une fonction prédéfinie aux valeurs de nœud source pour déterminer la valeur du nœud de destination, ils n’ont pas d’état entraînable (poids ou biais).</span><span class="sxs-lookup"><span data-stu-id="84cf4-310">Because response normalization bundles apply a predefined function to source node values to determine the destination node value, they have no trainable state (weights or biases).</span></span>   

<span data-ttu-id="84cf4-311">**Alerte** : les nœuds de la couche de destination correspondent aux neurones centraux des noyaux.</span><span class="sxs-lookup"><span data-stu-id="84cf4-311">**Alert**: The nodes in the destination layer correspond to neurons that are the central nodes of the kernels.</span></span> <span data-ttu-id="84cf4-312">Par exemple, si KernelShape[d] est impair, *KernelShape[d]/2* correspond au nœud de noyau central.</span><span class="sxs-lookup"><span data-stu-id="84cf4-312">For example, if KernelShape[d] is odd, then *KernelShape[d]/2* corresponds to the central kernel node.</span></span> <span data-ttu-id="84cf4-313">Si la valeur *KernelShape[d]* est paire, le nœud central a la valeur *KernelShape[d]/2 - 1*.</span><span class="sxs-lookup"><span data-stu-id="84cf4-313">If *KernelShape[d]* is even, the central node is at *KernelShape[d]/2 - 1*.</span></span> <span data-ttu-id="84cf4-314">Ainsi, si le paramètre **Padding**[d] a la valeur False, les premier et dernier nœuds *KernelShape[d]/2* ne sont associés à aucun nœud correspondant dans la couche de destination.</span><span class="sxs-lookup"><span data-stu-id="84cf4-314">Therefore, if **Padding**[d] is False, the first and the last *KernelShape[d]/2* nodes do not have corresponding nodes in the destination layer.</span></span> <span data-ttu-id="84cf4-315">Pour éviter cette situation, définissez **Padding** sur [true, true, …, true].</span><span class="sxs-lookup"><span data-stu-id="84cf4-315">To avoid this situation, define **Padding** as [true, true, …, true].</span></span>  

<span data-ttu-id="84cf4-316">Outre les quatre attributs décrits précédemment, les faisceaux de normalisation de réponse prennent également en charge les attributs suivants :</span><span class="sxs-lookup"><span data-stu-id="84cf4-316">In addition to the four attributes described earlier, response normalization bundles also support the following attributes:</span></span>  

* <span data-ttu-id="84cf4-317">**Alpha** : (obligatoire) spécifie une valeur à virgule flottante qui correspond à ***α*** dans la formule précédente.</span><span class="sxs-lookup"><span data-stu-id="84cf4-317">**Alpha**: (required) Specifies a floating-point value that corresponds to ***α*** in the previous formula.</span></span> 
* <span data-ttu-id="84cf4-318">**Beta** : (obligatoire) spécifie une valeur à virgule flottante qui correspond à ***β*** dans la formule précédente.</span><span class="sxs-lookup"><span data-stu-id="84cf4-318">**Beta**: (required) Specifies a floating-point value that corresponds to ***β*** in the previous formula.</span></span> 
* <span data-ttu-id="84cf4-319">**Offset** : (facultatif) spécifie une valeur à virgule flottante correspondant à l’élément ***k*** dans la formule précédente.</span><span class="sxs-lookup"><span data-stu-id="84cf4-319">**Offset**: (optional) Specifies a floating-point value that corresponds to ***k*** in the previous formula.</span></span> <span data-ttu-id="84cf4-320">Par défaut, elle est de 1.</span><span class="sxs-lookup"><span data-stu-id="84cf4-320">It defaults to 1.</span></span>  

<span data-ttu-id="84cf4-321">L'exemple suivant définit un faisceau de normalisation de réponse utilisant ces attributs :</span><span class="sxs-lookup"><span data-stu-id="84cf4-321">The following example defines a response normalization bundle using these attributes:</span></span>  

    hidden RN1 [5, 10, 10]
      from P1 response norm {
        InputShape  = [ 5, 12, 12];
        KernelShape = [ 1,  3,  3];
        Alpha = 0.001;
        Beta = 0.75;
      }  

* <span data-ttu-id="84cf4-322">La couche source inclut cinq signatures, chacune présentant des dimensions 12x12, pour un total de 1 440 nœuds.</span><span class="sxs-lookup"><span data-stu-id="84cf4-322">The source layer includes five maps, each with aof dimension of 12x12, totaling in 1440 nodes.</span></span> 
* <span data-ttu-id="84cf4-323">La valeur de **KernelShape** indique qu’il s’agit d’une couche de normalisation de même signature, où le voisinage est un rectangle 3x3.</span><span class="sxs-lookup"><span data-stu-id="84cf4-323">The value of **KernelShape** indicates that this is a same map normalization layer, where the neighborhood is a 3x3 rectangle.</span></span> 
* <span data-ttu-id="84cf4-324">La valeur par défaut de **Padding** est False. La couche de destination n’a donc que 10 nœuds par dimension.</span><span class="sxs-lookup"><span data-stu-id="84cf4-324">The default value of **Padding** is False, thus the destination layer has only 10 nodes in each dimension.</span></span> <span data-ttu-id="84cf4-325">Pour inclure un seul nœud dans la couche de destination correspondant à tous les nœuds de la couche source, ajoutez la chaîne Padding = [true, true, true] et remplacez la taille de l'élément RN1 par [5, 12, 12].</span><span class="sxs-lookup"><span data-stu-id="84cf4-325">To include one node in the destination layer that corresponds to every node in the source layer, add Padding = [true, true, true]; and change the size of RN1 to [5, 12, 12].</span></span>  

## <a name="share-declaration"></a><span data-ttu-id="84cf4-326">Déclaration de partage</span><span class="sxs-lookup"><span data-stu-id="84cf4-326">Share declaration</span></span>
<span data-ttu-id="84cf4-327">Net# peut prendre en charge la définition de plusieurs faisceaux avec des poids partagés.</span><span class="sxs-lookup"><span data-stu-id="84cf4-327">Net# optionally supports defining multiple bundles with shared weights.</span></span> <span data-ttu-id="84cf4-328">Les poids de deux faisceaux quelconques peuvent être partagés tant qu’ils ont la même structure.</span><span class="sxs-lookup"><span data-stu-id="84cf4-328">The weights of any two bundles can be shared if their structures are the same.</span></span> <span data-ttu-id="84cf4-329">La syntaxe suivante définit des faisceaux avec des poids partagés :</span><span class="sxs-lookup"><span data-stu-id="84cf4-329">The following syntax defines bundles with shared weights:</span></span>  

    share-declaration:
        share    {    layer-list    }
        share    {    bundle-list    }
       share    {    bias-list    }

    layer-list:
        layer-name    ,    layer-name
        layer-list    ,    layer-name

    bundle-list:
       bundle-spec    ,    bundle-spec
        bundle-list    ,    bundle-spec

    bundle-spec:
       layer-name    =>     layer-name

    bias-list:
        bias-spec    ,    bias-spec
        bias-list    ,    bias-spec

    bias-spec:
        1    =>    layer-name

    layer-name:
        identifier  

<span data-ttu-id="84cf4-330">Par exemple, la déclaration de partage spécifie les noms de couches, indiquant que les poids et biais doivent être partagés :</span><span class="sxs-lookup"><span data-stu-id="84cf4-330">For example, the following share-declaration specifies the layer names, indicating that both weights and biases should be shared:</span></span>  

    Const {
      InputSize = 37;
      HiddenSize = 50;
    }
    input {
      Data1 [InputSize];
      Data2 [InputSize];
    }
    hidden {
      H1 [HiddenSize] from Data1 all;
      H2 [HiddenSize] from Data2 all;
    }
    output Result [2] {
      from H1 all;
      from H2 all;
    }
    share { H1, H2 } // share both weights and biases  

* <span data-ttu-id="84cf4-331">Les caractéristiques d'entrée sont partitionnées en deux couches d'entrée de même taille.</span><span class="sxs-lookup"><span data-stu-id="84cf4-331">The input features are partitioned into two equal sized input layers.</span></span> 
* <span data-ttu-id="84cf4-332">Les couches masquées calculent alors des caractéristiques de niveau supérieur sur les deux couches d'entrée.</span><span class="sxs-lookup"><span data-stu-id="84cf4-332">The hidden layers then compute higher level features on the two input layers.</span></span> 
* <span data-ttu-id="84cf4-333">La déclaration de partage spécifie que *H1* et *H2* doivent être calculés de la même façon à partir de leurs entrées respectives.</span><span class="sxs-lookup"><span data-stu-id="84cf4-333">The share-declaration specifies that *H1* and *H2* must be computed in the same way from their respective inputs.</span></span>  

<span data-ttu-id="84cf4-334">Il est également possible de spécifier cet élément avec deux déclarations de partage indépendantes, comme suit :</span><span class="sxs-lookup"><span data-stu-id="84cf4-334">Alternatively, this could be specified with two separate share-declarations as follows:</span></span>  

    share { Data1 => H1, Data2 => H2 } // share weights  

<!-- -->

    share { 1 => H1, 1 => H2 } // share biases  

<span data-ttu-id="84cf4-335">Vous ne pouvez utiliser la forme abrégée que si les couches contiennent un seul faisceau.</span><span class="sxs-lookup"><span data-stu-id="84cf4-335">You can use the short form only when the layers contain a single bundle.</span></span> <span data-ttu-id="84cf4-336">En général, le partage n’est possible que lorsque les structures concernées sont identiques, c’est-à-dire si elles ont la même taille, la même géométrie convolutionnelle, etc.</span><span class="sxs-lookup"><span data-stu-id="84cf4-336">In general, sharing is possible only when the relevant structure is identical, meaning that they have the same size, same convolutional geometry, and so forth.</span></span>  

## <a name="examples-of-net-usage"></a><span data-ttu-id="84cf4-337">Exemples d’utilisation de Net#</span><span class="sxs-lookup"><span data-stu-id="84cf4-337">Examples of Net# usage</span></span>
<span data-ttu-id="84cf4-338">Cette section fournit des exemples d’utilisation de Net# pour ajouter des couches masquées, définir la façon dont elles interagissent avec les autres couches et construire des réseaux convolutionnels.</span><span class="sxs-lookup"><span data-stu-id="84cf4-338">This section provides some examples of how you can use Net# to add hidden layers, define the way that hidden layers interact with other layers, and build convolutional networks.</span></span>   

### <a name="define-a-simple-custom-neural-network-hello-world-example"></a><span data-ttu-id="84cf4-339">Définition d'un réseau neuronal personnalisé simple : exemple « Hello World »</span><span class="sxs-lookup"><span data-stu-id="84cf4-339">Define a simple custom neural network: "Hello World" example</span></span>
<span data-ttu-id="84cf4-340">Cet exemple simple montre comment créer un modèle de réseau neuronal contenant une seule couche masquée.</span><span class="sxs-lookup"><span data-stu-id="84cf4-340">This simple example demonstrates how to create a neural network model that has a single hidden layer.</span></span>  

    input Data auto;
    hidden H [200] from Data all;
    output Out [10] sigmoid from H all;  

<span data-ttu-id="84cf4-341">Cet exemple illustre certaines commandes de base, comme suit :</span><span class="sxs-lookup"><span data-stu-id="84cf4-341">The example illustrates some basic commands as follows:</span></span>  

* <span data-ttu-id="84cf4-342">La première ligne définit la couche d’entrée (nommé *Données*).</span><span class="sxs-lookup"><span data-stu-id="84cf4-342">The first line defines the input layer (named *Data*).</span></span> <span data-ttu-id="84cf4-343">Quand vous utilisez le mot-clé **auto**, le réseau neuronal inclut automatiquement toutes les colonnes de caractéristique dans les exemples d’entrée.</span><span class="sxs-lookup"><span data-stu-id="84cf4-343">When you use the  **auto** keyword, the neural network automatically includes all feature columns in the input examples.</span></span> 
* <span data-ttu-id="84cf4-344">La deuxième ligne crée la couche masquée.</span><span class="sxs-lookup"><span data-stu-id="84cf4-344">The second line creates the hidden layer.</span></span> <span data-ttu-id="84cf4-345">Le nom *H* est affecté à la couche masquée, qui contient 200 nœuds.</span><span class="sxs-lookup"><span data-stu-id="84cf4-345">The name *H* is assigned to the hidden layer, which has 200 nodes.</span></span> <span data-ttu-id="84cf4-346">Cette couche est entièrement connectée à la couche d’entrée.</span><span class="sxs-lookup"><span data-stu-id="84cf4-346">This layer is fully connected to the input layer.</span></span>
* <span data-ttu-id="84cf4-347">La troisième ligne définit la couche de sortie (appelée*O*), qui contient 10 nœuds de sortie.</span><span class="sxs-lookup"><span data-stu-id="84cf4-347">The third line defines the output layer (named *O*), which contains 10 output nodes.</span></span> <span data-ttu-id="84cf4-348">Si le réseau neuronal est utilisé pour une classification, il y a un nœud de sortie par classe.</span><span class="sxs-lookup"><span data-stu-id="84cf4-348">If the neural network is used for classification, there is one output node per class.</span></span> <span data-ttu-id="84cf4-349">Le mot clé **sigmoid** indique que la fonction de sortie est appliquée à la couche de sortie.</span><span class="sxs-lookup"><span data-stu-id="84cf4-349">The keyword **sigmoid** indicates that the output function is applied to the output layer.</span></span>   

### <a name="define-multiple-hidden-layers-computer-vision-example"></a><span data-ttu-id="84cf4-350">Définir plusieurs couches masquées : exemple de vision</span><span class="sxs-lookup"><span data-stu-id="84cf4-350">Define multiple hidden layers: computer vision example</span></span>
<span data-ttu-id="84cf4-351">L’exemple suivant montre comment définir un réseau neuronal un peu plus complexe, avec plusieurs couches masquées personnalisées.</span><span class="sxs-lookup"><span data-stu-id="84cf4-351">The following example demonstrates how to define a slightly more complex neural network, with multiple custom hidden layers.</span></span>  

    // Define the input layers 
    input Pixels [10, 20];
    input MetaData [7];

    // Define the first two hidden layers, using data only from the Pixels input
    hidden ByRow [10, 12] from Pixels where (s,d) => s[0] == d[0];
    hidden ByCol [5, 20] from Pixels where (s,d) => abs(s[1] - d[1]) <= 1;

    // Define the third hidden layer, which uses as source the hidden layers ByRow and ByCol
    hidden Gather [100] 
    {
      from ByRow all;
      from ByCol all;
    }

    // Define the output layer and its sources
    output Result [10]  
    {
      from Gather all;
      from MetaData all;
    }  

<span data-ttu-id="84cf4-352">Cet exemple illustre plusieurs caractéristiques du langage de spécification des réseaux neuronaux :</span><span class="sxs-lookup"><span data-stu-id="84cf4-352">This example illustrates several features of the neural networks specification language:</span></span>  

* <span data-ttu-id="84cf4-353">La structure a deux couches d’entrée, *Pixels* et *MetaData*.</span><span class="sxs-lookup"><span data-stu-id="84cf4-353">The structure has two input layers, *Pixels* and *MetaData*.</span></span>
* <span data-ttu-id="84cf4-354">La couche *Pixels* est une couche source pour deux ensembles de connexion, avec les couches de destination, *ByRow* et *ByCol*.</span><span class="sxs-lookup"><span data-stu-id="84cf4-354">The *Pixels* layer is a source layer for two connection bundles, with destination layers, *ByRow* and *ByCol*.</span></span>
* <span data-ttu-id="84cf4-355">Les couches *Gather* et *Result* sont des couches de destination dans plusieurs ensembles de connexion.</span><span class="sxs-lookup"><span data-stu-id="84cf4-355">The layers *Gather* and *Result* are destination layers in multiple connection bundles.</span></span>
* <span data-ttu-id="84cf4-356">La couche de sortie, *Result*, est une couche de destination dans deux ensembles de connexion, le premier indiquant la deuxième couche masquée (Gather) en tant que couche de destination et le deuxième (MetaData) en tant que couche de destination.</span><span class="sxs-lookup"><span data-stu-id="84cf4-356">The output layer, *Result*, is a destination layer in two connection bundles; one with the second level hidden (Gather) as a destination layer, and the other with the input layer (MetaData) as a destination layer.</span></span>
* <span data-ttu-id="84cf4-357">Les couches masquées *ByRow* et *ByCol* spécifient une connectivité filtrée en utilisant des expressions de prédicat.</span><span class="sxs-lookup"><span data-stu-id="84cf4-357">The hidden layers, *ByRow* and *ByCol*, specify filtered connectivity by using predicate expressions.</span></span> <span data-ttu-id="84cf4-358">Plus précisément, le nœud dans *ByRow* à [x, y] est connecté aux nœuds dans l’élément *Pixels*, car la première coordonnée d’index est égale à la première coordonnée du nœud, x.</span><span class="sxs-lookup"><span data-stu-id="84cf4-358">More precisely, the node in *ByRow* at [x, y] is connected to the nodes in *Pixels* that have the first index coordinate equal to the node's first coordinate, x.</span></span> <span data-ttu-id="84cf4-359">De même, le nœud de l’élément *ByCol à [x, y] est connecté à ces nœuds dans _Pixels*, dont la deuxième coordonnée d’index se trouve dans la deuxième coordonnée du nœud, y.</span><span class="sxs-lookup"><span data-stu-id="84cf4-359">Similarly, the node in *ByCol at [x, y] is connected to the nodes in _Pixels* that have the second index coordinate within one of the node's second coordinate, y.</span></span>  

### <a name="define-a-convolutional-network-for-multiclass-classification-digit-recognition-example"></a><span data-ttu-id="84cf4-360">Définir un réseau convolutionnel pour la classification multiclasse : exemple de reconnaissance de chiffre</span><span class="sxs-lookup"><span data-stu-id="84cf4-360">Define a convolutional network for multiclass classification: digit recognition example</span></span>
<span data-ttu-id="84cf4-361">La définition du réseau ci-après, conçu pour reconnaître les chiffres, illustre certaines techniques avancées de personnalisation d’un réseau neuronal.</span><span class="sxs-lookup"><span data-stu-id="84cf4-361">The definition of the following network is designed to recognize numbers, and it illustrates some advanced techniques for customizing a neural network.</span></span>  

    input Image [29, 29];
    hidden Conv1 [5, 13, 13] from Image convolve 
    {
       InputShape  = [29, 29];
       KernelShape = [ 5,  5];
       Stride      = [ 2,  2];
       MapCount    = 5;
    }
    hidden Conv2 [50, 5, 5]
    from Conv1 convolve 
    {
       InputShape  = [ 5, 13, 13];
       KernelShape = [ 1,  5,  5];
       Stride      = [ 1,  2,  2];
       Sharing     = [false, true, true];
       MapCount    = 10;
    }
    hidden Hid3 [100] from Conv2 all;
    output Digit [10] from Hid3 all;  


* <span data-ttu-id="84cf4-362">La structure a une seule couche d’entrée, *Image*.</span><span class="sxs-lookup"><span data-stu-id="84cf4-362">The structure has a single input layer, *Image*.</span></span>
* <span data-ttu-id="84cf4-363">Le mot clé **convolve** indique que les couches nommées *Conv1* et *Conv2* sont des couches convolutionnelles.</span><span class="sxs-lookup"><span data-stu-id="84cf4-363">The keyword **convolve** indicates that the layers named *Conv1* and *Conv2* are convolutional layers.</span></span> <span data-ttu-id="84cf4-364">Chacune de ces déclarations de couche est suivie d'une liste des attributs de convolution.</span><span class="sxs-lookup"><span data-stu-id="84cf4-364">Each of these layer declarations is followed by a list of the convolution attributes.</span></span>
* <span data-ttu-id="84cf4-365">Le réseau contient une troisième couche masquée, *Hid3*, entièrement connectée à la deuxième couche masquée, *Conv2*.</span><span class="sxs-lookup"><span data-stu-id="84cf4-365">The net has a third hidden layer, *Hid3*, which is fully connected to the second hidden layer, *Conv2*.</span></span>
* <span data-ttu-id="84cf4-366">La couche de sortie, *Digit*, n’est connectée qu’à la troisième couche masquée, *Hid3*.</span><span class="sxs-lookup"><span data-stu-id="84cf4-366">The output layer, *Digit*, is connected only to the third hidden layer, *Hid3*.</span></span> <span data-ttu-id="84cf4-367">Le mot clé **all** indique que la couche de sortie est entièrement connectée à *Hid3*.</span><span class="sxs-lookup"><span data-stu-id="84cf4-367">The keyword **all** indicates that the output layer is fully connected to *Hid3*.</span></span>
* <span data-ttu-id="84cf4-368">L’arité de la convolution est de 3 (longueur des tuples **InputShape**, **KernelShape**, **Stride** et **Sharing**).</span><span class="sxs-lookup"><span data-stu-id="84cf4-368">The arity of the convolution is three (the length of the tuples **InputShape**, **KernelShape**, **Stride**, and **Sharing**).</span></span> 
* <span data-ttu-id="84cf4-369">Le nombre de poids par noyau est de *1 + **KernelShape**\[0] * **KernelShape**\[1] * **KernelShape**\[2] = 1 + 1 * 5 * 5 = 26. Ou 26 * 50 = 1 300*.</span><span class="sxs-lookup"><span data-stu-id="84cf4-369">The number of weights per kernel is *1 + **KernelShape**\[0] * **KernelShape**\[1] * **KernelShape**\[2] = 1 + 1 * 5 * 5 = 26. Or 26 * 50 = 1300*.</span></span>
* <span data-ttu-id="84cf4-370">Vous pouvez calculer les nœuds de chaque couche masquée comme suit :</span><span class="sxs-lookup"><span data-stu-id="84cf4-370">You can calculate the nodes in each hidden layer as follows:</span></span>
  * <span data-ttu-id="84cf4-371">**NodeCount**\[0] = (5 - 1) / 1 + 1 = 5.</span><span class="sxs-lookup"><span data-stu-id="84cf4-371">**NodeCount**\[0] = (5 - 1) / 1 + 1 = 5.</span></span>
  * <span data-ttu-id="84cf4-372">**NodeCount**\[1] = (13 - 5) / 2 + 1 = 5.</span><span class="sxs-lookup"><span data-stu-id="84cf4-372">**NodeCount**\[1] = (13 - 5) / 2 + 1 = 5.</span></span> 
  * <span data-ttu-id="84cf4-373">**NodeCount**\[2] = (13 - 5) / 2 + 1 = 5.</span><span class="sxs-lookup"><span data-stu-id="84cf4-373">**NodeCount**\[2] = (13 - 5) / 2 + 1 = 5.</span></span> 
* <span data-ttu-id="84cf4-374">Vous pouvez calculer le nombre total de nœuds en utilisant la dimensionnalité déclarée de la couche, [50, 5, 5], comme suit : ***MapCount** * **NodeCount**\[0] * **NodeCount**\[1] * **NodeCount**\[2] = 10 * 5 * 5 * 5*</span><span class="sxs-lookup"><span data-stu-id="84cf4-374">The total number of nodes can be calculated by using the declared dimensionality of the layer, [50, 5, 5], as follows: ***MapCount** * **NodeCount**\[0] * **NodeCount**\[1] * **NodeCount**\[2] = 10 * 5 * 5 * 5*</span></span>
* <span data-ttu-id="84cf4-375">Puisque **Sharing**[d] est faux uniquement pour *d == 0*, le nombre de noyaux est ***MapCount**  * **NodeCount**\[0] = 10 * 5 = 50*.</span><span class="sxs-lookup"><span data-stu-id="84cf4-375">Because **Sharing**[d] is False only for *d == 0*, the number of kernels is ***MapCount** * **NodeCount**\[0] = 10 * 5 = 50*.</span></span> 

## <a name="acknowledgements"></a><span data-ttu-id="84cf4-376">Remerciements</span><span class="sxs-lookup"><span data-stu-id="84cf4-376">Acknowledgements</span></span>
<span data-ttu-id="84cf4-377">Le langage Net # pour la personnalisation de l'architecture des réseaux neuronaux a été développée chez Microsoft par Shon Katzenberger (architecte, Machine Learning) et Alexey Kamenev (ingénieur logiciel, Microsoft Research).</span><span class="sxs-lookup"><span data-stu-id="84cf4-377">The Net# language for customizing the architecture of neural networks was developed at Microsoft by Shon Katzenberger (Architect, Machine Learning) and Alexey Kamenev (Software Engineer, Microsoft Research).</span></span> <span data-ttu-id="84cf4-378">Il est utilisé en interne pour l'apprentissage de projets et d'applications allant de la détection d'images à l'analyse de texte.</span><span class="sxs-lookup"><span data-stu-id="84cf4-378">It is used internally for machine learning projects and applications ranging from image detection to text analytics.</span></span> <span data-ttu-id="84cf4-379">Pour plus d’informations, consultez [Neural Nets in Azure ML - Introduction to Net#](http://blogs.technet.com/b/machinelearning/archive/2015/02/16/neural-nets-in-azure-ml-introduction-to-net.aspx) (Réseaux neuronaux dans Azure ML - présentation de Net #).</span><span class="sxs-lookup"><span data-stu-id="84cf4-379">For more information, see [Neural Nets in Azure ML - Introduction to Net#](http://blogs.technet.com/b/machinelearning/archive/2015/02/16/neural-nets-in-azure-ml-introduction-to-net.aspx)</span></span>

<span data-ttu-id="84cf4-380">[1]:./media/machine-learning-azure-ml-netsharp-reference-guide/formula_large.gif</span><span class="sxs-lookup"><span data-stu-id="84cf4-380">[1]:./media/machine-learning-azure-ml-netsharp-reference-guide/formula_large.gif</span></span>

