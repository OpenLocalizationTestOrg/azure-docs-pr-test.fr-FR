---
title: "Ingénierie et sélection de caractéristiques dans Azure Machine Learning | Microsoft Docs"
description: "Cette rubrique explique les finalités de l’ingénierie de caractéristiques et de la sélection de caractéristiques et fournit des exemples de leur rôle dans le processus d’amélioration des données de l’apprentissage automatique."
services: machine-learning
documentationcenter: 
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: 9ceb524d-842e-4f77-9eae-a18e599442d6
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 01/18/2017
ms.author: zhangya;bradsev
ROBOTS: NOINDEX
redirect_url: machine-learning-data-science-create-features
redirect_document_id: TRUE
ms.openlocfilehash: 51a5d8fed492cb9301e048c2b6a721e4573a47d9
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 07/11/2017
---
# <a name="feature-engineering-and-selection-in-azure-machine-learning"></a><span data-ttu-id="8e8ba-103">Ingénierie et sélection de caractéristiques dans Azure Machine Learning</span><span class="sxs-lookup"><span data-stu-id="8e8ba-103">Feature engineering and selection in Azure Machine Learning</span></span>
<span data-ttu-id="8e8ba-104">Cette rubrique explique les finalités de l’ingénierie et de la sélection de caractéristiques dans le processus d’amélioration des données de l’apprentissage automatique.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-104">This topic explains the purposes of feature engineering and feature selection in the data-enhancement process of machine learning.</span></span> <span data-ttu-id="8e8ba-105">Elle présente les étapes de ces processus à l’aide des exemples fournis par Azure Machine Learning Studio.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-105">It illustrates what these processes involve by using examples provided by Azure Machine Learning Studio.</span></span>

[!INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]

<span data-ttu-id="8e8ba-106">Les données d'apprentissage utilisées dans l'apprentissage automatique peuvent souvent être améliorées par la sélection ou l'extraction de caractéristiques à partir des données brutes collectées.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-106">The training data used in machine learning can often be enhanced by the selection or extraction of features from the raw data collected.</span></span> <span data-ttu-id="8e8ba-107">Un exemple de conception de caractéristique dans le cadre de l’apprentissage de la classification des images de caractères écrits à la main est la carte de densité de bits construite à partir des données brutes de distribution de bits.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-107">An example of an engineered feature in the context of learning how to classify the images of handwritten characters is a bit-density map constructed from the raw bit distribution data.</span></span> <span data-ttu-id="8e8ba-108">Cette carte peut aider à localiser les bords des caractères plus efficacement que la distribution brute.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-108">This map can help locate the edges of the characters more efficiently than the raw distribution.</span></span>

<span data-ttu-id="8e8ba-109">L’ingénierie et la sélection de caractéristiques augmentent l’efficacité du processus d’apprentissage qui tend à extraire les informations essentielles contenues dans les données.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-109">Engineered and selected features increase the efficiency of the training process, which attempts to extract the key information contained in the data.</span></span> <span data-ttu-id="8e8ba-110">Ces processus améliorent également les performances de ces modèles pour classifier les données d'entrée avec précision et prédire les résultats pertinents de façon plus consistante.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-110">They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly.</span></span> <span data-ttu-id="8e8ba-111">L'ingénierie et la sélection de caractéristiques peuvent également être combinées afin de rendre l'apprentissage plus souple d'un point de vue informatique.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-111">Feature engineering and selection can also combine to make the learning more computationally tractable.</span></span> <span data-ttu-id="8e8ba-112">Cela se fait grâce à l'amélioration puis à la réduction du nombre de caractéristiques nécessaires à l'étalonnage ou l'apprentissage d'un modèle.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-112">It does so by enhancing and then reducing the number of features needed to calibrate or train a model.</span></span> <span data-ttu-id="8e8ba-113">D'un point de vue mathématique, les caractéristiques sélectionnées pour effectuer l'apprentissage du modèle sont un ensemble minimum de variables indépendantes qui expliquent les modèles des données puis prédisent correctement les résultats.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-113">Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.</span></span>

<span data-ttu-id="8e8ba-114">L'ingénierie et la sélection des caractéristiques constituent une partie d'un processus plus vaste, qui se compose généralement de quatre étapes :</span><span class="sxs-lookup"><span data-stu-id="8e8ba-114">The engineering and selection of features is one part of a larger process, which typically consists of four steps:</span></span>

* <span data-ttu-id="8e8ba-115">Collecte des données</span><span class="sxs-lookup"><span data-stu-id="8e8ba-115">Data collection</span></span>
* <span data-ttu-id="8e8ba-116">Amélioration des données</span><span class="sxs-lookup"><span data-stu-id="8e8ba-116">Data enhancement</span></span>
* <span data-ttu-id="8e8ba-117">Construction de modèles</span><span class="sxs-lookup"><span data-stu-id="8e8ba-117">Model construction</span></span>
* <span data-ttu-id="8e8ba-118">Post-traitement</span><span class="sxs-lookup"><span data-stu-id="8e8ba-118">Post-processing</span></span>

<span data-ttu-id="8e8ba-119">L’ingénierie et la sélection constituent l’étape d’amélioration des données de l’apprentissage automatique.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-119">Engineering and selection make up the data enhancement step of machine learning.</span></span> <span data-ttu-id="8e8ba-120">Trois aspects de ce processus peuvent être distingués relativement à nos objectifs :</span><span class="sxs-lookup"><span data-stu-id="8e8ba-120">Three aspects of this process may be distinguished for our purposes:</span></span>

* <span data-ttu-id="8e8ba-121">**Prétraitement des données** : ce processus tente de s’assurer que les données collectées sont normales et cohérentes.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-121">**Data pre-processing**: This process tries to ensure that the collected data is clean and consistent.</span></span> <span data-ttu-id="8e8ba-122">Ce processus inclut des tâches telles que l’intégration de jeux de données multiples, la gestion des données manquantes, la gestion des données inconsistantes et la conversion des types de données.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-122">It includes tasks such as integrating multiple data sets, handling missing data, handling inconsistent data, and converting data types.</span></span>
* <span data-ttu-id="8e8ba-123">**Ingénierie de caractéristiques** : ce processus tente de créer des caractéristiques supplémentaires pertinentes à partir de caractéristiques brutes existantes dans les données et d’augmenter la performance de prédiction de l’algorithme d’apprentissage.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-123">**Feature engineering**: This process attempts to create additional relevant features from the existing raw features in the data and to increase predictive power to the learning algorithm.</span></span>
* <span data-ttu-id="8e8ba-124">**Sélection de caractéristiques** : ce processus sélectionne le sous-ensemble clé des caractéristiques de données d’origine pour réduire la dimensionnalité du problème d’apprentissage.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-124">**Feature selection**: This process selects the key subset of original data features to reduce the dimensionality of the training problem.</span></span>

<span data-ttu-id="8e8ba-125">Cette rubrique traite uniquement des aspects de l’ingénierie et de la sélection de caractéristiques du processus d’amélioration des données.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-125">This topic only covers the feature engineering and feature selection aspects of the data enhancement process.</span></span> <span data-ttu-id="8e8ba-126">Pour plus d’informations sur l’étape de prétraitement des données, consultez la vidéo [Pre-processing data in Azure Machine Learning Studio](https://azure.microsoft.com/documentation/videos/preprocessing-data-in-azure-ml-studio/) (Prétraitement des données dans Azure Machine Learning Studio).</span><span class="sxs-lookup"><span data-stu-id="8e8ba-126">For more information on the data pre-processing step, see [Pre-processing data in Azure Machine Learning Studio](https://azure.microsoft.com/documentation/videos/preprocessing-data-in-azure-ml-studio/).</span></span>

## <a name="creating-features-from-your-data--feature-engineering"></a><span data-ttu-id="8e8ba-127">Création de caractéristiques à partir de vos données : ingénierie de caractéristiques</span><span class="sxs-lookup"><span data-stu-id="8e8ba-127">Creating features from your data--feature engineering</span></span>
<span data-ttu-id="8e8ba-128">Les données d'apprentissage se constituent d'une matrice composée d'exemples (enregistrements ou observations stockées dans les lignes), chaque exemple disposant d'un ensemble de caractéristiques (variables ou champs stockés dans des colonnes).</span><span class="sxs-lookup"><span data-stu-id="8e8ba-128">The training data consists of a matrix composed of examples (records or observations stored in rows), each of which has a set of features (variables or fields stored in columns).</span></span> <span data-ttu-id="8e8ba-129">Les caractéristiques spécifiées dans la conception expérimentale sont supposées caractériser les modèles dans les données.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-129">The features specified in the experimental design are expected to characterize the patterns in the data.</span></span> <span data-ttu-id="8e8ba-130">Bien que plusieurs champs de données brutes puissent être directement inclus dans l’ensemble des caractéristiques sélectionnées utilisées pour l’apprentissage d’un modèle, des caractéristiques conçues supplémentaires doivent souvent être construites à partir des caractéristiques dans les données brutes pour générer un jeu de données de formation amélioré.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-130">Although many of the raw data fields can be directly included in the selected feature set used to train a model, additional engineered features often need to be constructed from the features in the raw data to generate an enhanced training data set.</span></span>

<span data-ttu-id="8e8ba-131">Quelles sont les caractéristiques qui doivent être créées pour améliorer le jeu de données durant l’apprentissage d’un modèle ?</span><span class="sxs-lookup"><span data-stu-id="8e8ba-131">What kind of features should be created to enhance the data set when training a model?</span></span> <span data-ttu-id="8e8ba-132">Les caractéristiques conçues améliorant l'apprentissage offrent des informations qui permettent de mieux différencier les modèles dans les données.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-132">Engineered features that enhance the training provide information that better differentiates the patterns in the data.</span></span> <span data-ttu-id="8e8ba-133">Les nouvelles caractéristiques devraient fournir des informations supplémentaires qui ne sont pas clairement capturées ou facilement visibles dans l’ensemble des caractéristiques existantes ou d’origine, mais ce processus est tout un art.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-133">You expect the new features to provide additional information that is not clearly captured or easily apparent in the original or existing feature set, but this process is something of an art.</span></span> <span data-ttu-id="8e8ba-134">Des décisions réfléchies et productives nécessitent souvent une spécialisation dans le domaine.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-134">Sound and productive decisions often require some domain expertise.</span></span>

<span data-ttu-id="8e8ba-135">En débutant avec Azure Machine Learning, il est plus facile de comprendre correctement le processus avec des exemples fournis dans Machine Learning Studio.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-135">When starting with Azure Machine Learning, it is easiest to grasp this process concretely by using samples provided in Machine Learning Studio.</span></span> <span data-ttu-id="8e8ba-136">Deux exemples sont présentés ici :</span><span class="sxs-lookup"><span data-stu-id="8e8ba-136">Two examples are presented here:</span></span>

* <span data-ttu-id="8e8ba-137">Un exemple de régression ([Prédiction du nombre de locations de vélo](http://gallery.cortanaintelligence.com/Experiment/Regression-Demand-estimation-4)) dans une expérimentation supervisée, où les valeurs cibles sont connues</span><span class="sxs-lookup"><span data-stu-id="8e8ba-137">A regression example ([Prediction of the number of bike rentals](http://gallery.cortanaintelligence.com/Experiment/Regression-Demand-estimation-4)) in a supervised experiment where the target values are known</span></span>
* <span data-ttu-id="8e8ba-138">un exemple de classification d'exploration de texte utilisant le [hachage de caractéristiques][feature-hashing]</span><span class="sxs-lookup"><span data-stu-id="8e8ba-138">A text-mining classification example using [Feature Hashing][feature-hashing]</span></span>

### <a name="example-1-adding-temporal-features-for-a-regression-model"></a><span data-ttu-id="8e8ba-139">Exemple 1 : ajout de caractéristiques temporelles pour un modèle de régression</span><span class="sxs-lookup"><span data-stu-id="8e8ba-139">Example 1: Adding temporal features for a regression model</span></span>
<span data-ttu-id="8e8ba-140">Nous allons utiliser l’exemple de « prévision de la demande de vélos » dans Azure Machine Learning Studio afin de démontrer comment concevoir des caractéristiques pour une tâche de régression.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-140">To demonstrate how to engineer features for a regression task, let's use the experiment "Demand forecasting of bikes" in Azure Machine Learning Studio.</span></span> <span data-ttu-id="8e8ba-141">L’objectif de cette expérimentation est de prédire la demande de vélos, autrement dit le nombre de locations de vélo pour un mois, un jour ou une heure spécifique.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-141">The objective of this experiment is to predict the demand for the bikes, that is, the number of bike rentals within a specific month, day, or hour.</span></span> <span data-ttu-id="8e8ba-142">Le **jeu de données de location de vélo UCI** est utilisé en tant que données brutes d’entrée.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-142">The data set **Bike Rental UCI data set** is used as the raw input data.</span></span>

<span data-ttu-id="8e8ba-143">Le jeu de données se base sur des données réelles de la société Capital Bikeshare qui gère un réseau de location de vélos à Washington DC aux États-Unis.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-143">This data set is based on real data from the Capital Bikeshare company that maintains a bike rental network in Washington DC in the United States.</span></span> <span data-ttu-id="8e8ba-144">Ce jeu de données représente le nombre de locations de vélo pour une heure spécifique d’un jour, de 2011 à 2012, et contient 17379 lignes et 17 colonnes.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-144">The data set represents the number of bike rentals within a specific hour of a day, from 2011 to 2012, and it contains 17379 rows and 17 columns.</span></span> <span data-ttu-id="8e8ba-145">L’ensemble des caractéristiques brutes contient des conditions météorologiques (température, humidité, vitesse du vent) et le type de jour (vacances ou jour de semaine).</span><span class="sxs-lookup"><span data-stu-id="8e8ba-145">The raw feature set contains weather conditions (temperature, humidity, wind speed) and the type of the day (holiday or weekday).</span></span> <span data-ttu-id="8e8ba-146">Le champ à prédire est **cnt**, nombre qui représente les locations de vélo pour une heure spécifique et qui est compris entre 1 et 977.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-146">The field to predict is **cnt**, a count that represents the bike rentals within a specific hour and that ranges from 1 to 977.</span></span>

<span data-ttu-id="8e8ba-147">Pour construire des caractéristiques efficaces dans les données d’apprentissage, quatre modèles de régression sont générés à l’aide du même algorithme, mais avec quatre jeux de données d’apprentissage différents.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-147">To construct effective features in the training data, four regression models are built by using the same algorithm, but with four different training data sets.</span></span> <span data-ttu-id="8e8ba-148">Les quatre jeux de données représentent les mêmes données d’entrée brutes, mais avec un nombre croissant de jeux de caractéristiques.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-148">The four data sets represent the same raw input data, but with an increasing number of features set.</span></span> <span data-ttu-id="8e8ba-149">Ces caractéristiques sont regroupées en quatre catégories :</span><span class="sxs-lookup"><span data-stu-id="8e8ba-149">These features are grouped into four categories:</span></span>

1. <span data-ttu-id="8e8ba-150">A = caractéristiques météo + vacances + jour de semaine + week-end pour le jour prévu</span><span class="sxs-lookup"><span data-stu-id="8e8ba-150">A = weather + holiday + weekday + weekend features for the predicted day</span></span>
2. <span data-ttu-id="8e8ba-151">B = nombre de vélos loués au cours de chacune des 12 dernières heures</span><span class="sxs-lookup"><span data-stu-id="8e8ba-151">B = number of bikes that were rented in each of the previous 12 hours</span></span>
3. <span data-ttu-id="8e8ba-152">C = nombre de vélos loués au cours de chacun des 12 derniers jours à la même heure</span><span class="sxs-lookup"><span data-stu-id="8e8ba-152">C = number of bikes that were rented in each of the previous 12 days at the same hour</span></span>
4. <span data-ttu-id="8e8ba-153">D = nombre de vélos loués au cours de chacune des 12 dernières semaines le même jour à la même heure</span><span class="sxs-lookup"><span data-stu-id="8e8ba-153">D = number of bikes that were rented in each of the previous 12 weeks at the same hour and the same day</span></span>

<span data-ttu-id="8e8ba-154">Excepté l’ensemble de caractéristiques A, qui existe déjà dans les données brutes d’origine, les trois autres ensembles de caractéristiques sont créés via le processus de conception des caractéristiques.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-154">Besides feature set A, which already exists in the original raw data, the other three sets of features are created through the feature engineering process.</span></span> <span data-ttu-id="8e8ba-155">L’ensemble de caractéristiques B capture les toutes dernières demandes de vélos.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-155">Feature set B captures the recent demand for the bikes.</span></span> <span data-ttu-id="8e8ba-156">L'ensemble de caractéristiques C capture la demande de vélos pour une heure en particulier.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-156">Feature set C captures the demand for bikes at a particular hour.</span></span> <span data-ttu-id="8e8ba-157">L'ensemble de caractéristiques D capture la demande de vélos pour une heure particulière et un jour de la semaine particulier.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-157">Feature set D captures demand for bikes at particular hour and particular day of the week.</span></span> <span data-ttu-id="8e8ba-158">Chacun des quatre jeux de données d’apprentissage inclut respectivement les ensembles de caractéristiques A, A + B, A + B + C et A + B + C + D.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-158">Each of the four training data sets includes feature sets A, A+B, A+B+C, and A+B+C+D, respectively.</span></span>

<span data-ttu-id="8e8ba-159">Dans l’expérimentation d’Azure Machine Learning, ces quatre jeux de données d’apprentissage sont constitués via quatre branches à partir du jeu de données d’entrée traité au préalable.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-159">In the Azure Machine Learning experiment, these four training data sets are formed via four branches from the pre-processed input data set.</span></span> <span data-ttu-id="8e8ba-160">Sauf pour la branche la plus à gauche, chacune de ces branches contient un module [Exécuter le Script R][execute-r-script], dans lequel un ensemble de caractéristiques dérivées (ensembles de caractéristiques B, C et D) est respectivement construit et ajouté au jeu de données importé.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-160">Except for the leftmost branch, each of these branches contains an [Execute R Script][execute-r-script] module in which a set of derived features (feature sets B, C, and D) is respectively constructed and appended to the imported data set.</span></span> <span data-ttu-id="8e8ba-161">La figure suivante montre le script R utilisé pour créer un ensemble de caractéristiques B dans la deuxième branche de gauche.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-161">The following figure demonstrates the R script used to create feature set B in the second left branch.</span></span>

![Créer un ensemble de caractéristiques](./media/machine-learning-feature-selection-and-engineering/addFeature-Rscripts.png)

<span data-ttu-id="8e8ba-163">La table suivante résume la comparaison des résultats de performance des quatre modèles.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-163">The following table summarizes the comparison of the performance results of the four models.</span></span> <span data-ttu-id="8e8ba-164">Les caractéristiques A + B + C affichent les meilleurs résultats.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-164">The best results are shown by features A+B+C.</span></span> <span data-ttu-id="8e8ba-165">Notez que le taux d’erreur diminue quand un ensemble de caractéristiques supplémentaires est inclus dans les données d’apprentissage.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-165">Note that the error rate decreases when additional feature sets are included in the training data.</span></span> <span data-ttu-id="8e8ba-166">Cela confirme l’idée présupposée que les ensembles de caractéristiques B et C fournissent des informations supplémentaires pertinentes pour la tâche de régression.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-166">This verifies our presumption that the feature sets B and C provide additional relevant information for the regression task.</span></span> <span data-ttu-id="8e8ba-167">L’ajout de l’ensemble de caractéristiques D semble ne pas fournir une réduction du taux d’erreur supplémentaire.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-167">Adding the D feature set does not seem to provide any additional reduction in the error rate.</span></span>

![Comparer les résultats des performances](./media/machine-learning-feature-selection-and-engineering/result1.png)

### <span data-ttu-id="8e8ba-169"><a name="example2"></a> Exemple 2 : création de caractéristiques dans l’exploration de texte</span><span class="sxs-lookup"><span data-stu-id="8e8ba-169"><a name="example2"></a> Example 2: Creating features in text mining</span></span>
<span data-ttu-id="8e8ba-170">L'ingénierie de caractéristiques s'applique largement aux tâches liées à l'exploration de texte, telles que la classification de document et l'analyse de sentiments.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-170">Feature engineering is widely applied in tasks related to text mining, such as document classification and sentiment analysis.</span></span> <span data-ttu-id="8e8ba-171">Par exemple, quand vous souhaitez classer des documents en plusieurs catégories, l’hypothèse typique est que les mots ou expressions inclus dans une catégorie de document sont moins susceptibles de se produire dans une autre catégorie de document.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-171">For example, when you want to classify documents into several categories, a typical assumption is that the words or phrases included in one document category are less likely to occur in another document category.</span></span> <span data-ttu-id="8e8ba-172">Autrement dit, la fréquence de la distribution de mots ou d’expressions est capable d’identifier les différentes catégories de document.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-172">In other words, the frequency of the word or phrase distribution is able to characterize different document categories.</span></span> <span data-ttu-id="8e8ba-173">Dans les applications d’exploration de texte, le processus d’ingénierie des caractéristiques est nécessaire pour créer les caractéristiques impliquant des fréquences de mot ou d’expression, car chaque élément des contenus de textes est généralement utilisé en tant que données d’entrée.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-173">In text mining applications, the feature engineering process is needed to create the features involving word or phrase frequencies because individual pieces of text-contents usually serve as the input data.</span></span>

<span data-ttu-id="8e8ba-174">Pour effectuer cette tâche, une technique appelée *hachage de caractéristiques* est appliquée pour transformer efficacement les caractéristiques de texte arbitraires en index.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-174">To achieve this task, a technique called *feature hashing* is applied to efficiently turn arbitrary text features into indices.</span></span> <span data-ttu-id="8e8ba-175">Au lieu d’associer chaque caractéristique de texte (mots ou d’expressions) à un index particulier, cette méthode fonctionne en appliquant une caractéristique de hachage aux caractéristiques et en utilisant directement leurs valeurs de hachage en tant qu’index.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-175">Instead of associating each text feature (words or phrases) to a particular index, this method functions by applying a hash function to the features and by using their hash values as indices directly.</span></span>

<span data-ttu-id="8e8ba-176">Dans Azure Machine Learning, il existe un module de [hachage de caractéristiques][feature-hashing] qui crée ces caractéristiques de mot ou expression.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-176">In Azure Machine Learning, there is a [Feature Hashing][feature-hashing] module that creates these word or phrase features.</span></span> <span data-ttu-id="8e8ba-177">La figure suivante montre un exemple d’utilisation de ce module.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-177">The following figure shows an example of using this module.</span></span> <span data-ttu-id="8e8ba-178">Le jeu de données d’entrée contient deux colonnes : l’évaluation du livre allant de 1 à 5 et le contenu même de la critique.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-178">The input data set contains two columns: the book rating ranging from 1 to 5 and the actual review content.</span></span> <span data-ttu-id="8e8ba-179">L’objectif de ce module de [hachage de caractéristiques][feature-hashing] est de récupérer de nouvelles caractéristiques qui montrent la fréquence d’occurrence des mots ou expressions correspondants dans cette critique de livre en particulier.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-179">The goal of this [Feature Hashing][feature-hashing] module is to retrieve new features that show the occurrence frequency of the corresponding words or phrases within the particular book review.</span></span> <span data-ttu-id="8e8ba-180">Pour utiliser ce module, vous devez effectuer les étapes suivantes :</span><span class="sxs-lookup"><span data-stu-id="8e8ba-180">To use this module, you need to complete the following steps:</span></span>

1. <span data-ttu-id="8e8ba-181">Sélectionnez la colonne qui contient le texte d’entrée (**Col2** pour cet exemple).</span><span class="sxs-lookup"><span data-stu-id="8e8ba-181">Select the column that contains the input text (**Col2** in this example).</span></span>
2. <span data-ttu-id="8e8ba-182">Définissez le *nombre de bits de hachage* sur 8, ce qui signifie que 2 ^ 8 = 256 caractéristiques sont créées.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-182">Set *Hashing bitsize* to 8, which means 2^8=256 features are created.</span></span> <span data-ttu-id="8e8ba-183">Le mot ou l’expression est alors haché en 256 index dans tout le texte.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-183">The word or phrase in the text is then hashed to 256 indices.</span></span> <span data-ttu-id="8e8ba-184">Le paramètre *hachage du nombre de bits* est compris entre 1 et 31.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-184">The parameter *Hashing bitsize* ranges from 1 to 31.</span></span> <span data-ttu-id="8e8ba-185">Si le paramètre est défini sur un nombre plus élevé, les mots ou expressions sont moins susceptibles d’être hachés sur le même index.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-185">If the parameter is set to a larger number, the words or phrases are less likely to be hashed into the same index.</span></span>
3. <span data-ttu-id="8e8ba-186">Définissez le paramètre *N-grammes* sur 2.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-186">Set the parameter *N-grams* to 2.</span></span> <span data-ttu-id="8e8ba-187">Celui-ci permet de récupérer la fréquence d’occurrence d’unigrammes (une caractéristique pour chaque mot) et de bigrammes (une caractéristique pour chaque paire de mots juxtaposés) à partir du texte d’entrée.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-187">This retrieves the occurrence frequency of unigrams (a feature for every single word) and bigrams (a feature for every pair of adjacent words) from the input text.</span></span> <span data-ttu-id="8e8ba-188">Le paramètre *N-grammes* est compris entre 0 et 10, ce qui indique le nombre maximum de mots séquentiels à inclure dans une caractéristique.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-188">The parameter *N-grams* ranges from 0 to 10, which indicates the maximum number of sequential words to be included in a feature.</span></span>  

![Module hachage de caractéristiques](./media/machine-learning-feature-selection-and-engineering/feature-Hashing1.png)

<span data-ttu-id="8e8ba-190">La figure suivante montre à quoi ressemblent ces nouvelles caractéristiques.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-190">The following figure shows what these new features look like.</span></span>

![Exemple de hachage de caractéristiques](./media/machine-learning-feature-selection-and-engineering/feature-Hashing2.png)

## <a name="filtering-features-from-your-data--feature-selection"></a><span data-ttu-id="8e8ba-192">Filtrage des caractéristiques à partir de vos données : sélection de caractéristiques</span><span class="sxs-lookup"><span data-stu-id="8e8ba-192">Filtering features from your data--feature selection</span></span>
<span data-ttu-id="8e8ba-193">La *sélection de caractéristiques* est un processus qui s’applique en général à la construction de jeux de données d’apprentissage pour les tâches de modélisation prédictives telles que les tâches de classification ou de régression.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-193">*Feature selection* is a process that is commonly applied to the construction of training data sets for predictive modeling tasks such as classification or regression tasks.</span></span> <span data-ttu-id="8e8ba-194">L’objectif est de sélectionner un sous-ensemble des caractéristiques du jeu de données d’origine qui réduit ses dimensions à l’aide d’un ensemble minimal de caractéristiques pour représenter l’écart de quantité maximum dans les données.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-194">The goal is to select a subset of the features from the original data set that reduces its dimensions by using a minimal set of features to represent the maximum amount of variance in the data.</span></span> <span data-ttu-id="8e8ba-195">Les caractéristiques de ce sous-ensemble contiennent les seules caractéristiques à inclure à l’apprentissage du modèle.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-195">This subset of features contains the only features to be included to train the model.</span></span> <span data-ttu-id="8e8ba-196">La sélection de caractéristiques a deux principaux objectifs :</span><span class="sxs-lookup"><span data-stu-id="8e8ba-196">Feature selection serves two main purposes:</span></span>

* <span data-ttu-id="8e8ba-197">Elle augmente souvent la précision de classification en éliminant les caractéristiques non pertinentes, redondantes ou fortement liées.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-197">Feature selection often increases classification accuracy by eliminating irrelevant, redundant, or highly correlated features.</span></span>
* <span data-ttu-id="8e8ba-198">Elle réduit le nombre de caractéristiques qui rendent le processus d’apprentissage du modèle plus efficace.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-198">Feature selection decreases the number of features, which makes the model training process more efficient.</span></span> <span data-ttu-id="8e8ba-199">Cela est particulièrement important pour les apprenants dont l'apprentissage est coûteux tels que les machines à vecteurs de support.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-199">This is particularly important for learners that are expensive to train such as support vector machines.</span></span>

<span data-ttu-id="8e8ba-200">Bien que la sélection des caractéristiques ait pour objet de réduire le nombre de caractéristiques dans le jeu de données utilisé pour l’apprentissage du modèle, celle-ci ne correspond généralement pas au terme de *réduction de la dimensionnalité*.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-200">Although feature selection seeks to reduce the number of features in the data set used to train the model, it is not usually referred to by the term *dimensionality reduction.*</span></span> <span data-ttu-id="8e8ba-201">Les méthodes de sélection de caractéristiques extraient un sous-ensemble des caractéristiques d'origine dans les données sans les modifier.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-201">Feature selection methods extract a subset of original features in the data without changing them.</span></span>  <span data-ttu-id="8e8ba-202">Les méthodes de réduction de la dimensionnalité utilisent l'ingénierie des caractéristiques qui peuvent transformer les caractéristiques d'origine et donc les modifier.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-202">Dimensionality reduction methods employ engineered features that can transform the original features and thus modify them.</span></span> <span data-ttu-id="8e8ba-203">Parmi les exemples de méthodes de réduction de la dimensionnalité, on peut noter l’analyse du composant principal, l’analyse canonique des corrélations et la décomposition en valeurs uniques.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-203">Examples of dimensionality reduction methods include principal component analysis, canonical correlation analysis, and singular value decomposition.</span></span>

<span data-ttu-id="8e8ba-204">L’une des méthodes de sélection de caractéristiques de catégorie largement appliquée dans un contexte supervisé est la sélection de caractéristiques par filtrage.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-204">One widely applied category of feature selection methods in a supervised context is filter-based feature selection.</span></span> <span data-ttu-id="8e8ba-205">En évaluant la corrélation entre chaque caractéristique et l'attribut cible, ces méthodes appliquent une mesure statistique pour attribuer un score à chaque caractéristique.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-205">By evaluating the correlation between each feature and the target attribute, these methods apply a statistical measure to assign a score to each feature.</span></span> <span data-ttu-id="8e8ba-206">Les caractéristiques sont ensuite classées suivant le score, que vous pouvez utiliser pour définir le seuil de conservation ou d’élimination d’une caractéristique spécifique.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-206">The features are then ranked by the score, which you can use to set the threshold for keeping or eliminating a specific feature.</span></span> <span data-ttu-id="8e8ba-207">Parmi les exemples de mesures statistiques utilisées dans ces méthodes, on peut noter la corrélation de Pearson, des informations mutuelles et le test de la loi du Khi-deux.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-207">Examples of the statistical measures used in these methods include Pearson Correlation, mutual information, and the Chi-squared test.</span></span>

<span data-ttu-id="8e8ba-208">Azure Machine Learning Studio fournit des modules pour la sélection des caractéristiques.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-208">Azure Machine Learning Studio provides modules for feature selection.</span></span> <span data-ttu-id="8e8ba-209">Comme indiqué dans la figure suivante, ces modules comprennent une [sélection de caractéristiques basée sur les filtres][filter-based-feature-selection] et une [analyse discriminante linéaire de Fisher][fisher-linear-discriminant-analysis].</span><span class="sxs-lookup"><span data-stu-id="8e8ba-209">As shown in the following figure, these modules include [Filter-Based Feature Selection][filter-based-feature-selection] and [Fisher Linear Discriminant Analysis][fisher-linear-discriminant-analysis].</span></span>

![Exemple de sélection de caractéristiques](./media/machine-learning-feature-selection-and-engineering/feature-Selection.png)

<span data-ttu-id="8e8ba-211">Par exemple, utilisez le module de [sélection de caractéristiques par filtrage][filter-based-feature-selection] avec l’exemple d’exploration de texte décrit précédemment.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-211">For example, use the [Filter-Based Feature Selection][filter-based-feature-selection] module with the text mining example outlined previously.</span></span> <span data-ttu-id="8e8ba-212">Supposons que vous voulez créer un modèle de régression après avoir créé un ensemble de 256 caractéristiques via le module de [hachage de caractéristiques][feature-hashing] et que la variable de réponse est **Col1** et représente une critique de livre notée de 1 à 5.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-212">Assume that you want to build a regression model after a set of 256 features is created through the [Feature Hashing][feature-hashing] module, and that the response variable is **Col1** and represents a book review rating ranging from 1 to 5.</span></span> <span data-ttu-id="8e8ba-213">Définissez **Feature scoring method** (Méthode de notation des caractéristiques) sur **Pearson Correlation** (Corrélation de Pearson), **Target column** (Colonne cible) sur **Col1** et **Number of desired features** (Nombre de caractéristiques souhaitées) sur **50**.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-213">Set **Feature scoring method** to **Pearson Correlation**, **Target column** to **Col1**, and **Number of desired features** to **50**.</span></span> <span data-ttu-id="8e8ba-214">Le module de [sélection de caractéristiques basée sur les filtres][filter-based-feature-selection] produit ensuite un jeu de données contenant 50 caractéristiques avec l’attribut cible **Col1**.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-214">The module [Filter-Based Feature Selection][filter-based-feature-selection] then produces a data set containing 50 features together with the target attribute **Col1**.</span></span> <span data-ttu-id="8e8ba-215">La figure suivante montre le flux de cette expérimentation et les paramètres d’entrée.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-215">The following figure shows the flow of this experiment and the input parameters.</span></span>

![Exemple de sélection de caractéristiques](./media/machine-learning-feature-selection-and-engineering/feature-Selection1.png)

<span data-ttu-id="8e8ba-217">La figure suivante montre les jeux de données qui en résultent.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-217">The following figure shows the resulting data sets.</span></span> <span data-ttu-id="8e8ba-218">Chaque caractéristique obtient un score basé sur la corrélation de Pearson entre elle et l’attribut cible **Col1**.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-218">Each feature is scored based on the Pearson Correlation between itself and the target attribute **Col1**.</span></span> <span data-ttu-id="8e8ba-219">Les caractéristiques avec les scores les plus élevés sont conservées.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-219">The features with top scores are kept.</span></span>

![Jeux de données avec sélection de caractéristiques par filtrage](./media/machine-learning-feature-selection-and-engineering/feature-Selection2.png)

<span data-ttu-id="8e8ba-221">La figure suivante indique les scores correspondants aux caractéristiques sélectionnées.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-221">The following figure shows the corresponding scores of the selected features.</span></span>

![Scores des caractéristiques sélectionnées](./media/machine-learning-feature-selection-and-engineering/feature-Selection3.png)

<span data-ttu-id="8e8ba-223">En appliquant ce module de [sélection de caractéristiques basée sur les filtres][filter-based-feature-selection], 50 des 256 caractéristiques sont sélectionnées, car elles présentent les fonctionnalités corrélées les plus importantes avec la variable cible **Col1** selon la méthode de notation **corrélation de Pearson**.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-223">By applying this [Filter-Based Feature Selection][filter-based-feature-selection] module, 50 out of 256 features are selected because they have the most features correlated with the target variable **Col1** based on the scoring method **Pearson Correlation**.</span></span>

## <a name="conclusion"></a><span data-ttu-id="8e8ba-224">Conclusion</span><span class="sxs-lookup"><span data-stu-id="8e8ba-224">Conclusion</span></span>
<span data-ttu-id="8e8ba-225">L’ingénierie de caractéristiques et la sélection de caractéristiques sont deux étapes couramment effectuées pour préparer les données d’apprentissage durant la création d’un modèle d’apprentissage automatique.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-225">Feature engineering and feature selection are two steps commonly performed to prepare the training data when building a machine learning model.</span></span> <span data-ttu-id="8e8ba-226">En général, l’ingénierie de caractéristiques s’applique d’abord à la génération de caractéristiques supplémentaires. L’étape de sélection de caractéristiques est alors effectuée pour éliminer les caractéristiques inutiles, redondantes ou fortement corrélées.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-226">Normally, feature engineering is applied first to generate additional features, and then the feature selection step is performed to eliminate irrelevant, redundant, or highly correlated features.</span></span>

<span data-ttu-id="8e8ba-227">Il n’est pas toujours nécessaire d’effectuer l’ingénierie de caractéristiques ou la sélection des caractéristiques.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-227">It is not always necessarily to perform feature engineering or feature selection.</span></span> <span data-ttu-id="8e8ba-228">Les données que vous avez à disposition ou que vous collectez, l’algorithme que vous choisissez et les objectifs de l’expérimentation déterminent si cela est nécessaire.</span><span class="sxs-lookup"><span data-stu-id="8e8ba-228">Whether it is needed depends on the data you have or collect, the algorithm you pick, and the objective of the experiment.</span></span>

<!-- Module References -->
[execute-r-script]: https://msdn.microsoft.com/library/azure/30806023-392b-42e0-94d6-6b775a6e0fd5/
[feature-hashing]: https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/
[filter-based-feature-selection]: https://msdn.microsoft.com/library/azure/918b356b-045c-412b-aa12-94a1d2dad90f/
[fisher-linear-discriminant-analysis]: https://msdn.microsoft.com/library/azure/dcaab0b2-59ca-4bec-bb66-79fd23540080/
