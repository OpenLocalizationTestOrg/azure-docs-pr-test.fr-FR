---
title: "Sélection de fonctionnalités dans le processus TDSP (Team Data Science Process) | Microsoft Docs"
description: "Explique la finalité de la sélection de fonctionnalités et fournit des exemples de son rôle dans le processus d’amélioration des données de l’apprentissage automatique."
services: machine-learning
documentationcenter: 
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: 878541f5-1df8-4368-889a-ced6852aba47
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 03/24/2017
ms.author: zhangya;bradsev
ms.openlocfilehash: ab97ee8278be567ff46d9b0f762d3c5c6cafa412
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 07/11/2017
---
# <a name="feature-selection-in-the-team-data-science-process-tdsp"></a><span data-ttu-id="aba68-103">Sélection de fonctionnalités dans le processus TDSP (Team Data Science Process)</span><span class="sxs-lookup"><span data-stu-id="aba68-103">Feature selection in the Team Data Science Process (TDSP)</span></span>
<span data-ttu-id="aba68-104">Cet article explique les finalités de la sélection de fonctionnalités et fournit des exemples de son rôle dans le processus d’amélioration des données de l’apprentissage automatique.</span><span class="sxs-lookup"><span data-stu-id="aba68-104">This article explains the purposes of feature selection and provides examples of its role in the data enhancement process of machine learning.</span></span> <span data-ttu-id="aba68-105">Ces exemples sont tirés d’Azure Machine Learning Studio.</span><span class="sxs-lookup"><span data-stu-id="aba68-105">These examples are drawn from Azure Machine Learning Studio.</span></span> 

[!INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]

<span data-ttu-id="aba68-106">La conception et la sélection de fonctionnalités constituent une partie du processus TDSP présenté dans [Qu’est ce que le processus TDSP (Team Data Science Process) ?](data-science-process-overview.md).</span><span class="sxs-lookup"><span data-stu-id="aba68-106">The engineering and selection of features is one part of the Team Data Science Process (TDSP) outlined in [What is the Team Data Science Process?](data-science-process-overview.md).</span></span> <span data-ttu-id="aba68-107">La conception et la sélection de fonctionnalités sont des parties de l’étape de **développement de fonctionnalités** du processus TDSP.</span><span class="sxs-lookup"><span data-stu-id="aba68-107">Feature engineering and selection are parts of the **Develop features** step of the TDSP.</span></span>

* <span data-ttu-id="aba68-108">La **conception de caractéristiques** : ce processus tente de créer des caractéristiques supplémentaires pertinentes à partir de caractéristiques brutes existantes dans les données et d'augmenter la performance de prédiction de l'algorithme d'apprentissage.</span><span class="sxs-lookup"><span data-stu-id="aba68-108">**feature engineering**: This process attempts to create additional relevant features from the existing raw features in the data, and to increase predictive power to the learning algorithm.</span></span>
* <span data-ttu-id="aba68-109">La **sélection de caractéristiques** : ce processus sélectionne le sous-ensemble clé des caractéristiques de données d'origine afin de réduire la dimensionnalité du problème d'apprentissage.</span><span class="sxs-lookup"><span data-stu-id="aba68-109">**feature selection**: This process selects the key subset of original data features in an attempt to reduce the dimensionality of the training problem.</span></span>

<span data-ttu-id="aba68-110">En général, l’**ingénierie de caractéristiques** s’applique d’abord à la génération de caractéristiques supplémentaires. L’étape de **sélection de caractéristiques** est alors effectuée pour éliminer les caractéristiques inutiles, redondantes ou fortement corrélées.</span><span class="sxs-lookup"><span data-stu-id="aba68-110">Normally **feature engineering** is applied first to generate additional features, and then the **feature selection** step is performed to eliminate irrelevant, redundant, or highly correlated features.</span></span>

## <a name="filtering-features-from-your-data---feature-selection"></a><span data-ttu-id="aba68-111">Filtrage des caractéristiques à partir de vos données - Sélection de caractéristiques</span><span class="sxs-lookup"><span data-stu-id="aba68-111">Filtering Features from Your Data - Feature Selection</span></span>
<span data-ttu-id="aba68-112">La sélection de caractéristiques est un processus qui s'applique en général à la construction de jeux de données d'apprentissage pour les tâches de modélisation prédictives telles que les tâches de classification ou de régression.</span><span class="sxs-lookup"><span data-stu-id="aba68-112">Feature selection is a process that is commonly applied for the construction of training datasets for predictive modeling tasks such as classification or regression tasks.</span></span> <span data-ttu-id="aba68-113">L'objectif est de sélectionner un sous-ensemble des caractéristiques du jeu de données d'origine qui réduit ses dimensions à l'aide d'un ensemble minimal de caractéristiques pour représenter l'écart de quantité maximum dans les données.</span><span class="sxs-lookup"><span data-stu-id="aba68-113">The goal is to select a subset of the features from the original dataset that reduce its dimensions by using a minimal set of features to represent the maximum amount of variance in the data.</span></span> <span data-ttu-id="aba68-114">Les caractéristiques de ce sous-ensemble sont les seules caractéristiques à inclure à l'apprentissage du modèle.</span><span class="sxs-lookup"><span data-stu-id="aba68-114">This subset of features are, then, the only features to be included to train the model.</span></span> <span data-ttu-id="aba68-115">La sélection de caractéristiques a deux principaux objectifs.</span><span class="sxs-lookup"><span data-stu-id="aba68-115">Feature selection serves two main purposes.</span></span>

* <span data-ttu-id="aba68-116">Tout d'abord, la sélection des caractéristiques augmente souvent la précision de classification en éliminant les caractéristiques non pertinentes, redondantes ou fortement liées.</span><span class="sxs-lookup"><span data-stu-id="aba68-116">First, feature selection often increases classification accuracy by eliminating irrelevant, redundant, or highly correlated features.</span></span>
* <span data-ttu-id="aba68-117">De plus, il réduit le nombre de caractéristiques qui rendent le processus d'apprentissage du modèle plus efficace.</span><span class="sxs-lookup"><span data-stu-id="aba68-117">Second, it decreases the number of features which makes model training process more efficient.</span></span> <span data-ttu-id="aba68-118">Cela est particulièrement important pour les apprenants dont l'apprentissage est coûteux tels que les machines à vecteurs de support.</span><span class="sxs-lookup"><span data-stu-id="aba68-118">This is particularly important for learners that are expensive to train such as support vector machines.</span></span>

<span data-ttu-id="aba68-119">Bien que la sélection des caractéristiques ait pour objet de réduire le nombre de caractéristiques dans le jeu de données utilisé pour l'apprentissage du modèle, celle-ci ne correspond généralement pas au terme de « réduction de la dimensionnalité ».</span><span class="sxs-lookup"><span data-stu-id="aba68-119">Although feature selection does seek to reduce the number of features in the dataset used to train the model, it is not usually referred to by the term "dimensionality reduction".</span></span> <span data-ttu-id="aba68-120">Les méthodes de sélection de caractéristiques extraient un sous-ensemble des caractéristiques d'origine dans les données sans les modifier.</span><span class="sxs-lookup"><span data-stu-id="aba68-120">Feature selection methods extract a subset of original features in the data without changing them.</span></span>  <span data-ttu-id="aba68-121">Les méthodes de réduction de la dimensionnalité utilisent l'ingénierie des caractéristiques qui peuvent transformer les caractéristiques d'origine et donc les modifier.</span><span class="sxs-lookup"><span data-stu-id="aba68-121">Dimensionality reduction methods employ engineered features that can transform the original features and thus modify them.</span></span> <span data-ttu-id="aba68-122">Parmi les exemples de méthodes de réduction de la dimensionnalité, on peut noter l'analyse du composant principal, l'analyse canonique des corrélations et la décomposition en valeurs uniques.</span><span class="sxs-lookup"><span data-stu-id="aba68-122">Examples of dimensionality reduction methods include Principal Component Analysis, canonical correlation analysis, and Singular Value Decomposition.</span></span>

<span data-ttu-id="aba68-123">Notamment, l'une des méthodes de sélection de caractéristiques de catégorie largement appliquée dans un contexte supervisé est appelée « sélection de caractéristiques basée sur les filtres ».</span><span class="sxs-lookup"><span data-stu-id="aba68-123">Among others, one widely applied category of feature selection methods in a supervised context is called "filter based feature selection".</span></span> <span data-ttu-id="aba68-124">En évaluant la corrélation entre chaque caractéristique et l'attribut cible, ces méthodes appliquent une mesure statistique pour attribuer un score à chaque caractéristique.</span><span class="sxs-lookup"><span data-stu-id="aba68-124">By evaluating the correlation between each feature and the target attribute, these methods apply a statistical measure to assign a score to each feature.</span></span> <span data-ttu-id="aba68-125">Les caractéristiques sont ensuite classées suivant le score qui peut être utilisé pour aider à définir le seuil de conservation ou d'élimination d'une caractéristique spécifique.</span><span class="sxs-lookup"><span data-stu-id="aba68-125">The features are then ranked by the score, which may be used to help set the threshold for keeping or eliminating a specific feature.</span></span> <span data-ttu-id="aba68-126">Parmi les exemples de mesures statistiques utilisées dans ces méthodes, on peut noter la corrélation de Pearson, des informations mutuelles et le test de la loi du Khi-deux.</span><span class="sxs-lookup"><span data-stu-id="aba68-126">Examples of the statistical measures used in these methods include Person correlation, mutual information, and the Chi squared test.</span></span>

<span data-ttu-id="aba68-127">Dans Azure Machine Learning Studio, des modules sont fournis pour la sélection des caractéristiques.</span><span class="sxs-lookup"><span data-stu-id="aba68-127">In Azure Machine Learning Studio, there are modules provided for feature selection.</span></span> <span data-ttu-id="aba68-128">Comme indiqué dans la figure suivante, ces modules comprennent une [sélection de caractéristiques par filtrage][filter-based-feature-selection] et une [analyse discriminante linéaire de Fisher][fisher-linear-discriminant-analysis].</span><span class="sxs-lookup"><span data-stu-id="aba68-128">As shown in the following figure, these modules include [Filter-Based Feature Selection][filter-based-feature-selection] and [Fisher Linear Discriminant Analysis][fisher-linear-discriminant-analysis].</span></span>

![Exemple de sélection de caractéristiques](./media/machine-learning-data-science-select-features/feature-Selection.png)

<span data-ttu-id="aba68-130">Prenons l’exemple de l’utilisation du module de [sélection de caractéristiques basée par filtrage][filter-based-feature-selection].</span><span class="sxs-lookup"><span data-stu-id="aba68-130">Consider, for example, the use of the [Filter-Based Feature Selection][filter-based-feature-selection] module.</span></span> <span data-ttu-id="aba68-131">Pour plus de commodité, nous continuons d'utiliser l'exemple d'exploration de texte présenté ci-dessus.</span><span class="sxs-lookup"><span data-stu-id="aba68-131">For the purpose of convenience, we continue to use the text mining example outlined above.</span></span> <span data-ttu-id="aba68-132">Supposons que nous voulons créer un modèle de régression après avoir créé un ensemble de 256 caractéristiques via le module de [hachage de caractéristiques][feature-hashing] et que la variable de réponse est le « Col1 » et représente une critique de livre notée de 1 à 5.</span><span class="sxs-lookup"><span data-stu-id="aba68-132">Assume that we want to build a regression model after a set of 256 features are created through the [Feature Hashing][feature-hashing] module, and that the response variable is the "Col1" and represents a book review ratings ranging from 1 to 5.</span></span> <span data-ttu-id="aba68-133">La « Méthode de notation des caractéristiques » doit être définie en tant que « corrélation de Pearson », la « Colonne cible » en tant que « Col1 » et le « nombre de caractéristiques souhaitées » à 50.</span><span class="sxs-lookup"><span data-stu-id="aba68-133">By setting "Feature scoring method" to be "Pearson Correlation", the "Target column" to be "Col1", and the "Number of desired features" to 50.</span></span> <span data-ttu-id="aba68-134">Le module de [sélection de caractéristiques par filtrage][filter-based-feature-selection] produira ensuite un jeu de données contenant 50 caractéristiques avec l’attribut cible « Col1 ».</span><span class="sxs-lookup"><span data-stu-id="aba68-134">Then the module [Filter-Based Feature Selection][filter-based-feature-selection] will produce a dataset containing 50 features together with the target attribute "Col1".</span></span> <span data-ttu-id="aba68-135">La figure suivante montre le flux de cette expérience et les paramètres d'entrée que nous venons de décrire.</span><span class="sxs-lookup"><span data-stu-id="aba68-135">The following figure shows the flow of this experiment and the input parameters we just described.</span></span>

![Exemple de sélection de caractéristiques](./media/machine-learning-data-science-select-features/feature-Selection1.png)

<span data-ttu-id="aba68-137">La figure suivante montre les jeux de données qui en résultent.</span><span class="sxs-lookup"><span data-stu-id="aba68-137">The following figure shows the resulting datasets.</span></span> <span data-ttu-id="aba68-138">Chaque caractéristique obtient un score basé sur la corrélation de Pearson entre elle et l'attribut cible « Col1 ».</span><span class="sxs-lookup"><span data-stu-id="aba68-138">Each feature is scored based on the Pearson Correlation between itself and the target attribute "Col1".</span></span> <span data-ttu-id="aba68-139">Les caractéristiques avec les scores les plus élevés sont conservées.</span><span class="sxs-lookup"><span data-stu-id="aba68-139">The features with top scores are kept.</span></span>

![Exemple de sélection de caractéristiques](./media/machine-learning-data-science-select-features/feature-Selection2.png)

<span data-ttu-id="aba68-141">Les scores correspondants aux caractéristiques sélectionnées sont indiqués dans la figure suivante.</span><span class="sxs-lookup"><span data-stu-id="aba68-141">The corresponding scores of the selected features are shown in the following figure.</span></span>

![Exemple de sélection de caractéristiques](./media/machine-learning-data-science-select-features/feature-Selection3.png)

<span data-ttu-id="aba68-143">En appliquant ce module de [sélection de caractéristiques par filtrage][filter-based-feature-selection], 50 des 256 caractéristiques sont sélectionnées, car elles présentent la corrélation la plus importante avec la variable cible « Col1 » selon la méthode de notation « corrélation de Pearson ».</span><span class="sxs-lookup"><span data-stu-id="aba68-143">By applying this [Filter-Based Feature Selection][filter-based-feature-selection] module, 50 out of 256 features are selected because they have the most correlated features with the target variable "Col1", based on the scoring method "Pearson Correlation".</span></span>

## <a name="conclusion"></a><span data-ttu-id="aba68-144">Conclusion</span><span class="sxs-lookup"><span data-stu-id="aba68-144">Conclusion</span></span>
<span data-ttu-id="aba68-145">La conception et la sélection de fonctionnalités augmentent toutes deux l’efficacité du processus d’apprentissage qui tend à extraire les informations essentielles contenues dans les données.</span><span class="sxs-lookup"><span data-stu-id="aba68-145">Feature engineering and feature selection are two commonly Engineered and selected features increase the efficiency of the training process which attempts to extract the key information contained in the data.</span></span> <span data-ttu-id="aba68-146">Ces processus améliorent également les performances de ces modèles pour classifier les données d'entrée avec précision et prédire les résultats pertinents de façon plus consistante.</span><span class="sxs-lookup"><span data-stu-id="aba68-146">They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly.</span></span> <span data-ttu-id="aba68-147">L'ingénierie et la sélection de caractéristiques peuvent également être combinées afin de rendre l'apprentissage plus souple d'un point de vue informatique.</span><span class="sxs-lookup"><span data-stu-id="aba68-147">Feature engineering and selection can also combine to make the learning more computationally tractable.</span></span> <span data-ttu-id="aba68-148">Cela se fait grâce à l'amélioration puis à la réduction du nombre de caractéristiques nécessaires à l'étalonnage ou l'apprentissage d'un modèle.</span><span class="sxs-lookup"><span data-stu-id="aba68-148">It does so by enhancing and then reducing the number of features needed to calibrate or train a model.</span></span> <span data-ttu-id="aba68-149">D'un point de vue mathématique, les caractéristiques sélectionnées pour effectuer l'apprentissage du modèle sont un ensemble minimum de variables indépendantes qui expliquent les modèles des données puis prédisent correctement les résultats.</span><span class="sxs-lookup"><span data-stu-id="aba68-149">Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.</span></span>

<span data-ttu-id="aba68-150">Notez qu'il n'est pas toujours nécessaire d'effectuer l'ingénierie de caractéristiques ou la sélection des caractéristiques.</span><span class="sxs-lookup"><span data-stu-id="aba68-150">Note that it is not always necessarily to perform feature engineering or feature selection.</span></span> <span data-ttu-id="aba68-151">Que cela soit nécessaire ou non dépend des données que l'on a à disposition ou qui sont collectées, de l'algorithme choisi et des objectifs de l'expérience.</span><span class="sxs-lookup"><span data-stu-id="aba68-151">Whether it is needed or not depends on the data we have or collect, the algorithm we pick, and the objective of the experiment.</span></span>

<!-- Module References -->
[feature-hashing]: https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/
[filter-based-feature-selection]: https://msdn.microsoft.com/library/azure/918b356b-045c-412b-aa12-94a1d2dad90f/
[fisher-linear-discriminant-analysis]: https://msdn.microsoft.com/library/azure/dcaab0b2-59ca-4bec-bb66-79fd23540080/

