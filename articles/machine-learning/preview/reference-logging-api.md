---
title: "Informations de référence sur l’API de journalisation d’Azure ML | Microsoft Docs"
description: "Informations de référence sur l’API de journalisation."
services: machine-learning
author: akshaya-a
ms.author: akannava
manager: mwinkle
ms.reviewer: garyericson, jasonwhowell, mldocs
ms.service: machine-learning
ms.workload: data-services
ms.topic: article
ms.date: 09/25/2017
ms.openlocfilehash: 1906425c6657fb6232a9dc306b05f9171c9c7bef
ms.sourcegitcommit: 6699c77dcbd5f8a1a2f21fba3d0a0005ac9ed6b7
ms.translationtype: HT
ms.contentlocale: fr-FR
ms.lasthandoff: 10/11/2017
---
# <a name="logging-api-reference"></a>Informations de référence sur l’API de journalisation

La bibliothèque de journalisation d’Azure ML permet au programme d’émettre des métriques et des fichiers dont le suivi est assuré par le service d’historique à des fins d’analyse. Actuellement, quelques types de métriques et de fichiers de base sont pris en charge. Ce nombre augmentera dans les futures versions du package Python.

## <a name="uploading-metrics"></a>Téléchargement des métriques

```python
# import logging API package
from azureml.logging import get_azureml_logger

# initialize a logger object
logger = get_azureml_logger()

# log "scalar" metrics
logger.log("simple integer value", 7)
logger.log("simple float value", 3.141592)
logger.log("simple string value", "this is a string metric")

# log a list of numerical values. 
# this automatically creates a chart in the Run History details page
logger.log("chart data points", [1, 3, 5, 10, 6, 4])
```

Par défaut, toutes les métriques sont envoyées de manière asynchrone afin que la soumission n’entrave pas l’exécution du programme. Cela peut entraîner des problèmes de classement lorsque plusieurs métriques sont envoyées à la périphérie. C’est par exemple le cas lorsque deux métriques sont consignées en même temps, mais que pour une raison quelconque, l’utilisateur préfère que leur classement exact soit conservé. C’est aussi le cas lorsqu’un suivi des métriques doit être effectué avant d’exécuter une partie du code dont on sait qu’elle risque d’échouer rapidement. Dans ces deux cas, la solution consiste à _attendre_ que la métrique soit entièrement consignée avant de poursuivre :

```python
# blocking call
logger.log("my metric 1", 1).wait()
logger.log("my metric 2", 2).wait()
```

## <a name="consuming-metrics"></a>Utilisation des métriques

Ces métriques sont stockées par le service d’historique. Elles sont de plus associées à l’exécution qui les a produites. Une fois l’exécution terminée, l’onglet Historique d’exécution et la commande de l’interface de ligne de commande ci-dessous vous permettent tous deux de les récupérer (ainsi que les artefacts).

```azurecli
# show the last run
$ az ml history last

# list all past runs
$ az ml history list 

# show a paritcular run
$ az ml history info -r <runid>
```

## <a name="artifacts-files"></a>Artefacts (fichiers)

En plus des métriques, Azure ML permet à l’utilisateur d’effectuer le suivi des fichiers. Par défaut, tous les fichiers écrits dans le dossier `outputs` qui correspond au répertoire de travail du programme (le dossier du projet dans un contexte de calcul) sont téléchargés dans le service d’historique, puis suivis à des fins d’analyse. L’inconvénient reste que la taille des fichiers doit être inférieure à 512 Mo.


```Python
# Log content as an artifact
logger.upload("artifact/path", "This should be the contents of artifact/path in the service")
```

## <a name="consuming-artifacts"></a>Utilisation des artefacts

Pour imprimer le contenu d’un artefact dont le suivi a été effectué, l’utilisateur peut ouvrir l’onglet Historique d’exécution pour l’exécution concernée afin de **télécharger** ou de **promouvoir** l’artefact, ou encore utiliser les commandes de l’interface de ligne de commande ci-dessous pour obtenir le même effet.

```azurecli
# show all artifacts generated by a run
$ az ml history info -r <runid> -a <artifact/path>

# promote a particular artifact
$ az ml history promote -r <runid> -ap <artifact/prefix> -n <name of asset to create>
```
## <a name="next-steps"></a>Étapes suivantes
- Parcourez le [didacticiel Classifying Iris, part 2](tutorial-classifying-iris-part-2.md) (Classification d’Iris, partie 2) pour voir les API de journalisation à l’œuvre.
- Lisez [How to Use Run History and Model Metrics in Azure Machine Learning Workbench](how-to-use-run-history-model-metrics.md) (Comment utiliser les métriques de l’historique d’exécution et des modèles dans Azure Machine Learning Workbench) pour comprendre plus exactement l’utilisation des API de journalisation dans l’historique d’exécution.
